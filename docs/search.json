[{"path":"index.html","id":"introduction","chapter":"Introduction","heading":"Introduction","text":"textbook designed provide comprehensive introduction fundamental statistical techniques practical applications context biological data analysis. sets book apart “flipped-order” approach teaching. Unlike traditional statistics courses often begin theoretical concepts, book takes different approach initially presenting real-world data. grounding material practical scenarios, aims make learning process accessible, particularly new statistics.“flipped-order” introduction, readers able engage concrete examples statistical methods use delving underlying theories concepts support . pedagogical approach aims provide solid foundation readers comprehend apply statistical techniques effectively biological sciences.","code":""},{"path":"what-is-r-and-r-studio.html","id":"what-is-r-and-r-studio","chapter":"1 What is R and R studio?","heading":"1 What is R and R studio?","text":"book, use R RStudio organize, analyze, visualize data.R programming language software environment specifically designed statistical computing data analysis. widely used data analysts, statisticians, researchers tasks like:Data manipulation visualizationData manipulation visualizationStatistical modelingStatistical modelingProgrammingProgrammingR open-source, means ’s free use constantly improved large community contributors.RStudio Integrated Development Environment (IDE) R. provides user-friendly interface write, debug, run R code. R just console-based language, RStudio enhances productivity features like:Script editor: Write save R code syntax highlighting.Script editor: Write save R code syntax highlighting.Console: Run R commands interactively.Console: Run R commands interactively.Environment pane: View variables, datasets, objects workspace.Environment pane: View variables, datasets, objects workspace.Plots pane: Display export visualizations directly.Plots pane: Display export visualizations directly.Package manager: Install manage R packages effortlessly.Package manager: Install manage R packages effortlessly.RStudio helps bridge gap R’s powerful capabilities ease use, making preferred tool working R.summary, R engine analysis, RStudio dashboard makes driving engine simpler!","code":""},{"path":"installing-r-and-rstudio.html","id":"installing-r-and-rstudio","chapter":"2 Installing R and RStudio","heading":"2 Installing R and RStudio","text":"computer installed R RStudio!","code":""},{"path":"installing-r-and-rstudio.html","id":"step-1-install-r","chapter":"2 Installing R and RStudio","heading":"2.1 Step 1: Install R","text":"Visit R Project’s Website\nGo CRAN (Comprehensive R Archive Network).\nGo CRAN (Comprehensive R Archive Network).Download R\nChoose operating system:\nWindows: Click “Download R Windows” → “base” → “Download R X.X.X Windows”.\nmacOS: Click “Download R (Mac) OS X” → Select appropriate package macOS version.\nLinux: Follow instructions specific distribution.\n\nChoose operating system:\nWindows: Click “Download R Windows” → “base” → “Download R X.X.X Windows”.\nmacOS: Click “Download R (Mac) OS X” → Select appropriate package macOS version.\nLinux: Follow instructions specific distribution.\nWindows: Click “Download R Windows” → “base” → “Download R X.X.X Windows”.macOS: Click “Download R (Mac) OS X” → Select appropriate package macOS version.Linux: Follow instructions specific distribution.Install R\nRun downloaded file follow -screen instructions.\nusers, default settings work.\nRun downloaded file follow -screen instructions.users, default settings work.","code":""},{"path":"installing-r-and-rstudio.html","id":"step-2-install-rstudio","chapter":"2 Installing R and RStudio","heading":"2.2 Step 2: Install RStudio","text":"Visit RStudio’s Website\nGo RStudio Download.\nGo RStudio Download.Download RStudio\nClick “Download” free RStudio Desktop version.\nSelect installer operating system (Windows, macOS, Linux).\nClick “Download” free RStudio Desktop version.Select installer operating system (Windows, macOS, Linux).Install RStudio\nRun downloaded file follow installation prompts.\nRun downloaded file follow installation prompts.","code":""},{"path":"installing-r-and-rstudio.html","id":"step-3-launch-rstudio","chapter":"2 Installing R and RStudio","heading":"2.3 Step 3: Launch RStudio","text":"Open RStudio. find downloaded RStudio, type RStudio search window computer.RStudio automatically detect installed R version.R RStudio installation now ready use!","code":""},{"path":"r-project.html","id":"r-project","chapter":"3 R Project","heading":"3 R Project","text":"Throughout book, use R Project central workspace unit. R Project provides unified location relevant materials—R scripts (.R files) data sets—organized managed. various ways structure projects, recommend creating single R Project collection scripts data contribute specific publication.","code":""},{"path":"r-project.html","id":"setup-r-project","chapter":"3 R Project","heading":"3.1 Setup R Project","text":"set R Project, open RStudio follow procedure outlined :Go File > New Project top menuSelect New DirectorySelect New ProjectA new window appear, prompting name directory select location computer. choose location directory, click ‘Browse’ button. organizing project directories computer, highly recommend creating dedicated space. instance, computer, folder named github/ store R Project directories.Now, go R Project folder created, double-click .Rproj file. encounter interface depicted Figure 3.1. Initially, interface comprises three primary panels: Console, Environment, Files.Console execute calculations, data manipulation, analysis running codes. Environment panel lists saved objects, Files panel displays files designated location computer.\nFigure 3.1: RStudio interface.\nLet’s play little bit console see happens. pasting script console, notice variable x appears environment panel.x object assigns information. particular case, assigned 1 object x. assigned information x, can access simply typing x.Excellent! Let’s try following simple exercise get used :Create object y assign 2 .Create object y assign 2 .Create object zero assign 0 .Create object zero assign 0 .","code":"\nx <- 1\nx## [1] 1"},{"path":"r-project.html","id":"script-editor","chapter":"3 R Project","heading":"3.2 Script Editor","text":"can type code directly console, approach generally considered poor practice. project grows, complexity increases, often leading accumulation substantial material—potentially exceeding 2,000 lines code single project. Managing revisiting code written directly console quickly becomes unwieldy inefficient.highly recommended manage scripts Editor instead. Editor draft fine-tune code executing Console. create space Editor, press Ctrl + Shift + N. new panel appear top left corner. Type following script Editor (note key combination Ctrl + Shift + N assumes Windows Linux operating system. ’re using Mac, can use Command + Shift + N instead)., hit Ctr + S save Editor file. RStudio prompt enter file name Editor1 save .R file.","code":"\ny <- 5"},{"path":"r-project.html","id":"file-name","chapter":"3 R Project","heading":"3.3 File Name","text":"also crucial establish consistent naming rules files. project progresses, number files within sub-directory may increase significantly. Without clear consistent naming rules files, navigating project can become challenging, also others involved. alleviate issue, consider implementing following recommendations file naming:SPACE. Use underscore.\n: script_week1.R\nDon’t: script week1.R\n: script_week1.RDon’t: script week1.RNO UPPERCASE. Use lowercase file names.\n: script_week1.R\nDon’t: Script_week1.R\n: script_week1.RDon’t: Script_week1.RBE CONSISTENT. Apply consistent naming rules within project.\n: R scripts figures always start common prefix, e.g., figure_XXX.R figure_YYY.R(XXX YYY specifies details).\nDon’t: R scripts figures start random text, e.g., XXX_fig.R , Figure_Y2.R , plotB.R.\n: R scripts figures always start common prefix, e.g., figure_XXX.R figure_YYY.R(XXX YYY specifies details).Don’t: R scripts figures start random text, e.g., XXX_fig.R , Figure_Y2.R , plotB.R.","code":""},{"path":"r-project.html","id":"structure-your-project","chapter":"3 R Project","heading":"3.4 Structure Your Project","text":"internal structure R Project crucial effective navigation ensures clarity others published. R Project typically consists various file types, .R, .csv, .rds, .Rmd, others. Without organized arrangement files, high probability encountering significant coding errors. Therefore, place great importance maintaining well-structured project. Table 3.1, present recommended subdirectory structure.Table 3.1:  Suggested internal structure R Project","code":""},{"path":"r-project.html","id":"robust-coding","chapter":"3 R Project","heading":"3.5 Robust coding","text":"mandatory, highly recommend using RStudio conjunction Git GitHub. Coding inherently prone errors, even skilled programmers make mistakes – without exception. However, crucial difference beginner advanced programmers lies ability develop robust coding practices accompanied self-error-detection system. Git plays vital role process. Throughout book, occasionally delve importance Git usage.","code":""},{"path":"data-structure.html","id":"data-structure","chapter":"4 Data Structure","heading":"4 Data Structure","text":"","code":""},{"path":"data-structure.html","id":"overview","chapter":"4 Data Structure","heading":"4.1 Overview","text":"R six basic data types:Character: e.g., \"aquatic\", \"ecology\" (inherent order).Factor: Similar character includes levels, ordered alphabetically default.Numeric: e.g., 20.0, 15.5.Integer: e.g., 3, 7.Logical: e.g., TRUE, FALSE.Complex: e.g., 1 + 2i (numbers real imaginary parts).data types form basis R’s data structures, include:Vector: series elements, data type.Matrix: Elements arranged rows columns, data type.Dataframe: Similar matrix allows different data types different columns.following examples, use (x <- something) demonstrate code, outer parentheses () automatically print contents object x. Without parentheses, assignedvalues x displayed. example, following code assigns value x print :see value assigned x, need type:Using parentheses around assignment performs steps —- assigning value printing – simultaneously:","code":"\nx <- 1\nx## [1] 1\n(x <- 1)## [1] 1"},{"path":"data-structure.html","id":"vector","chapter":"4 Data Structure","heading":"4.2 Vector","text":"","code":""},{"path":"data-structure.html","id":"create-vector","chapter":"4 Data Structure","heading":"4.2.1 Create Vector","text":"examples atomic character vectors, numeric vectors, integer vectors, etc. many ways create vector data. following examples use c(), :, seq(), rep():Combine function c() combines multiple elements create single vector.Replicate function rep() replicates element(s) multiple times.Sequence function seq() creates vector based starting ending values specified interval vector length.Colon : creates vector based starting ending integer values interval one. works non-integer values, behavior odd (recommended; use functions non-integer vectors).","code":"\n# ex.1a manually create a vector using c()\n(x <- c(1, 3, 4, 8))## [1] 1 3 4 8\n# ex.1b character\n(x <- c(\"a\", \"b\", \"c\"))## [1] \"a\" \"b\" \"c\"\n# ex.1c logical\n(x <- c(TRUE, FALSE, FALSE))## [1]  TRUE FALSE FALSE\n# ex.3a replicate same numbers or characters\n(x <- rep(2, times = 5)) # replicate 2 five times## [1] 2 2 2 2 2\n# ex.3b replicate same numbers or characters\n(x <- rep(\"a\", 5)) # replicate \"a\" five times## [1] \"a\" \"a\" \"a\" \"a\" \"a\"\n# ex.3c replicate each element multiple times\n(x <- rep(c(1, 2), each = 2))## [1] 1 1 2 2\n# ex.4a use seq() function\n# create a vector from 1 to 5 with interval 1\n(x <- seq(1, 5, by = 1))## [1] 1 2 3 4 5\n# ex.4b use seq() function\n# create a vector from 1 to 5 with 20 elements\n(x <- seq(1, 5, length = 20))##  [1] 1.000000 1.210526 1.421053 1.631579 1.842105 2.052632 2.263158 2.473684\n##  [9] 2.684211 2.894737 3.105263 3.315789 3.526316 3.736842 3.947368 4.157895\n## [17] 4.368421 4.578947 4.789474 5.000000\n# ex.5a use colon `:` - interval is fixed at one\n(x <- 1:5)## [1] 1 2 3 4 5\n# this works, but odd\n# NOT recommended\n1.5:4.7## [1] 1.5 2.5 3.5 4.5"},{"path":"data-structure.html","id":"extract-summary","chapter":"4 Data Structure","heading":"4.2.2 Extract Summary","text":"Numerical character information can summarized using functions. Let’s try basic functions see works:Mean function mean() calculates arithmetic mean.Sum function sum() calculates summation.Length function length() returns number elements vector.","code":"\nx <- c(10, 15, 20)\nmean(x)## [1] 15\nx <- c(10, 15, 20)\nsum(x)## [1] 45\nx <- c(10, 15, 20)\nlength(x)## [1] 3"},{"path":"data-structure.html","id":"access","chapter":"4 Data Structure","heading":"4.2.3 Access","text":"Element ID Use brackets [] accessing specific elements object. example, want access element #2 vector x, may specify x[2]:Equation R provides many ways access elements meet specific conditions. can use mathematical symbols specify need, example:== equal> larger >= equal & larger < smaller <= equal & smaller thanwhich() function returns element # suffices specified conditionThe following examples return logical vector indicating whether element x suffices specified condition:can access elements meet specified condition using brackets, example:Using (), can see elements (.e., #) matches need:","code":"\nx <- c(2, 2, 3, 2, 5)\nx[2] # access element #2## [1] 2\nx[c(2, 4)] # access elements #2 and 4## [1] 2 2\nx[2:4] # access elements #2-4## [1] 2 3 2\n# creating a vector\nx <- c(2, 2, 3, 2, 5)\n\n# ex.1a equal\nx == 2## [1]  TRUE  TRUE FALSE  TRUE FALSE\n# ex.1b larger than\nx > 2 ## [1] FALSE FALSE  TRUE FALSE  TRUE\n# ex.2a equal\nx[x == 2]## [1] 2 2 2\n# ex.2b larger than\nx[x > 2]## [1] 3 5\n# ex.3a equal\nwhich(x == 2) # returns which elements are equal to 2## [1] 1 2 4\n# ex.3b larger than\nwhich(x > 2)## [1] 3 5"},{"path":"data-structure.html","id":"caveat","chapter":"4 Data Structure","heading":"4.2.4 Caveat","text":"single vector contain multiple types data. example, try following code. R coerced x character vector, numeric.Also, character data type limited letters; numbers can character, double-quoted. example, calculate mean following vector numbers recognized character R.","code":"\n(x <- c(1, 2, \"a\"))## [1] \"1\" \"2\" \"a\"\n## class() returns a data type of an object\nclass(x)## [1] \"character\"\n## enter numbers as character \"1\" \"2\"\n(x <- c(\"1\", \"2\"))## [1] \"1\" \"2\"\n## class() returns a data type of an object\nclass(x)## [1] \"character\"\n## cannot calculate the mean\nmean(x)## [1] NA"},{"path":"data-structure.html","id":"exercise","chapter":"4 Data Structure","heading":"4.2.5 Exercise","text":"Create three numeric vectors length 3, 6 20, respectively. three vectors must created using different functions (c(), rep(), seq()).Create two character vectors length 3 20, respectively. two vectors must created using two different functions (c(), rep()).Copy following script R script perform following analysis:Identify element IDs y greater 2.0Identify element values y greater 2.0Calculate arithmetic mean y","code":"\nset.seed(1)\ny <- rnorm(100)"},{"path":"data-structure.html","id":"matrix","chapter":"4 Data Structure","heading":"4.3 Matrix","text":"","code":""},{"path":"data-structure.html","id":"create-matrix","chapter":"4 Data Structure","heading":"4.3.1 Create Matrix","text":"Matrix set elements (single data type) organized rows columns:Column bind function cbind() combines two vectors column.Row bind function rbind() combines two vectors row.Matrix function matrix() organize vector matrix specifying numbers rows columns","code":"\n## numeric\n(m_x <- cbind(c(1,2,3), c(4,5,6)))##      [,1] [,2]\n## [1,]    1    4\n## [2,]    2    5\n## [3,]    3    6\n## character\n(m_x <- cbind(c(\"a\", \"b\", \"c\"), c(\"d\", \"e\", \"f\")))##      [,1] [,2]\n## [1,] \"a\"  \"d\" \n## [2,] \"b\"  \"e\" \n## [3,] \"c\"  \"f\"\n## numeric\n(m_x <- rbind(c(1, 2, 3), c(4, 5, 6)))##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    4    5    6\n## character\n(m_x <- rbind(c(\"a\", \"b\", \"c\"), c(\"d\", \"e\", \"f\")))##      [,1] [,2] [,3]\n## [1,] \"a\"  \"b\"  \"c\" \n## [2,] \"d\"  \"e\"  \"f\"\n## numeric: vector 1:9 is organized into 3 x 2 matrix\n(m_x <- matrix(1:9, nrow = 3, ncol = 2))##      [,1] [,2]\n## [1,]    1    4\n## [2,]    2    5\n## [3,]    3    6\n(m_x <- matrix(c(\"a\", \"b\", \"c\",\n                 \"d\", \"e\", \"f\"),\n               nrow = 3,\n               ncol = 2))##      [,1] [,2]\n## [1,] \"a\"  \"d\" \n## [2,] \"b\"  \"e\" \n## [3,] \"c\"  \"f\""},{"path":"data-structure.html","id":"access-1","chapter":"4 Data Structure","heading":"4.3.2 Access","text":"Element ID accessing matrix elements, need pick row(s) /column(s). brackets, specify row ID(s) comma, column ID(s) comma ([row ID, column ID]). example:Equation can assess element mathematical expressions just like vectors:However, care must taken accessing elements, converted vector data:() needs additional argument return row column #:","code":"\n(m_x <- matrix(1:9, nrow = 3, ncol = 3))##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\nm_x[2, 3] # access an element in row #2 and column #3## [1] 8\nm_x[2,] # access elements in row #2## [1] 2 5 8\nm_x[c(2, 3),] # access elements in rows #2 and #3##      [,1] [,2] [,3]\n## [1,]    2    5    8\n## [2,]    3    6    9\nm_x[,c(2, 3)] # access elements in columns #2 and #3##      [,1] [,2]\n## [1,]    4    7\n## [2,]    5    8\n## [3,]    6    9\nm_x == 2 # equal##       [,1]  [,2]  [,3]\n## [1,] FALSE FALSE FALSE\n## [2,]  TRUE FALSE FALSE\n## [3,] FALSE FALSE FALSE\nm_x > 2 # larger than##       [,1] [,2] [,3]\n## [1,] FALSE TRUE TRUE\n## [2,] FALSE TRUE TRUE\n## [3,]  TRUE TRUE TRUE\nm_x[m_x == 2] # equal## [1] 2\nm_x[m_x > 2] # larger than## [1] 3 4 5 6 7 8 9\nwhich(m_x == 2, arr.ind = TRUE)##      row col\n## [1,]   2   1\nwhich(m_x > 2, arr.ind = TRUE)##      row col\n## [1,]   3   1\n## [2,]   1   2\n## [3,]   2   2\n## [4,]   3   2\n## [5,]   1   3\n## [6,]   2   3\n## [7,]   3   3"},{"path":"data-structure.html","id":"exercise-1","chapter":"4 Data Structure","heading":"4.3.3 Exercise","text":"","code":""},{"path":"data-structure.html","id":"matrix-1","chapter":"4 Data Structure","heading":"4.3.4 Matrix","text":"Create numeric matrix 4 rows 4 columns using either rbind() cbind() function.Create character matrix 4 rows 4 columns using matrix() function.Copy following script R script perform following analysis:Identify element IDs x greater 2.0 (specify row column IDs)Identify element values x greater 2.0 calculate mean selected elements.","code":"\nset.seed(1)\nx <- matrix(rnorm(100), nrow = 10, ncol = 10)"},{"path":"data-structure.html","id":"data-frame","chapter":"4 Data Structure","heading":"4.4 Data Frame","text":"data frame collection elements organized rows columns, differs matrix several ways.allows inclusion multiple data types different columns.column data frame name associated .can access columns data frame respective names using $ operator.data frame commonly used data structure manipulating ecological data. loading dataset spreadsheet (discuss later), automatically recognized data frame. Let’s consider example:","code":""},{"path":"data-structure.html","id":"creating-a-data-frame","chapter":"4 Data Structure","heading":"4.4.1 Creating a data frame","text":"following example, variables x y organized single data frame named df0. variables renamed part process creating data frame.","code":"\n# Create data frame\n\n# Lake type\nx <- c(\"Pristine\", \"Pristine\", \"Disturbed\", \"Disturbed\", \"Pristine\")\n\n# TSS: total suspended solids (mg/L)\ny <- c(1.2, 2.2, 10.9, 50.0, 3.0)\n\n# x is named as \"LakeType\" while y is named as \"TSS\"\n(df0 <- data.frame(LakeType = x, TSS = y))##    LakeType  TSS\n## 1  Pristine  1.2\n## 2  Pristine  2.2\n## 3 Disturbed 10.9\n## 4 Disturbed 50.0\n## 5  Pristine  3.0"},{"path":"data-structure.html","id":"access-by-columns","chapter":"4 Data Structure","heading":"4.4.2 Access by columns","text":"access elements column names, use $ dataframe, column name.can access elements like matrix well:","code":"\ndf0$LakeType # access LakeType## [1] \"Pristine\"  \"Pristine\"  \"Disturbed\" \"Disturbed\" \"Pristine\"\ndf0$TSS # access TSS## [1]  1.2  2.2 10.9 50.0  3.0\ndf0[, 1] # access column #1## [1] \"Pristine\"  \"Pristine\"  \"Disturbed\" \"Disturbed\" \"Pristine\"\ndf0[1, ] # access row #1##   LakeType TSS\n## 1 Pristine 1.2\ndf0[c(2, 4),] # access row #2 and 4##    LakeType  TSS\n## 2  Pristine  2.2\n## 4 Disturbed 50.0"},{"path":"data-structure.html","id":"exercise-2","chapter":"4 Data Structure","heading":"4.4.3 Exercise","text":"Copy following script R script perform following analysis:Access temperature column.Calculate means temperature abundance states VA NC separately.","code":"\nset.seed(1)\nx <- rnorm(100, mean = 10, sd = 3)\ny <- rpois(100, lambda = 10)\nz <- rep(c(\"VA\", \"NC\"), 50)\ndf0 <- data.frame(temperature = x, abundance = y, state = z)"},{"path":"data-manipulation.html","id":"data-manipulation","chapter":"5 Data Manipulation","heading":"5 Data Manipulation","text":"R packages collections functions extend functionality R programming language. provide convenient way users access utilize specialized tools data analysis, visualization, statistical modeling, . Among , tidyverse provides useful functions data manipulation visualization.tidyverse:: bundles R packages designed make data manipulation, exploration, visualization efficient intuitive. Developed Hadley Wickham contributors, Tidyverse packages share common philosophy syntax, emphasizing consistent tidy data format. core packages, dplyr::, tidyr::, ggplot2::, provide powerful tools data wrangling, reshaping, creating visualizations.Chapter, use iris data, available default R. default data structure iris prepared data.frame(), convert data format tibble() exercise. Copy run code . use iris_sub following demonstration.cover basics familiar tidyverse::, plenty documentation available tidyverse package.Tidyverse websitedplyrtidyrdplyr:: tidyr:: (others) offer useful functions data manipulation. Key manipulations include: row/column manipulations, group operation, reshape, .","code":"\nlibrary(tidyverse)\n\nset.seed(123)\n\niris_sub <- as_tibble(iris) %>% \n  group_by(Species) %>% \n  sample_n(3) %>% \n  ungroup()\n\nprint(iris_sub)## # A tibble: 9 × 5\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n## 1          4.8         3.1          1.6         0.2 setosa    \n## 2          5.8         4            1.2         0.2 setosa    \n## 3          4.3         3            1.1         0.1 setosa    \n## 4          6.9         3.1          4.9         1.5 versicolor\n## 5          6.1         3            4.6         1.4 versicolor\n## 6          5.8         2.6          4           1.2 versicolor\n## 7          6.3         3.4          5.6         2.4 virginica \n## 8          5.7         2.5          5           2   virginica \n## 9          6.7         3.3          5.7         2.1 virginica"},{"path":"data-manipulation.html","id":"row-manipulation","chapter":"5 Data Manipulation","heading":"5.1 Row Manipulation","text":"","code":""},{"path":"data-manipulation.html","id":"subset-rows","chapter":"5 Data Manipulation","heading":"5.1.1 Subset rows","text":"filter() function (dplyr package) used subsetting rows data frame (tibble) based conditions.Single match ==Multiple match %%Except !=Except multiple !=Greater >Greater equal >=Less <Less equal <=Multiple conditions () & (,)Multiple conditions () |","code":"\nfilter(iris_sub, Species == \"virginica\")## # A tibble: 3 × 5\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>    \n## 1          6.3         3.4          5.6         2.4 virginica\n## 2          5.7         2.5          5           2   virginica\n## 3          6.7         3.3          5.7         2.1 virginica\nfilter(iris_sub, Species %in% c(\"virginica\", \"versicolor\"))## # A tibble: 6 × 5\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n## 1          6.9         3.1          4.9         1.5 versicolor\n## 2          6.1         3            4.6         1.4 versicolor\n## 3          5.8         2.6          4           1.2 versicolor\n## 4          6.3         3.4          5.6         2.4 virginica \n## 5          5.7         2.5          5           2   virginica \n## 6          6.7         3.3          5.7         2.1 virginica\nfilter(iris_sub, Species != \"virginica\")## # A tibble: 6 × 5\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n## 1          4.8         3.1          1.6         0.2 setosa    \n## 2          5.8         4            1.2         0.2 setosa    \n## 3          4.3         3            1.1         0.1 setosa    \n## 4          6.9         3.1          4.9         1.5 versicolor\n## 5          6.1         3            4.6         1.4 versicolor\n## 6          5.8         2.6          4           1.2 versicolor\nfilter(iris_sub, !(Species %in% c(\"virginica\", \"versicolor\")))## # A tibble: 3 × 5\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n## 1          4.8         3.1          1.6         0.2 setosa \n## 2          5.8         4            1.2         0.2 setosa \n## 3          4.3         3            1.1         0.1 setosa\nfilter(iris_sub, Sepal.Length > 5)## # A tibble: 7 × 5\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n## 1          5.8         4            1.2         0.2 setosa    \n## 2          6.9         3.1          4.9         1.5 versicolor\n## 3          6.1         3            4.6         1.4 versicolor\n## 4          5.8         2.6          4           1.2 versicolor\n## 5          6.3         3.4          5.6         2.4 virginica \n## 6          5.7         2.5          5           2   virginica \n## 7          6.7         3.3          5.7         2.1 virginica\nfilter(iris_sub, Sepal.Length >= 5)## # A tibble: 7 × 5\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n## 1          5.8         4            1.2         0.2 setosa    \n## 2          6.9         3.1          4.9         1.5 versicolor\n## 3          6.1         3            4.6         1.4 versicolor\n## 4          5.8         2.6          4           1.2 versicolor\n## 5          6.3         3.4          5.6         2.4 virginica \n## 6          5.7         2.5          5           2   virginica \n## 7          6.7         3.3          5.7         2.1 virginica\nfilter(iris_sub, Sepal.Length < 5)## # A tibble: 2 × 5\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n## 1          4.8         3.1          1.6         0.2 setosa \n## 2          4.3         3            1.1         0.1 setosa\nfilter(iris_sub, Sepal.Length <= 5)## # A tibble: 2 × 5\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n## 1          4.8         3.1          1.6         0.2 setosa \n## 2          4.3         3            1.1         0.1 setosa\n# Sepal.Length is less than 5 AND Species equals \"setosa\"\nfilter(iris_sub,\n       Sepal.Length < 5 & Species == \"setosa\")## # A tibble: 2 × 5\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n## 1          4.8         3.1          1.6         0.2 setosa \n## 2          4.3         3            1.1         0.1 setosa\n# same; \",\" works like \"&\"\nfilter(iris_sub,\n       Sepal.Length < 5, Species == \"setosa\")## # A tibble: 2 × 5\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n## 1          4.8         3.1          1.6         0.2 setosa \n## 2          4.3         3            1.1         0.1 setosa\n# Either Sepal.Length is less than 5 OR Species equals \"setosa\"\nfilter(iris_sub,\n       Sepal.Length < 5 | Species == \"setosa\")## # A tibble: 3 × 5\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n## 1          4.8         3.1          1.6         0.2 setosa \n## 2          5.8         4            1.2         0.2 setosa \n## 3          4.3         3            1.1         0.1 setosa"},{"path":"data-manipulation.html","id":"arrange-rows","chapter":"5 Data Manipulation","heading":"5.1.2 Arrange rows","text":"arrange() function (package dplyr) used reorder rows data frame (tibble) based values one columns.Increasing/ascending orderDecreasing/descending order","code":"\narrange(iris_sub, Sepal.Length)## # A tibble: 9 × 5\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n## 1          4.3         3            1.1         0.1 setosa    \n## 2          4.8         3.1          1.6         0.2 setosa    \n## 3          5.7         2.5          5           2   virginica \n## 4          5.8         4            1.2         0.2 setosa    \n## 5          5.8         2.6          4           1.2 versicolor\n## 6          6.1         3            4.6         1.4 versicolor\n## 7          6.3         3.4          5.6         2.4 virginica \n## 8          6.7         3.3          5.7         2.1 virginica \n## 9          6.9         3.1          4.9         1.5 versicolor\narrange(iris_sub, desc(Sepal.Length))## # A tibble: 9 × 5\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n## 1          6.9         3.1          4.9         1.5 versicolor\n## 2          6.7         3.3          5.7         2.1 virginica \n## 3          6.3         3.4          5.6         2.4 virginica \n## 4          6.1         3            4.6         1.4 versicolor\n## 5          5.8         4            1.2         0.2 setosa    \n## 6          5.8         2.6          4           1.2 versicolor\n## 7          5.7         2.5          5           2   virginica \n## 8          4.8         3.1          1.6         0.2 setosa    \n## 9          4.3         3            1.1         0.1 setosa"},{"path":"data-manipulation.html","id":"exercise-3","chapter":"5 Data Manipulation","heading":"5.1.3 Exercise","text":"Using iris_sub dataframe, select rows match following contidionsSepal.Width greater 3.0 assign new dataframe iris_3Sepal.Width greater 3.0 assign new dataframe iris_3Species \"setosa\" assign new dataframe iris_setosaSpecies \"setosa\" assign new dataframe iris_setosaSepal.Width greater 3.0 Species \"setosa\", assign new dataframe iris_3_setosaSepal.Width greater 3.0 Species \"setosa\", assign new dataframe iris_3_setosa","code":""},{"path":"data-manipulation.html","id":"column-manipulation","chapter":"5 Data Manipulation","heading":"5.2 Column Manipulation","text":"","code":""},{"path":"data-manipulation.html","id":"select-columns","chapter":"5 Data Manipulation","heading":"5.2.1 Select columns","text":"select() function (package dplyr) used choose specific columns data frame tibble. particularly useful narrowing dataset focus relevant variables.Select one columnSelect multiple columnsRemove one columnRemove multiple columnsSelect/Remove starts_with()Select ends_with()","code":"\nselect(iris_sub, Sepal.Length)## # A tibble: 9 × 1\n##   Sepal.Length\n##          <dbl>\n## 1          4.8\n## 2          5.8\n## 3          4.3\n## 4          6.9\n## 5          6.1\n## 6          5.8\n## 7          6.3\n## 8          5.7\n## 9          6.7\nselect(iris_sub, c(Sepal.Length, Sepal.Width))## # A tibble: 9 × 2\n##   Sepal.Length Sepal.Width\n##          <dbl>       <dbl>\n## 1          4.8         3.1\n## 2          5.8         4  \n## 3          4.3         3  \n## 4          6.9         3.1\n## 5          6.1         3  \n## 6          5.8         2.6\n## 7          6.3         3.4\n## 8          5.7         2.5\n## 9          6.7         3.3\nselect(iris_sub, -Sepal.Length)## # A tibble: 9 × 4\n##   Sepal.Width Petal.Length Petal.Width Species   \n##         <dbl>        <dbl>       <dbl> <fct>     \n## 1         3.1          1.6         0.2 setosa    \n## 2         4            1.2         0.2 setosa    \n## 3         3            1.1         0.1 setosa    \n## 4         3.1          4.9         1.5 versicolor\n## 5         3            4.6         1.4 versicolor\n## 6         2.6          4           1.2 versicolor\n## 7         3.4          5.6         2.4 virginica \n## 8         2.5          5           2   virginica \n## 9         3.3          5.7         2.1 virginica\nselect(iris_sub, -c(Sepal.Length, Sepal.Width))## # A tibble: 9 × 3\n##   Petal.Length Petal.Width Species   \n##          <dbl>       <dbl> <fct>     \n## 1          1.6         0.2 setosa    \n## 2          1.2         0.2 setosa    \n## 3          1.1         0.1 setosa    \n## 4          4.9         1.5 versicolor\n## 5          4.6         1.4 versicolor\n## 6          4           1.2 versicolor\n## 7          5.6         2.4 virginica \n## 8          5           2   virginica \n## 9          5.7         2.1 virginica\n# select columns starting with \"Sepal\"\nselect(iris_sub, starts_with(\"Sepal\"))## # A tibble: 9 × 2\n##   Sepal.Length Sepal.Width\n##          <dbl>       <dbl>\n## 1          4.8         3.1\n## 2          5.8         4  \n## 3          4.3         3  \n## 4          6.9         3.1\n## 5          6.1         3  \n## 6          5.8         2.6\n## 7          6.3         3.4\n## 8          5.7         2.5\n## 9          6.7         3.3\n# remove columns starting with \"Sepal\"\nselect(iris_sub, -starts_with(\"Sepal\"))## # A tibble: 9 × 3\n##   Petal.Length Petal.Width Species   \n##          <dbl>       <dbl> <fct>     \n## 1          1.6         0.2 setosa    \n## 2          1.2         0.2 setosa    \n## 3          1.1         0.1 setosa    \n## 4          4.9         1.5 versicolor\n## 5          4.6         1.4 versicolor\n## 6          4           1.2 versicolor\n## 7          5.6         2.4 virginica \n## 8          5           2   virginica \n## 9          5.7         2.1 virginica\n# select columns ending with \"Sepal\"\nselect(iris_sub, ends_with(\"Width\"))## # A tibble: 9 × 2\n##   Sepal.Width Petal.Width\n##         <dbl>       <dbl>\n## 1         3.1         0.2\n## 2         4           0.2\n## 3         3           0.1\n## 4         3.1         1.5\n## 5         3           1.4\n## 6         2.6         1.2\n## 7         3.4         2.4\n## 8         2.5         2  \n## 9         3.3         2.1\n# remove columns ending with \"Sepal\"\nselect(iris_sub, -ends_with(\"Width\"))## # A tibble: 9 × 3\n##   Sepal.Length Petal.Length Species   \n##          <dbl>        <dbl> <fct>     \n## 1          4.8          1.6 setosa    \n## 2          5.8          1.2 setosa    \n## 3          4.3          1.1 setosa    \n## 4          6.9          4.9 versicolor\n## 5          6.1          4.6 versicolor\n## 6          5.8          4   versicolor\n## 7          6.3          5.6 virginica \n## 8          5.7          5   virginica \n## 9          6.7          5.7 virginica"},{"path":"data-manipulation.html","id":"add-columns","chapter":"5 Data Manipulation","heading":"5.2.2 Add columns","text":"mutate() function (package dplyr) package used add new variables modify existing ones data frame tibble. allows apply transformations columns without changing original dataset.Add new columnModify existing column","code":"\n# nrow() returns the number of rows of the dataframe\n(x_max <- nrow(iris_sub))## [1] 9\n# create a vector from 1 to x_max\nx <- 1:x_max\n\n# add as a new column\n# named `x` as `row_id` when added\nmutate(iris_sub, row_id = x)## # A tibble: 9 × 6\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species    row_id\n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>       <int>\n## 1          4.8         3.1          1.6         0.2 setosa          1\n## 2          5.8         4            1.2         0.2 setosa          2\n## 3          4.3         3            1.1         0.1 setosa          3\n## 4          6.9         3.1          4.9         1.5 versicolor      4\n## 5          6.1         3            4.6         1.4 versicolor      5\n## 6          5.8         2.6          4           1.2 versicolor      6\n## 7          6.3         3.4          5.6         2.4 virginica       7\n## 8          5.7         2.5          5           2   virginica       8\n## 9          6.7         3.3          5.7         2.1 virginica       9\n# twice `Sepal.Length` and add as a new column\nmutate(iris_sub, sl_two_times = 2 * Sepal.Length)## # A tibble: 9 × 6\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species    sl_two_times\n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>             <dbl>\n## 1          4.8         3.1          1.6         0.2 setosa              9.6\n## 2          5.8         4            1.2         0.2 setosa             11.6\n## 3          4.3         3            1.1         0.1 setosa              8.6\n## 4          6.9         3.1          4.9         1.5 versicolor         13.8\n## 5          6.1         3            4.6         1.4 versicolor         12.2\n## 6          5.8         2.6          4           1.2 versicolor         11.6\n## 7          6.3         3.4          5.6         2.4 virginica          12.6\n## 8          5.7         2.5          5           2   virginica          11.4\n## 9          6.7         3.3          5.7         2.1 virginica          13.4"},{"path":"data-manipulation.html","id":"exercise-4","chapter":"5 Data Manipulation","heading":"5.2.3 Exercise","text":"Using iris_sub dataframe, select columns match following contidionsSelect column Petal.Width Species assign new dataframe iris_pwSelect column Petal.Width Species assign new dataframe iris_pwSelect columns starting text \"Petal\" assign new dataframe iris_petalSelect columns starting text \"Petal\" assign new dataframe iris_petalAdd new column pw_two_times doubling values column Petal.Width, assign new dataframe iris_pw_twoAdd new column pw_two_times doubling values column Petal.Width, assign new dataframe iris_pw_two","code":""},{"path":"data-manipulation.html","id":"piping","chapter":"5 Data Manipulation","heading":"5.3 Piping","text":"","code":""},{"path":"data-manipulation.html","id":"pipe","chapter":"5 Data Manipulation","heading":"5.3.1 Pipe %>%","text":"%>% (pipe) allows sequential operations multiple functions (hot key: Ctr + Shift + M). pipe passes object following function first argument. example, following code subset rows Species column, select column Sepal.Length.piping, example becomes:","code":"\n## \ndf_vir <- filter(iris_sub, Species == \"virginica\")\ndf_vir_sl <- select(df_vir, Sepal.Length)\n\nprint(df_vir_sl)## # A tibble: 3 × 1\n##   Sepal.Length\n##          <dbl>\n## 1          6.3\n## 2          5.7\n## 3          6.7\ndf_vir_sl <- iris_sub %>% \n  filter(Species == \"virginica\") %>% \n  select(Sepal.Length)\n\nprint(df_vir_sl)## # A tibble: 3 × 1\n##   Sepal.Length\n##          <dbl>\n## 1          6.3\n## 2          5.7\n## 3          6.7"},{"path":"data-manipulation.html","id":"exercise-5","chapter":"5 Data Manipulation","heading":"5.3.2 Exercise","text":"Subset iris_sub Species column (choose \"setosa\" ) add new column pw_two_times doubling values column Petal.Width. Assign resultant dataframe iris_pipe. USE pipe %>% operation.","code":""},{"path":"data-manipulation.html","id":"group-operation","chapter":"5 Data Manipulation","heading":"5.4 Group Operation","text":"","code":""},{"path":"data-manipulation.html","id":"grouping-group_by","chapter":"5 Data Manipulation","heading":"5.4.1 Grouping group_by()","text":"Often, may want calculate summary statistics group. cases, group_by() function R provides effective way group data applying summary functions.original iris_sub dataframe, group structure imposed, meaning print dataframe, display information grouping.Now, let’s add group structure dataframe. following code groups data based values Species column:dataframe remains unchanged, ’ll notice additional text top, indicating Groups: Species[3]. means dataframe now contains three groups, defined unique values Species column.","code":"\nprint(iris_sub)## # A tibble: 9 × 5\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n## 1          4.8         3.1          1.6         0.2 setosa    \n## 2          5.8         4            1.2         0.2 setosa    \n## 3          4.3         3            1.1         0.1 setosa    \n## 4          6.9         3.1          4.9         1.5 versicolor\n## 5          6.1         3            4.6         1.4 versicolor\n## 6          5.8         2.6          4           1.2 versicolor\n## 7          6.3         3.4          5.6         2.4 virginica \n## 8          5.7         2.5          5           2   virginica \n## 9          6.7         3.3          5.7         2.1 virginica\niris_sub %>% \n  group_by(Species)## # A tibble: 9 × 5\n## # Groups:   Species [3]\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n## 1          4.8         3.1          1.6         0.2 setosa    \n## 2          5.8         4            1.2         0.2 setosa    \n## 3          4.3         3            1.1         0.1 setosa    \n## 4          6.9         3.1          4.9         1.5 versicolor\n## 5          6.1         3            4.6         1.4 versicolor\n## 6          5.8         2.6          4           1.2 versicolor\n## 7          6.3         3.4          5.6         2.4 virginica \n## 8          5.7         2.5          5           2   virginica \n## 9          6.7         3.3          5.7         2.1 virginica"},{"path":"data-manipulation.html","id":"summarize-with-summarize","chapter":"5 Data Manipulation","heading":"5.4.2 Summarize with summarize()","text":"group_by() function becomes especially useful combined functions like summarize() mutate(). instance, calculate mean values species iris_sub dataframe, can use following code:want calculate multiple summary statistics, can separate operations comma , shown :","code":"\niris_sub %>% \n  group_by(Species) %>% \n  summarize(mu_sl = mean(Sepal.Length))## # A tibble: 3 × 2\n##   Species    mu_sl\n##   <fct>      <dbl>\n## 1 setosa      4.97\n## 2 versicolor  6.27\n## 3 virginica   6.23\niris_sub %>% \n  group_by(Species) %>% \n  summarize(mu_sl = mean(Sepal.Length),\n            sum_sl = sum(Sepal.Length))## # A tibble: 3 × 3\n##   Species    mu_sl sum_sl\n##   <fct>      <dbl>  <dbl>\n## 1 setosa      4.97   14.9\n## 2 versicolor  6.27   18.8\n## 3 virginica   6.23   18.7"},{"path":"data-manipulation.html","id":"summarize-with-mutate","chapter":"5 Data Manipulation","heading":"5.4.3 Summarize with mutate()","text":"summarize() function returns summary table, group represented single row. hand, mutate() retains original individual rows, adding summary columns. case, row within group summary value. Remember use ungroup() grouping operations prevent errors subsequent operations.","code":"\n# grouping by \"Species\", then take means \"Speal.Length\" for each species\niris_sub %>% \n  group_by(Species) %>% \n  mutate(mu_sl = mean(Sepal.Length)) %>% \n  ungroup()## # A tibble: 9 × 6\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species    mu_sl\n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>      <dbl>\n## 1          4.8         3.1          1.6         0.2 setosa      4.97\n## 2          5.8         4            1.2         0.2 setosa      4.97\n## 3          4.3         3            1.1         0.1 setosa      4.97\n## 4          6.9         3.1          4.9         1.5 versicolor  6.27\n## 5          6.1         3            4.6         1.4 versicolor  6.27\n## 6          5.8         2.6          4           1.2 versicolor  6.27\n## 7          6.3         3.4          5.6         2.4 virginica   6.23\n## 8          5.7         2.5          5           2   virginica   6.23\n## 9          6.7         3.3          5.7         2.1 virginica   6.23"},{"path":"data-manipulation.html","id":"reshape","chapter":"5 Data Manipulation","heading":"5.5 Reshape","text":"pivot_wider() : reshape data frame wide formatpivot_longer(): reshape data frame long format","code":"\niris_w <- iris_sub %>% \n  mutate(id = rep(1:3, 3)) %>% # add an ID column\n  select(id, Sepal.Length, Species) %>% \n  pivot_wider(id_cols = \"id\", # unique row ID based on\n              values_from = \"Sepal.Length\", # values in each cell from\n              names_from = \"Species\") # new column names from\n\nprint(iris_w)## # A tibble: 3 × 4\n##      id setosa versicolor virginica\n##   <int>  <dbl>      <dbl>     <dbl>\n## 1     1    4.8        6.9       6.3\n## 2     2    5.8        6.1       5.7\n## 3     3    4.3        5.8       6.7\niris_l <- iris_w %>% \n  pivot_longer(cols = c(\"setosa\",\n                        \"versicolor\",\n                        \"virginica\"), # columns with values to be reshaped\n               names_to = \"Species\", # column IDs move to \"Species\"\n               values_to = \"Sepal.Length\") # column values move to \"Sepal.Length\"\n\nprint(iris_l)## # A tibble: 9 × 3\n##      id Species    Sepal.Length\n##   <int> <chr>             <dbl>\n## 1     1 setosa              4.8\n## 2     1 versicolor          6.9\n## 3     1 virginica           6.3\n## 4     2 setosa              5.8\n## 5     2 versicolor          6.1\n## 6     2 virginica           5.7\n## 7     3 setosa              4.3\n## 8     3 versicolor          5.8\n## 9     3 virginica           6.7"},{"path":"data-manipulation.html","id":"join","chapter":"5 Data Manipulation","heading":"5.6 Join","text":"left_join(): merge data frames based column(s)","code":"\n# matching by a single column\n## left join by \"Species\": one to one\ndf1 <- tibble(Species = c(\"A\", \"B\", \"C\"),\n              x = c(1, 2, 3))\n\ndf2 <- tibble(Species = c(\"A\", \"B\", \"C\"),\n              y = c(4, 5, 6))\n\nleft_join(x = df1,\n          y = df2,\n          by = \"Species\")## # A tibble: 3 × 3\n##   Species     x     y\n##   <chr>   <dbl> <dbl>\n## 1 A           1     4\n## 2 B           2     5\n## 3 C           3     6\n# matching by a single column\n## left join by \"Species\": one to many\ndf3 <- tibble(Species = c(\"A\", \"A\", \"B\", \"C\"),\n              y = c(4, 5, 6, 7))\n\nleft_join(x = df1,\n          y = df3,\n          by = \"Species\")## # A tibble: 4 × 3\n##   Species     x     y\n##   <chr>   <dbl> <dbl>\n## 1 A           1     4\n## 2 A           1     5\n## 3 B           2     6\n## 4 C           3     7\n# matching by a single column\n## left join by \"Species\": one to missing\ndf4 <- tibble(Species = c(\"A\", \"A\", \"C\"),\n              y = c(4, 5, 7))\n\nleft_join(x = df1,\n          y = df4,\n          by = \"Species\")## # A tibble: 4 × 3\n##   Species     x     y\n##   <chr>   <dbl> <dbl>\n## 1 A           1     4\n## 2 A           1     5\n## 3 B           2    NA\n## 4 C           3     7\n# matching by multiple columns\n## one to one\ndf5 <- tibble(Species = c(\"A\", \"B\", \"C\"),\n              x = c(1, 2, 3),\n              z = c(\"cool\", \"awesome\", \"magical\"))\n\nleft_join(x = df1,\n          y = df5,\n          by = c(\"Species\", \"x\"))## # A tibble: 3 × 3\n##   Species     x z      \n##   <chr>   <dbl> <chr>  \n## 1 A           1 cool   \n## 2 B           2 awesome\n## 3 C           3 magical\n# matching by multiple columns\n## one to many\ndf6 <- tibble(Species = c(\"A\", \"A\", \"B\", \"C\"),\n              x = c(1, 1, 2, 3),\n              z = c(\"cool\", \"cool\", \"awesome\", \"magical\"))\n\nleft_join(x = df1,\n          y = df6,\n          by = c(\"Species\", \"x\"))## # A tibble: 4 × 3\n##   Species     x z      \n##   <chr>   <dbl> <chr>  \n## 1 A           1 cool   \n## 2 A           1 cool   \n## 3 B           2 awesome\n## 4 C           3 magical\n# matching by multiple columns\n## one to missing\ndf6 <- tibble(Species = c(\"A\", \"B\", \"C\"),\n              x = c(1, 2, 4),\n              z = c(\"cool\", \"awesome\", \"magical\"))\n\nleft_join(x = df1,\n          y = df6,\n          by = c(\"Species\", \"x\"))## # A tibble: 3 × 3\n##   Species     x z      \n##   <chr>   <dbl> <chr>  \n## 1 A           1 cool   \n## 2 B           2 awesome\n## 3 C           3 <NA>"},{"path":"visualization.html","id":"visualization","chapter":"6 Visualization","heading":"6 Visualization","text":"ggplot2:: offers range convenient functions data visualization. foundational function, ggplot(), provides initial framework adding supplementary layers using + operator. ggplot(), define variables plotted x- y-axis aes(). example:code display data points; rather, creates base frame plotting. may add additional point/line layers visualize data, shown . Please note aes() refers columns data frame. Variables names exist data frame used.information, see:ggplot2from Data Viz","code":"\n# without pipe\nggplot(data = iris,\n       mapping = aes(x = Sepal.Length,\n                     y = Sepal.Width))\n\n# with pipe\niris %>% \n  ggplot(mapping = aes(x = Sepal.Length,\n                       y = Sepal.Width))"},{"path":"visualization.html","id":"point","chapter":"6 Visualization","heading":"6.0.1 Point","text":"geom_point() : Add point layer","code":"\n# basic plot\niris %>% \n  ggplot(aes(x = Sepal.Length,\n             y = Sepal.Width)) +\n  geom_point()\n# change color by \"Species\" column\niris %>% \n  ggplot(aes(x = Sepal.Length,\n             y = Sepal.Width,\n             color = Species)) +\n  geom_point()"},{"path":"visualization.html","id":"line","chapter":"6 Visualization","heading":"6.0.2 Line","text":"geom_line() : Add line layer","code":"\n# sample data\ndf0 <- tibble(x = rep(1:50, 3),\n              y = x * 2)\n\n# basic plot\ndf0 %>% \n  ggplot(aes(x = x,\n             y = y)) +\n  geom_line()"},{"path":"visualization.html","id":"histogram","chapter":"6 Visualization","heading":"6.0.3 Histogram","text":"geom_histogram() : add histogram layer","code":"\n# basic plot; bins = 30 by default\niris %>% \n  ggplot(aes(x = Sepal.Length)) +\n  geom_histogram()\n# change bin width\niris %>% \n  ggplot(aes(x = Sepal.Length)) +\n  geom_histogram(binwidth = 0.5)\n# change bin number\niris %>% \n  ggplot(aes(x = Sepal.Length)) +\n  geom_histogram(bins = 50)"},{"path":"visualization.html","id":"boxplot","chapter":"6 Visualization","heading":"6.0.4 Boxplot","text":"geom_boxplot() : add boxplot layer","code":"\n# basic plot\niris %>% \n  ggplot(aes(x = Species,\n             y = Sepal.Length)) +\n  geom_boxplot()\n# change fill by \"Species\"\niris %>% \n  ggplot(aes(x = Species,\n             y = Sepal.Length,\n             fill = Species)) +\n  geom_boxplot()\n# change fill by \"Species\", but consistent color\niris %>% \n  ggplot(aes(x = Species,\n             y = Sepal.Length,\n             fill = Species)) +\n  geom_boxplot(color = \"darkgrey\")"},{"path":"descriptive-statistics.html","id":"descriptive-statistics","chapter":"7 Descriptive Statistics","heading":"7 Descriptive Statistics","text":"Descriptive statistics set summary measures provide concise overview dataset. help us understand characteristics properties data without delving complex statistical analyses. commonly used descriptive statistics include mean, standard deviation, median.illustrate concept, let’s consider fish length measurement \\(x\\). fish identified subscript, lengths denoted follows:\\[\nx_1 = 15.9\\\\\nx_2 = 15.1\\\\\nx_3 = 21.9\\\\\nx_4 = 13.3\\\\\nx_5 = 24.4\\\\\n\\]Often times, use subscript \\(\\) (character like) instead actual number indicate given data point. example, write fish length \\(x_i\\) individual \\(\\). Alternatively, instead writing individual data point, can represent vector using boldface, denoted \\(\\pmb{x}\\). case, vector \\(\\pmb{x}\\) can expressed :\\[\n\\pmb{x} = \\{15.9, 15.1, 21.9, 13.3, 24.4\\}\n\\]Now let’s see represent summary statistics vector \\(\\pmb{x}\\).","code":""},{"path":"descriptive-statistics.html","id":"central-tendency","chapter":"7 Descriptive Statistics","heading":"7.1 Central Tendency","text":"","code":""},{"path":"descriptive-statistics.html","id":"central-tendency-measures","chapter":"7 Descriptive Statistics","heading":"7.1.1 Central Tendency Measures","text":"Central tendency measures (Table 7.1) provide insights typical central value dataset. three commonly used measures:Arithmetic Mean. commonly used measure central tendency. represents additive average data. calculate arithmetic mean, sum values divide total number data points. can heavily influenced extreme values, known outliers.Arithmetic Mean. commonly used measure central tendency. represents additive average data. calculate arithmetic mean, sum values divide total number data points. can heavily influenced extreme values, known outliers.Geometric Mean. geometric mean multiplicative average. always smaller arithmetic mean less sensitive unusually large values. However, applicable data contain negative values.Geometric Mean. geometric mean multiplicative average. always smaller arithmetic mean less sensitive unusually large values. However, applicable data contain negative values.Median. median value separates higher half lower half dataset. represents 50th percentile data point. median less affected outliers compared arithmetic mean. calculate median, arrange data ascending order select middle value dataset odd number values. dataset even number values, take average two middle values.Median. median value separates higher half lower half dataset. represents 50th percentile data point. median less affected outliers compared arithmetic mean. calculate median, arrange data ascending order select middle value dataset odd number values. dataset even number values, take average two middle values.Table 7.1:  Common measures central tendency. \\(N\\) refers number data points.\\(x_{(\\frac{N + 1}{2})}\\) N odd\\(\\frac{1}{2}[x_{(\\frac{N}{2})} + x_{(\\frac{N}{2} + 1)}]\\) N even","code":""},{"path":"descriptive-statistics.html","id":"r-exercise","chapter":"7 Descriptive Statistics","heading":"7.1.2 R Exercise","text":"learn measures, let’s create vectors \\(\\pmb{x} = \\{15.9, 15.1, 21.9, 13.3, 24.4\\}\\) \\(\\pmb{y} = \\{15.9, 15.1, 21.9, 53.3, 24.4\\}\\) – \\(\\pmb{y}\\) identical \\(\\pmb{x}\\) contains one outlier value. make difference? construct vectors R, use c(), function stands “construct.” script:Confirm constructed correctly:Cool! Now can calculate summary statistics x y.Arithmetic Mean\\(\\frac{\\sum_i^N x_i}{N}\\)R function arithmetic mean(), let’s calculate value scratch:Compare outputs mean() :Geometric Mean\\((\\prod_i^N x_i)^{\\frac{1}{N}}\\)Unfortunately, build-function geometric mean \\(\\mu_{ge}\\) R (far know; packages though). , can calculate value scratch :Median\\(x_{(\\frac{N + 1}{2})}\\) N odd; \\(\\frac{1}{2}[x_{(\\frac{N}{2})} + x_{(\\frac{N}{2} + 1)}]\\) N evenLastly, let’s median:Compare outputs median()","code":"\n# construct vectors x and y\nx <- c(15.9, 15.1, 21.9, 13.3, 24.4)\ny <- c(15.9, 15.1, 21.9, 53.3, 24.4)\nx## [1] 15.9 15.1 21.9 13.3 24.4\ny## [1] 15.9 15.1 21.9 53.3 24.4\n# for vector x\nn_x <- length(x) # the number of elements in x = the number of data points\nsum_x <- sum(x) # summation for x\nmu_x <- sum_x / n_x # arithmetic mean\nprint(mu_x) # print calculated value## [1] 18.12\n# for vector y; we can calculate directly too\nmu_y <- sum(y) / length(y)\nprint(mu_y) # print calculated value## [1] 26.12\nprint(mean(x))## [1] 18.12\nprint(mean(y))## [1] 26.12\n# for vector x\nprod_x <- prod(x) # product of vector x; x1 * x2 * x3...\nn_x <- length(x)\nmug_x <- prod_x^(1 / n_x) # ^ means power\nprint(mug_x)## [1] 17.63648\n# for vector y\nmug_y <- prod(y)^(1 / length(y))\nprint(mug_y)## [1] 23.28022\n# for vector x\nx <- sort(x) # sort x from small to large\nindex <- (length(x) + 1) / 2 # (N + 1)/2 th index as length(x) is an odd number\nmed_x <- x[index]\nprint(med_x)## [1] 15.9\n# for vector y\ny <- sort(y) # sort y from small to large\nmed_y <- y[(length(y) + 1) / 2]\nprint(med_y)## [1] 21.9\nprint(median(x))## [1] 15.9\nprint(median(y))## [1] 21.9"},{"path":"descriptive-statistics.html","id":"variation","chapter":"7 Descriptive Statistics","heading":"7.2 Variation","text":"","code":""},{"path":"descriptive-statistics.html","id":"variation-measures","chapter":"7 Descriptive Statistics","heading":"7.2.1 Variation Measures","text":"Variation measures (Table 7.2) provide information spread data points.Variance. Variance statistical measure quantifies spread dispersion dataset. provides numerical value indicates far individual data points dataset deviate mean average value. words, variance measures average squared difference data point mean. Standard deviation (SD) square root variance.Variance. Variance statistical measure quantifies spread dispersion dataset. provides numerical value indicates far individual data points dataset deviate mean average value. words, variance measures average squared difference data point mean. Standard deviation (SD) square root variance.Inter-Quantile Range. interquartile range (IQR) statistical measure provides information spread dispersion dataset, specifically focusing middle 50% data. robust measure variability less affected outliers compared variance.Inter-Quantile Range. interquartile range (IQR) statistical measure provides information spread dispersion dataset, specifically focusing middle 50% data. robust measure variability less affected outliers compared variance.Median Absolute Deviation. Median Absolute Deviation (MAD) similar variance, provides robust estimation variability less affected outliers compared variance. MAD defined median absolute deviations data’s median.Median Absolute Deviation. Median Absolute Deviation (MAD) similar variance, provides robust estimation variability less affected outliers compared variance. MAD defined median absolute deviations data’s median.Coefficient Variation. coefficient variation (CV) statistical measure expresses relative variability dataset relation mean. particularly useful comparing variability different datasets different scales units measurement.Coefficient Variation. coefficient variation (CV) statistical measure expresses relative variability dataset relation mean. particularly useful comparing variability different datasets different scales units measurement.MAD/Median. MAD/Median statistical measure used assess relative variability dataset without assuming specific distribution parametric model. CV sensitive outliers reliance arithmetic mean. However, MAD/Median robust issue.MAD/Median. MAD/Median statistical measure used assess relative variability dataset without assuming specific distribution parametric model. CV sensitive outliers reliance arithmetic mean. However, MAD/Median robust issue.Table 7.2:  Metrics variation.","code":""},{"path":"descriptive-statistics.html","id":"r-exercise-1","chapter":"7 Descriptive Statistics","heading":"7.2.2 R Exercise","text":"Variance\\(\\frac{\\sum_i^N (x_i - \\mu)^2}{N}\\)Standard Deviation (SD)\\(\\sqrt{\\frac{\\sum_i^N (x_i - \\mu)^2}{N}}\\)IQR; , let use 25 75 percentiles \\(x_l\\) \\(x_h\\).\\(\\lvert x_l - x_h \\rvert\\)MAD\\(\\text{Median}(\\lvert x_i - \\mu_{med} \\rvert)\\)Coefficient Variation (CV)\\(\\frac{\\sigma}{\\mu}\\)MAD / Median\\(\\frac{\\text{MAD}}{\\mu_{med}}\\)","code":"\n# for x\nsqd_x <- (x - mean(x))^2 # sqared deviance\nsum_sqd_x <- sum(sqd_x)\nvar_x <- sum_sqd_x / length(x)\nprint(var_x)## [1] 18.2016\n# for y\nvar_y <- sum((y - mean(y))^2) / length(y)\nprint(var_y)## [1] 197.0816\n# for x\nsd_x <- sqrt(var_x) # sqrt(): square root\nprint(sd_x)## [1] 4.266333\n# for y\nsd_y <- sqrt(var_y)\nprint(sd_y)## [1] 14.03858\n# for x\nx_l <- quantile(x, 0.25) # quantile(): return quantile values, 25 percentile\nx_h <- quantile(x, 0.75) # quantile(): return quantile values, 75 percentile\niqr_x <- abs(x_l - x_h) # abs(): absolute value\nprint(iqr_x)## 25% \n## 6.8\n# for y\ny_q <- quantile(y, c(0.25, 0.75)) # return as a vector\niqr_y <- abs(y_q[1] - y_q[2]) # y_q[1] = 25 percentile; y_q[2] = 75 percentile\nprint(iqr_y)## 25% \n## 8.5\n# for x\nad_x <- abs(x - median(x))\nmad_x <- median(ad_x)\nprint(mad_x)## [1] 2.6\n# for y\nmad_y <- median(abs(y - median(y)))\nprint(mad_y)## [1] 6\n# for x \ncv_x <- sd_x / mean(x) \nprint(cv_x)  ## [1] 0.2354489\n# for y \ncv_y <- sd_y / mean(y) \nprint(cv_y)## [1] 0.5374646\n# for x\nmm_x <- mad_x / median(x)\nprint(mm_x)## [1] 0.163522\n# for y\nmm_y <- mad_y / median(y)\nprint(mm_y)## [1] 0.2739726"},{"path":"descriptive-statistics.html","id":"laboratory","chapter":"7 Descriptive Statistics","heading":"7.3 Laboratory","text":"","code":""},{"path":"descriptive-statistics.html","id":"comparing-central-tendency-measures","chapter":"7 Descriptive Statistics","heading":"7.3.1 Comparing Central Tendency Measures","text":"differences three measures central tendency? investigate , let’s perform following exercise.Create new vector z length \\(100\\) exp(rnorm(n = 100, mean = 0, sd = 0.1)), calculate arithmetic mean, geometric mean, median z.Create new vector z length \\(100\\) exp(rnorm(n = 100, mean = 0, sd = 0.1)), calculate arithmetic mean, geometric mean, median z.Draw histogram z using functions tibble(), ggplot(), geom_histogram().Draw histogram z using functions tibble(), ggplot(), geom_histogram().Draw vertical lines arithmetic mean, geometric mean, median histogram different colors using function geom_vline() .Draw vertical lines arithmetic mean, geometric mean, median histogram different colors using function geom_vline() .Visually compare values central tendency measures vertical lines drawn geom_vline().Visually compare values central tendency measures vertical lines drawn geom_vline().Create new vector z_rev -z + max(z) + 0.5, repeat step 1 – 4.Create new vector z_rev -z + max(z) + 0.5, repeat step 1 – 4.","code":""},{"path":"descriptive-statistics.html","id":"comparing-variation-measures","chapter":"7 Descriptive Statistics","heading":"7.3.2 Comparing Variation Measures","text":"absolute (variance, SD, MAD, IQR) relative measures (CV, MAD/Median) variation? understand , suppose 100 measurements fish weight unit “gram.” (w following script)Using data, perform following exercise:Convert unit w “milligram” create new vector m.Convert unit w “milligram” create new vector m.Calculate SD MAD w m.Calculate SD MAD w m.Calculate CV MAD/Median w m.Calculate CV MAD/Median w m.","code":"\nw <- rnorm(100, mean = 10, sd = 1)\nhead(w) # show first 10 elements in w## [1]  8.734939  9.313147  9.554338 11.224082 10.359814 10.400771"},{"path":"sampling.html","id":"sampling","chapter":"8 Sampling","heading":"8 Sampling","text":"“need statistics first place?” initial question arose entered field ecology. Initially, assumed straightforward query immediate response. However, soon realized profound question complex answer. short, “need statistics often possess partial information seek understand.” Now, let’s explore elaborate explanation .Key words: parameter, sample mean, sample variance, biased/unbiased","code":""},{"path":"sampling.html","id":"the-unknown-garden-plant-example","chapter":"8 Sampling","heading":"8.1 The Unknown: Garden Plant Example","text":"Consider scenario conducting study plant height garden. garden, exists thousand individual plants, making impractical single researcher measure . Instead, due resource limitations, sample \\(10\\) plants selected calculate average height extent variation among plant individuals:Cool. Let’s use data set learn pitfall behind . Create vector plant height h put tibble() analyze :format (tibble()) better raw vector height allows flexible analysis. Let’s add columns mu_height var_height using mutate(), function adds new column(s) existing tibble() (data.frame()):Awesome, able get average height variance! – however, confident ? obtained plant height 10…1000. different measure another set 10 plant individuals? Let’s see:Create another tibble() :Wow, ’s totally different.","code":"## # A tibble: 10 × 4\n##     ...1 plant_id height unit \n##    <dbl>    <dbl>  <dbl> <chr>\n##  1     1        1   16.9 cm   \n##  2     2        2   20.9 cm   \n##  3     3        3   15.8 cm   \n##  4     4        4   28   cm   \n##  5     5        5   21.6 cm   \n##  6     6        6   15.9 cm   \n##  7     7        7   22.4 cm   \n##  8     8        8   23.7 cm   \n##  9     9        9   22.9 cm   \n## 10    10       10   18.5 cm\nh <- c(16.9, 20.9, 15.8, 28, 21.6, 15.9, 22.4, 23.7, 22.9, 18.5)\n\ndf_h1 <- tibble(plant_id = 1:10, # a vector from 1 to 10 by 1\n                height = h, # height\n                unit = \"cm\") # unit\n# nrow() returns the number of rows\n# while piping, \".\" refers to the dataframe inherited \n# i.e., nrow(.) counts the number of rows in df_h1\ndf_h1 <- df_h1 %>% \n  mutate(mu_height = mean(height),\n         var_height = sum((height - mu_height)^2) / nrow(.))\n\nprint(df_h1)## # A tibble: 10 × 5\n##    plant_id height unit  mu_height var_height\n##       <int>  <dbl> <chr>     <dbl>      <dbl>\n##  1        1   16.9 cm         20.7       13.7\n##  2        2   20.9 cm         20.7       13.7\n##  3        3   15.8 cm         20.7       13.7\n##  4        4   28   cm         20.7       13.7\n##  5        5   21.6 cm         20.7       13.7\n##  6        6   15.9 cm         20.7       13.7\n##  7        7   22.4 cm         20.7       13.7\n##  8        8   23.7 cm         20.7       13.7\n##  9        9   22.9 cm         20.7       13.7\n## 10       10   18.5 cm         20.7       13.7## # A tibble: 10 × 4\n##     ...1 plant_id height unit \n##    <dbl>    <dbl>  <dbl> <chr>\n##  1    11       11   27.6 cm   \n##  2    12       12   21.9 cm   \n##  3    13       13   16.9 cm   \n##  4    14       14    8.9 cm   \n##  5    15       15   25.6 cm   \n##  6    16       16   19.8 cm   \n##  7    17       17   19.9 cm   \n##  8    18       18   24.7 cm   \n##  9    19       19   24.1 cm   \n## 10    20       20   23   cm\nh <- c(27.6, 21.9, 16.9, 8.9, 25.6, 19.8, 19.9, 24.7, 24.1, 23)\n\ndf_h2 <- tibble(plant_id = 11:20, # a vector from 11 to 20 by 1\n                height = h,\n                unit = \"cm\") %>% \n  mutate(mu_height = mean(height),\n         var_height = sum((height - mu_height)^2) / nrow(.))\n\nprint(df_h2)## # A tibble: 10 × 5\n##    plant_id height unit  mu_height var_height\n##       <int>  <dbl> <chr>     <dbl>      <dbl>\n##  1       11   27.6 cm         21.2       25.8\n##  2       12   21.9 cm         21.2       25.8\n##  3       13   16.9 cm         21.2       25.8\n##  4       14    8.9 cm         21.2       25.8\n##  5       15   25.6 cm         21.2       25.8\n##  6       16   19.8 cm         21.2       25.8\n##  7       17   19.9 cm         21.2       25.8\n##  8       18   24.7 cm         21.2       25.8\n##  9       19   24.1 cm         21.2       25.8\n## 10       20   23   cm         21.2       25.8"},{"path":"sampling.html","id":"linking-part-to-the-whole","chapter":"8 Sampling","heading":"8.2 Linking Part to the Whole","text":"exercise highlights important takeaway: can determine data average variance sample, may perfectly represent characteristics entire garden.field biological research, often impractical impossible sample entire population, must rely estimating unknowns (case, mean variance) available samples. statistics comes play, offering tool infer information entire population based partial information obtained samples.unknowns interested , population mean variance example, referred “parameters.” parameters directly measured can estimated samples statistical inference.Provided certain assumptions met, sample mean unbiased point estimate population mean. “unbiased” means sample means – repeat sampling process – centered around population mean. meantime, sample variance – use formula Chapter 7 – “biased.” tends smaller population variance.Let’s explore concept simple simulations. Suppose data thousand plant individuals, although scenario may unrealistic practice. However, conducting simulations, can examine different sample means variances can deviate true values.Download data containing height measurements thousand individuals, place file data_raw/ project directory. can load csv file R follows:Using synthetic dataset (generated random value generator), can calculate true mean variance (reference values). important note case, use term “calculate” mean variance represent parameters entire population, known us scenario.can simulate sampling 10 plant individuals randomly selecting 10 rows df_h0:Since sample_n() selects rows randomly, (likely) get different set 10 individuals/rows every single time. another set 10 rows (notice df_i overwritten new data set):Let’s obtain 100 sets 10 plant individuals (randomly selected) estimate mean variance . can perform random sampling one one, cumbersome – least, want . Instead, can leverage technique loop:Take look mu_i var_i :element mu_i var_i, saved estimated mean (\\(\\hat{\\mu}\\); reads mu hat) variance (\\(\\hat{\\sigma}^2\\)) 10 plant height measures dataset . drawing histogram values, can examine distributions mean variance estimates. use R package patchwork make better figure:\nFigure 8.1: Sample mean biased variance.\nsample means indeed symmetrically distributed around true mean, sample variances tend biased skewed right, often underestimating true variance.bias estimating variance arises due inferring parameter small number samples. However, good news: unbiased estimator variance exists. formula unbiased estimator variance follows:\\[\n\\frac{\\sum_i^N (x_i - \\mu)^2}{N-1}\n\\]correction denominator (\\(N\\) replaced \\(N-1\\)) compensates bias, providing estimate true variance without systematic underestimation (although seems simple correction, deep math underlies derivation \\(N-1\\)). default formula var() R, function used estimate unbiased variance (unbiased SD sd()). Comparison reveals works:Add histogram unbiased variance:\nFigure 8.2: Comparison biased unbiased variances.\nsummary, samples can provide information part whole population. complete picture entire population often unknown, rely estimating key parameters available samples. concept applies wide range parametric analyses statistics, use sample data make inferences population parameters.Recognizing limitations uncertainties associated working samples essential proper statistical analysis interpretation results various fields study.","code":"\n# load csv data on R\ndf_h0 <- read_csv(\"data_raw/data_plant_height.csv\")\n\n# show the first 10 rows\nprint(df_h0)## # A tibble: 1,000 × 4\n##     ...1 plant_id height unit \n##    <dbl>    <dbl>  <dbl> <chr>\n##  1     1        1   16.9 cm   \n##  2     2        2   20.9 cm   \n##  3     3        3   15.8 cm   \n##  4     4        4   28   cm   \n##  5     5        5   21.6 cm   \n##  6     6        6   15.9 cm   \n##  7     7        7   22.4 cm   \n##  8     8        8   23.7 cm   \n##  9     9        9   22.9 cm   \n## 10    10       10   18.5 cm   \n## # ℹ 990 more rows\nmu <- mean(df_h0$height)\nsigma2 <- sum((df_h0$height - mu)^2) / nrow(df_h0)\n\nprint(mu)## [1] 19.9426\nprint(sigma2)## [1] 26.74083\ndf_i <- df_h0 %>% \n  sample_n(size = 10) # size specifies the number of rows to be selected randomly\n\nprint(df_i)## # A tibble: 10 × 4\n##     ...1 plant_id height unit \n##    <dbl>    <dbl>  <dbl> <chr>\n##  1    52       52   16.9 cm   \n##  2   876      876   20.9 cm   \n##  3   534      534   17.7 cm   \n##  4   177      177   23.9 cm   \n##  5   554      554   18   cm   \n##  6   827      827   13.7 cm   \n##  7    84       84   12.4 cm   \n##  8   523      523   15.6 cm   \n##  9   633      633   16.3 cm   \n## 10   951      951   26   cm\ndf_i <- df_h0 %>% \n  sample_n(size = 10)\n\nprint(df_i)## # A tibble: 10 × 4\n##     ...1 plant_id height unit \n##    <dbl>    <dbl>  <dbl> <chr>\n##  1   392      392   23.1 cm   \n##  2   302      302   14.8 cm   \n##  3   597      597   15.9 cm   \n##  4   877      877   20.9 cm   \n##  5   706      706   13.3 cm   \n##  6   619      619   18.8 cm   \n##  7   589      589   21.9 cm   \n##  8   430      430   16.6 cm   \n##  9   710      710   28.8 cm   \n## 10   761      761   19.9 cm\n# for reproducibility\nset.seed(3)\n\nmu_i <- var_i <- NULL # create empty objects\n\n# repeat the work in {} from i = 1 to i = 100\nfor (i in 1:100) {\n  \n  df_i <- df_h0 %>% \n    sample_n(size = 10) # random samples of 10 individuals\n  \n  # save mean for sample set i\n  mu_i[i] <- mean(df_i$height)\n  \n  # save variance for sample set i\n  var_i[i] <- sum((df_i$height - mean(df_i$height))^2) / nrow(df_i) \n  \n}\nprint(mu_i)##   [1] 19.97 17.86 22.55 22.03 17.00 23.28 21.33 21.23 18.55 19.29 22.14 19.84\n##  [13] 22.43 22.13 21.13 19.40 19.39 20.43 19.12 20.66 21.01 19.48 21.73 19.63\n##  [25] 21.03 20.60 21.11 20.42 18.76 23.70 20.31 22.22 21.34 20.70 20.96 20.03\n##  [37] 21.77 19.19 19.87 21.38 19.64 23.31 19.89 19.21 19.68 19.54 17.54 19.05\n##  [49] 18.91 20.57 18.33 18.07 19.48 17.70 20.24 17.74 20.45 16.48 18.93 17.60\n##  [61] 17.23 20.75 18.06 20.06 20.80 21.72 19.02 25.08 18.90 20.69 23.28 20.87\n##  [73] 18.65 19.74 21.47 17.95 16.98 18.30 19.77 17.25 19.60 21.27 19.28 20.42\n##  [85] 19.60 18.41 20.15 21.24 19.70 21.56 20.75 19.54 17.54 18.52 19.85 18.40\n##  [97] 20.39 17.07 17.84 20.66\nprint(var_i)##   [1] 14.1961 21.6884 28.2685 21.6341  6.5700 24.5996 23.6941 24.8681 21.6485\n##  [10] 36.3689 17.8844 31.5784 21.0521 34.2341 20.6281 10.4420 20.0469 31.2781\n##  [19] 32.0316 23.0964  9.8789 31.8916 28.3061 19.6861  8.2641 23.3640 26.8769\n##  [28] 48.9416 27.7644 42.4220 38.4989 12.7076 15.9004  8.6740 11.7284 26.2061\n##  [37] 21.7461 25.9269 28.0421  6.2616 16.5584 27.7489 17.3609 23.1349 38.6936\n##  [46] 25.0984 13.5864 25.2625  7.7149 17.5341 11.4681 38.1561 12.6376 42.5480\n##  [55] 31.9224 13.9824 14.9725 25.8296 33.6781  8.2440 31.4741 29.7805 26.7324\n##  [64] 47.3704 26.8660 24.0536 29.5076 23.7376 14.8000 51.8449 13.3536 18.5581\n##  [73] 18.9185 12.4544 36.2201 16.3225 17.9236 42.8760 18.5421 22.0205 34.1680\n##  [82] 28.8841 26.6976 22.7356 35.2900 19.2229 34.3905 17.3784 23.1880 42.4264\n##  [91] 18.5365 18.5424 21.5204 13.1276 17.7185 21.2940 27.1649 35.3141 43.5624\n## [100] 55.1484\n#install.packages(\"patchwork\") # install only once\nlibrary(patchwork)\n\ndf_sample <- tibble(mu_hat = mu_i, var_hat = var_i)\n\n# histogram for mean\ng_mu <- df_sample %>% \n  ggplot(aes(x = mu_hat)) +\n  geom_histogram() +\n  geom_vline(xintercept = mu)\n\n# histogram for variance\ng_var <- df_sample %>% \n  ggplot(aes(x = var_hat)) +\n  geom_histogram() +\n  geom_vline(xintercept = sigma2)\n\n# layout vertically\n# possible only if \"patchwork\" is loaded\ng_mu / g_var\n# for reproducibility\nset.seed(3)\n\n# redo simulations ----\nmu_i <- var_i <- var_ub_i <- NULL # create empty objects\n\n# repeat the work in {} from i = 1 to i = 100\nfor (i in 1:100) {\n  \n  df_i <- df_h0 %>% \n    sample_n(size = 10) # random samples of 10 individuals\n  \n  # save mean for sample set i\n  mu_i[i] <- mean(df_i$height)\n  \n  # save variance for sample set i\n  var_i[i] <- sum((df_i$height - mean(df_i$height))^2) / nrow(df_i) \n  \n  var_ub_i[i] <- var(df_i$height)\n}\n# draw histograms ----\ndf_sample <- tibble(mu_hat = mu_i,\n                    var_hat = var_i,\n                    var_ub_hat = var_ub_i)\n\n# histogram for mu\ng_mu <- df_sample %>% \n  ggplot(aes(x = mu_hat)) +\n  geom_histogram() +\n  geom_vline(xintercept = mu)\n\n# histogram for variance\n# scale_x_continuous() adjusts scale in x-axis\ng_var <- df_sample %>% \n  ggplot(aes(x = var_hat)) +\n  geom_histogram() +\n  geom_vline(xintercept = sigma2) +\n  scale_x_continuous(limits= c(min(c(var_i, var_ub_i)),\n                               max(c(var_i, var_ub_i))))\n\n# histogram for unbiased variance\ng_var_ub <- df_sample %>% \n  ggplot(aes(x = var_ub_hat)) +\n  geom_histogram() +\n  geom_vline(xintercept = sigma2) +\n  scale_x_continuous(limits= c(min(c(var_i, var_ub_i)),\n                               max(c(var_i, var_ub_i))))\n\ng_mu / g_var / g_var_ub"},{"path":"sampling.html","id":"laboratory-1","chapter":"8 Sampling","heading":"8.3 Laboratory","text":"used 10 plants estimate sample means variances. Obtain 100 sub-datasets 50 100 measures , draw histograms sample means unbiased variances (use var()).used 10 plants estimate sample means variances. Obtain 100 sub-datasets 50 100 measures , draw histograms sample means unbiased variances (use var()).Sample means unbiased variances unbiased samples randomly selected. happens samples non-random? Suppose investigator unable find plants less 10 cm height – following code excludes less 10 cm height:\n\ndf_h10 <- df_h0 %>% \n  filter(height >= 10)\nRepeat step 1 df_h10 instead df_h0 compare results.Sample means unbiased variances unbiased samples randomly selected. happens samples non-random? Suppose investigator unable find plants less 10 cm height – following code excludes less 10 cm height:Repeat step 1 df_h10 instead df_h0 compare results.","code":"\ndf_h10 <- df_h0 %>% \n  filter(height >= 10)"},{"path":"probabilistic-view.html","id":"probabilistic-view","chapter":"9 Probabilistic View","heading":"9 Probabilistic View","text":"Chapter 8 emphasized concept sampling introduced crucial aspect statistics: randomness. Although mean represents central tendency data, encompass data points. Inevitable deviations occur, need way express “randomness.” concept probability distributions aids fully understanding stochastic nature observed data.Key words: probability density function (PDF), probability mass function (PMF), expected value","code":""},{"path":"probabilistic-view.html","id":"continuous-variable","chapter":"9 Probabilistic View","heading":"9.1 Continuous Variable","text":"","code":""},{"path":"probabilistic-view.html","id":"probability-density-function","chapter":"9 Probabilistic View","heading":"9.1.1 Probability Density Function","text":"Let use plant height example Chapter 8:Chapter 8 primarily focused exploring mean variance, let’s broaden perspective gain comprehensive understanding entire distribution height.\nFigure 9.1: Distribution plant height\ndistribution comprises thousand height measurements. However, certain patterns consider. data centered around mean exhibits symmetrical distribution. characteristic implies distribution can approximated simple formula relies key parameters.statistics, symmetrical bell-shaped form commonly approximated Normal distribution, often denoted “variable \\(x\\) assumed follow Normal distribution.” express using following mathematical expression:\\[\nx \\sim \\mbox{Normal}(\\mu, \\sigma^2)\n\\]Unlike “equation,” use “\\(=\\)” sign variable \\(x\\) equivalent Normal distribution. Rather, represents “stochastic” relationship signifies probability \\(x\\) taking specific range values.Normal distribution characterized two parameters: mean variance. Strictly speaking, terms “mean” “variance” probability distribution differ “sample mean” “sample variance,” although converge variable \\(x\\) assumed follow Normal distribution (refer Chapter 15 details). precisely, mean referred “expected value” denoted \\(\\mbox{E}(x)\\). variance represents expected value \\(x^2\\) denoted \\(\\mbox{E}(x^2)\\). expected value serves central tendency random variable, indicating values likely occur. hand, variance quantifies extent individual data points deviate expected value. Although use terms mean variance simplicity, important recognize distinction.parameters determined, formula defines Normal distribution yield probability density given range values. formula \\(f(x)\\) known probability density function (PDF) displayed :\\[\nf(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n\\]Well, understanding meaning formula can challenging without context. However, visualization can useful tool help comprehend mathematical expressions.","code":"\n# load csv data on R\ndf_h0 <- read_csv(\"data_raw/data_plant_height.csv\")\ndf_h0 %>% \n  ggplot(aes(x = height)) + \n  geom_histogram(binwidth = 1, # specify bin width\n                 center = 0.5) + # bin's center specification\n  geom_vline(aes(xintercept = mean(height))) # draw vertical line at the mean"},{"path":"probabilistic-view.html","id":"pdf-to-frequency-distribution","chapter":"9 Probabilistic View","heading":"9.1.2 PDF to frequency distribution","text":"R, function dnorm() can used calculate probability density given value. first argument, x, vector values want calculate probability density. second argument, mean, third argument, sd, correspond mean standard deviation distribution, respectively. Note must provide mean sd calculate probability density.encompass entire range observed heights, can use \\(\\text{min}(x)\\) \\(\\text{max}(x)\\) lower upper limits, respectively.\nFigure 9.2: Probability density function Normal distribution\nshape curve appears quite similar observed; however, scale y-axis different. y-axis represents “probability density.” convert actual “probability,” need calculate area curve. R, can utilize pnorm() function purpose. calculates probability variable less specified value, provided first argument q.make estimates comparable frequency data, can calculate probability 1 cm bin. expected frequency can obtained multiplying probability sample size, case 1000. allows estimate number observations expect 1 cm bin based calculated probabilities.Overlay:\nFigure 9.3: Histogram overlaid predicted frequency (red dots line)\nremarkable probability distribution, characterized just two parameters (mean variance), can effectively reproduce overall shape observed original data set 1000 data points. great news means determine key parameters, can infer general pattern data.","code":"\n# vector of x values\n# seq() generate min to max values with specified numbers of elements or interval\n# the following produce 100 elements\nx <- seq(min(df_h0$height), max(df_h0$height), length = 100)\n\n# calculate probability density\nmu <- mean(df_h0$height)\nsigma <- sd(df_h0$height)\npd <- dnorm(x, mean = mu, sd = sigma)\n\n# figure\ntibble(y = pd, x = x) %>% # data frame\n  ggplot(aes(x = x, y = y)) +\n  geom_line() + # draw lines\n  labs(y = \"Probability density\") # re-label\n# probability of x < 10\np10 <- pnorm(q = 10, mean = mu, sd = sigma)\nprint(p10)## [1] 0.02731905\n# probability of x < 20\np20 <- pnorm(q = 20, mean = mu, sd = sigma)\nprint(p20)## [1] 0.504426\n# probability of 10 < x < 20\np20_10 <- p20 - p10\nprint(p20_10)## [1] 0.4771069\nx_min <- floor(min(df_h0$height)) # floor takes the integer part of the value\nx_max <- ceiling(max(df_h0$height)) # ceiling takes the next closest integer\nbin <- seq(x_min, x_max, by = 1) # each bin has 1cm\n\np <- NULL # empty object for probability\nfor (i in 1:(length(bin) - 1)) {\n  p[i] <- pnorm(bin[i+1], mean = mu, sd = sigma) - pnorm(bin[i], mean = mu, sd = sigma)\n}\n\n# data frame for probability\n# bin: last element [-length(bin)] was removed to match length\n# expected frequency in each bin is \"prob times sample size\"\n# \"+ 0.5\" was added to represent a midpoint in each bin\ndf_prob <- tibble(p, bin = bin[-length(bin)] + 0.5) %>% \n  mutate(freq = p * nrow(df_h0))\ndf_h0 %>% \n  ggplot(aes(x = height)) + \n  geom_histogram(binwidth = 1, # specify bin width; must match the bin width used for probability\n                 center = 0.5) + # bin's center position\n  geom_point(data = df_prob,\n             aes(y = freq,\n                 x = bin),\n             color = \"salmon\") +\n  geom_line(data = df_prob,\n            aes(y = freq,\n                x = bin),\n            color = \"salmon\")"},{"path":"probabilistic-view.html","id":"discrete-variable","chapter":"9 Probabilistic View","heading":"9.2 Discrete Variable","text":"","code":""},{"path":"probabilistic-view.html","id":"probability-mass-function","chapter":"9 Probabilistic View","heading":"9.2.1 Probability Mass Function","text":"Let’s shift perspective examine density plants garden. analysis, counted number plant individuals within 30 plots, plot covering area one square meter (refer Figure 9.4).\nFigure 9.4: Garden view sampling plots. White squares represent plots. Red dots indicate plant individuals counted.\nDownload data load onto R:Make histogram:\nFigure 9.5: Histogram plant individuals per plot\nseveral significant differences compared plant height example considering density plants garden:possible values discrete (count data) since counting number plant individuals plot.possible values discrete (count data) since counting number plant individuals plot.possible values always positive, counts negative.possible values always positive, counts negative.distribution appears non-symmetric around sample mean.distribution appears non-symmetric around sample mean.Given characteristics, Normal distribution may appropriate choice representing variable. Instead, suitable use Poisson distribution characterize observed distribution discrete variable.\\[\nx \\sim \\mbox{Poisson}(\\lambda)\n\\]Poisson distribution, mean parameter (\\(\\lambda\\)) serves sole parameter2. Probability distributions describe discrete variables expressed using probability mass function (PMF):\\[\ng(x) = \\frac{\\lambda^x \\exp(-\\lambda)}{x!}\n\\]Unlike probability density function (PDF), probability mass function (PMF) represents probability discrete variable \\(x\\) taking specific value. instance, probability \\(x=2\\), denoted \\(\\Pr(x=2)\\), can calculated follows:\\[\n\\Pr(x=2) = g(2) = \\frac{\\lambda^2 \\exp(-\\lambda)}{2!}\n\\]generalized form, write:\\[\n\\Pr(x=k) = g(k) = \\frac{\\lambda^k \\exp(-\\lambda)}{k!}\n\\]","code":"\ndf_count <- read_csv(\"data_raw/data_garden_count.csv\")\nprint(df_count)## # A tibble: 30 × 3\n##     plot count nitrate\n##    <dbl> <dbl>   <dbl>\n##  1     1     3    31.3\n##  2     2     1    27.4\n##  3     3     4    31.9\n##  4     4     1    30.1\n##  5     5     4    29.8\n##  6     6     1    30.4\n##  7     7     2    32  \n##  8     8     0    18.3\n##  9     9     2    31.7\n## 10    10     2    34  \n## # ℹ 20 more rows\ndf_count %>% \n  ggplot(aes(x = count)) +\n  geom_histogram(binwidth = 0.5, # define binwidth\n                 center = 0) # relative position of each bin"},{"path":"probabilistic-view.html","id":"pmf-to-frequency-distribution","chapter":"9 Probabilistic View","heading":"9.2.2 PMF to frequency distribution","text":"R, can use dpois() function visualize Poisson distribution. function allows plot probability mass function (PMF) Poisson distribution gain insights distribution discrete variable. given example, utilize sample mean estimate \\(\\lambda\\) — somewhat counter-intuitive approach since \\(x\\) conform Normal distribution. However, delve detailed explanation Chapter 15, allowing assume validity time .\nFigure 9.6: Example Poisson distribution\nconvert y-axis probability frequency, multiply probabilities sample size (total number observations) obtain expected frequency. plant height example, can plot expected frequency histogram, providing visual representation distribution discrete variable.\nFigure 9.7: Observed histogram discrete variable overlaid Poisson expectation.\nCool, Poisson distribution excellent job characterizing distribution plant counts.","code":"\n# vector of x values\n# create a vector of 0 to 10 with an interval one\n# must be integer of > 0\nx <- seq(0, 10, by = 1)\n\n# calculate probability mass\nlambda_hat <- mean(df_count$count)\npm <- dpois(x, lambda = lambda_hat)\n\n# figure\ntibble(y = pm, x = x) %>% # data frame\n  ggplot(aes(x = x, y = y)) +\n  geom_line(linetype = \"dashed\") + # draw dashed lines\n  geom_point() + # draw points\n  labs(y = \"Probability\",\n       x = \"Count\") # re-label\ndf_prob <- tibble(x = x, y = pm) %>% \n  mutate(freq = y * nrow(df_count)) # prob x sample size\n\ndf_count %>% \n  ggplot(aes(x = count)) +\n  geom_histogram(binwidth = 0.5, # must be divisible number of one; e.g., 0.1, 0.25, 0.5...\n                 center = 0) +\n  geom_line(data = df_prob,\n            aes(x = x,\n                y = freq),\n            linetype = \"dashed\") +\n  geom_point(data = df_prob,\n             aes(x = x,\n                y = freq))"},{"path":"probabilistic-view.html","id":"why-probability-distributions","chapter":"9 Probabilistic View","heading":"9.3 Why Probability Distributions?","text":"seen examples, probability distributions provide framework describing likelihood different outcomes events. utilizing probability distributions, can:Make predictions estimate probabilities specific events ranges values.Determine likely values outcomes based available information.Quantify analyze uncertainty associated random variables processes.Probability distributions serve fundamental tools statistics, enabling us model, analyze, make informed decisions face uncertainty.","code":""},{"path":"probabilistic-view.html","id":"laboratory-2","chapter":"9 Probabilistic View","heading":"9.4 Laboratory","text":"","code":""},{"path":"probabilistic-view.html","id":"normal-distribution","chapter":"9 Probabilistic View","heading":"9.4.1 Normal Distribution","text":"function rnorm() produces random variable follows Normal distribution specified mean SD. Using function,Generate variable 50 observations.Create figure similar Figure 9.3","code":""},{"path":"probabilistic-view.html","id":"poisson-distribution","chapter":"9 Probabilistic View","heading":"9.4.2 Poisson Distribution","text":"function rpois() produces random variable follows Poisson distribution specified mean. Using function,Generate variable 1000 observations.Create figure similar Figure 9.7","code":""},{"path":"two-group-comparison.html","id":"two-group-comparison","chapter":"10 Two-Group Comparison","heading":"10 Two-Group Comparison","text":"Data consists “samples” extracted population interest, ’s important acknowledge samples flawless replicas entire population. Given imperfect information, can effectively investigate discern distinction two populations? Chapter, introduce one basic statistical tests – “t-test.” t-test takes following steps:Define test statistic signify difference groups (t-statistic).Define probability distribution t-statistic null hypothesis, .e., scenario assume difference groups.Estimate probability yielding greater less observed test statistic null hypothesis.Key words: t-statistic, null hypothesis, alternative hypothesis, p-value","code":""},{"path":"two-group-comparison.html","id":"explore-data-structure","chapter":"10 Two-Group Comparison","heading":"10.1 Explore Data Structure","text":"Suppose study fish populations species two lakes (Lake b). lakes stark contrast productivity (Lake b looks productive), interest difference mean body size two lakes. obtained \\(50\\) data points fish length lake (download ). Save data_raw/ use read_csv() read data:data frame, fish length data lake b recorded. Confirm unique() distinct() function:Visualization provides powerful tool summarizing data effectively. plotting individual data points overlaid mean values error bars, can observe distribution patterns within data, allowing us identify trends, variations, potential outliers:\nFigure 10.1: Example mean SD plot\nHmm, possible noticeable disparity body size Lake Lake b. However, can ascertain provide evidence claim?","code":"\nlibrary(tidyverse) # call add-in packages everytime you open new R session\ndf_fl <- read_csv(\"data_raw/data_fish_length.csv\")\nprint(df_fl)## # A tibble: 100 × 3\n##    lake  length unit \n##    <chr>  <dbl> <chr>\n##  1 a       10.8 cm   \n##  2 a       13.6 cm   \n##  3 a       10.1 cm   \n##  4 a       18.6 cm   \n##  5 a       14.2 cm   \n##  6 a       10.1 cm   \n##  7 a       14.7 cm   \n##  8 a       15.6 cm   \n##  9 a       15   cm   \n## 10 a       11.9 cm   \n## # ℹ 90 more rows\n# unique returns unique values as a vector\nunique(df_fl$lake)## [1] \"a\" \"b\"\n# distinct returns unique values as a tibble\ndistinct(df_fl, lake)## # A tibble: 2 × 1\n##   lake \n##   <chr>\n## 1 a    \n## 2 b\n# group mean and sd\ndf_fl_mu <- df_fl %>% \n  group_by(lake) %>% # group operation\n  summarize(mu_l = mean(length), # summarize by mean()\n            sd_l = sd(length)) # summarize with sd()\n\n# plot\n# geom_jitter() plot data points with scatter\n# geom_segment() draw lines\n# geom_point() draw points\ndf_fl %>% \n  ggplot(aes(x = lake,\n             y = length)) +\n  geom_jitter(width = 0.1, # scatter width\n              height = 0, # scatter height (no scatter with zero)\n              alpha = 0.25) + # transparency of data points\n  geom_segment(data = df_fl_mu, # switch data frame\n               aes(x = lake,\n                   xend = lake,\n                   y = mu_l - sd_l,\n                   yend = mu_l + sd_l)) +\n  geom_point(data = df_fl_mu, # switch data frame\n             aes(x = lake,\n                 y = mu_l),\n             size = 3) +\n  labs(x = \"Lake\", # x label\n       y = \"Fish body length\") # y label"},{"path":"two-group-comparison.html","id":"t-test-in-r","chapter":"10 Two-Group Comparison","heading":"10.2 t-test in R","text":"simple data set, can use t-test. R provides t.test() function perform analysis:reported t-statistic t = -3.2473, p-value p-value = 0.001596, degrees freedom agreed (df = 98) – however, values mean? supposed interpret t = -3.2473 p-value = 0.001596?","code":"\nx <- df_fl %>%\n  filter(lake == \"a\") %>%  # subset lake a\n  pull(length)\n\ny <- df_fl %>%\n  filter(lake == \"b\") %>% # subset lake b\n  pull(length)\n\nt.test(x, y, var.equal = TRUE)## \n##  Two Sample t-test\n## \n## data:  x and y\n## t = -3.2473, df = 98, p-value = 0.001596\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -3.3124399 -0.7995601\n## sample estimates:\n## mean of x mean of y \n##    13.350    15.406"},{"path":"two-group-comparison.html","id":"test-statistic","chapter":"10 Two-Group Comparison","heading":"10.3 Test Statistic","text":"","code":""},{"path":"two-group-comparison.html","id":"t-statistic","chapter":"10 Two-Group Comparison","heading":"10.3.1 t-statistic","text":"starting point define want look . Since focus examining difference means, logical estimate disparity sample means lake. Let denote sample means \\(\\hat{\\mu}_a\\) \\(\\hat{\\mu}_b\\) Lake Lake b, respectively. can estimate difference means using following approach:average fish body size Lake b approximately 2 cm larger Lake . However, crucial recognize still lacking vital information, specifically, uncertainty associated difference. discussed Chapter 8, must acknowledge sample means flawless representations entire population.Fortunately, can utilize sample variances address uncertainties. t-statistic common indicator difference means takes consideration uncertainty associated sample means.\\[\nt = \\frac{\\hat{\\mu_a} - \\hat{\\mu_b}}{\\sqrt{\\hat{\\sigma}^2_p \\left(\\frac{1}{N_a} + \\frac{1}{N_b}\\right)}}\n\\]\\(N_a\\) \\(N_b\\) sample sizes Lake b (.e., \\(N_a = N_b = 50\\) specific example), \\(\\hat{\\sigma}^2_p\\) weighted mean sample variances:\\[\n\\hat{\\sigma}^2_p = \\frac{N_a-1}{N_a + N_b - 2}\\hat{\\sigma}^2_a + \\frac{N_b-1}{N_a + N_b - 2}\\hat{\\sigma}^2_b\n\\]can calculate value R manually:difference sample means -2.06; therefore, t-statistic emphasizes distinction study lakes. occurrence can attributed t-statistic formula.denominator, observe inclusion \\(\\hat{\\sigma}^2_p\\) (estimated pooled variance), well inverses sample sizes \\(N_a\\) \\(N_b\\). Consequently, \\(\\hat{\\sigma}^2_p\\) decreases /sample sizes increase, t-statistic increases. reasonable outcome since decrease variance /increase sample size enhance certainty mean difference.","code":"\n# take another look at df_fl_mu\nprint(df_fl_mu)## # A tibble: 2 × 3\n##   lake   mu_l  sd_l\n##   <chr> <dbl> <dbl>\n## 1 a      13.4  2.92\n## 2 b      15.4  3.39\n# pull mu_l from tibble as vector\nv_mu <- df_fl_mu %>% \n  pull(mu_l)\n\n# lake a\nprint(v_mu[1])## [1] 13.35\n# lake b\nprint(v_mu[2])## [1] 15.406\n# difference\nv_mu[1] - v_mu[2]## [1] -2.056\n# group mean, variance, and sample size\ndf_t <- df_fl %>% \n  group_by(lake) %>% # group operation\n  summarize(mu_l = mean(length), # summarize by mean()\n            var_l = var(length), # summarize with sd()\n            n = n()) # count number of rows per group\n\nprint(df_t)## # A tibble: 2 × 4\n##   lake   mu_l var_l     n\n##   <chr> <dbl> <dbl> <int>\n## 1 a      13.4  8.52    50\n## 2 b      15.4 11.5     50\n# pull values as a vector\nv_mu <- pull(df_t, mu_l)\nv_var <- pull(df_t, var_l)\nv_n <- pull(df_t, n)\n\nvar_p <- ((v_n[1] - 1)/(sum(v_n) - 2)) * v_var[1] +\n  ((v_n[2] - 1)/(sum(v_n) - 2)) * v_var[2]\n\nt_value <- (v_mu[1] - v_mu[2]) / sqrt(var_p * ((1 / v_n[1]) + (1 / v_n[2])))\n\nprint(t_value)## [1] -3.247322"},{"path":"two-group-comparison.html","id":"null-hypothesis","chapter":"10 Two-Group Comparison","heading":"10.3.2 Null Hypothesis","text":"observed t-statistic serves dual purpose accounting disparity means accompanying uncertainty. However, significance t-statistic remains unclear.address , concept Null Hypothesis utilized substantiate observed t-statistic. Null Hypothesis, difference groups \\(\\mu_a = \\mu_b\\), allows us draw probability distribution test-statistic. know probability distribution, can estimate probability observing given t-statistic random chance difference mean body size lakes.Let’s begin assuming difference mean body size fish populations lakes, meaning true difference means zero (\\(\\mu_a - \\mu_b = 0\\); without hats formula!). assumption, can define probability distribution t-statistics, represents t-statistics distributed across range possible values.probability distribution known Student’s t-distribution characterized three parameters: mean, variance, degrees freedom (d.f.). Considering evaluating difference body size test statistic, mean (difference) assumed zero. variance estimated using \\(\\hat{\\sigma}^2_p\\), degrees freedom determined \\(N_a + N_b - 2\\) (d.f. can considered measure related sample size3).R function draw Student’s t-distribution; let’s try :\nFigure 10.2: Distribution t-statistics null hypothesis. probability density distribution determines likelihood observing particular t-statistic. Higher probability density indicates greater likelihood t-statistic occurring.\nprobability density distribution (Figure 10.2) determines likelihood observing particular t-statistic. Higher probability density indicates greater likelihood t-statistic occurring. Compare observed t-statistic null distribution:\nFigure 10.3: Observed t-statistic comparison null distrubution\nprobability distribution, area curve corresponds probability. Notably, area curve falls observed t-statistic (indicated vertical red lines) small (Figure 10.3), meaning observed difference body size unlikely occur null hypothesis (difference true means).function pt() allows us calculate area curve:p-value, .e., probability observing t-statistics less ( pr_below) greater (pr_above) observed t-statistic null hypothesis, can estimated sum pr_below pr_above.","code":"\n# produce 500 values from -5 to 5 with equal interval\nx <- seq(-5, 5, length = 500)\n\n# probability density of t-statistics with df = sum(v_n) - 2\ny <- dt(x, df = sum(v_n) - 2)\n\n# draw figure\ntibble(x, y) %>% \n  ggplot(aes(x = x,\n             y = y)) +\n  geom_line() +\n  labs(y = \"Probability density\",\n       x = \"t-statistic\")\n# draw entire range\ntibble(x, y) %>% \n  ggplot(aes(x = x,\n             y = y)) +\n  geom_line() +\n  geom_vline(xintercept = t_value,\n             color = \"salmon\") + # t_value is the observed t_value\n  geom_vline(xintercept = abs(t_value),\n             color = \"salmon\") + # t_value is the observed t_value\n  labs(y = \"Probability density\",\n       x = \"t-statistic\") \n# calculate area under the curve from -infinity to t_value\npr_below <- pt(q = t_value, df = sum(v_n) - 2)\n\n# calculate area under the curve from abs(t_value) to infinity\npr_above <- 1 - pt(q = abs(t_value), df = sum(v_n) - 2)\n# p_value\np_value <- pr_below + pr_above\nprint(p_value)## [1] 0.001595529"},{"path":"two-group-comparison.html","id":"interpretation","chapter":"10 Two-Group Comparison","heading":"10.3.3 Interpretation","text":"exercise called t-test – perhaps, well-known hypothesis testing. explain interpretation results, let use following notations. \\(t_{obs}\\), observed t-statistic; \\(t_0\\), possible t-statistics null hypothesis; \\(\\Pr(\\cdot)\\), probability “\\(\\cdot\\)” occurs – example, \\(\\Pr(x > 2)\\) means probability \\(x\\) exceeding \\(2\\).first calculated observed t-statistic \\(t_{obs}\\) using data fish body size, -3.25. , calculated p-value, .e., \\(\\Pr(t_0 < t_{obs}) + \\Pr(t_0 > t_{obs})\\) null hypothesis \\(\\mu_a = \\mu_b\\). found p-value low, meaning observed t-statistic unlikely occur \\(\\mu_a = \\mu_b\\) truth. Therefore, logical conclude Alternative Hypothesis \\(\\mu_a \\ne \\mu_b\\) likely.logic used many statistical analyses – define null hypothesis, estimate probability observing given value null hypothesis. tricky, find genius: can substantiate observation(s) objectively otherwise subjective claim!However, crucial recognize , even found observed t-statistic results high p-value (.e., common null hypothesis), finding result support null hypothesis – just can’t reject .","code":""},{"path":"two-group-comparison.html","id":"t-test-with-unequal-variance","chapter":"10 Two-Group Comparison","heading":"10.4 t-test with unequal variance","text":"example assumes relative similarity variance groups, unrealistic assumption. Luckily, variant t-test “Welch’s t-test,” assume unequal variance groups. implementation easy – set var.equal = FALSE t.test():Welch’s t-test, t-statistics defined differently account unequal variance groups4:\\[\nt = \\frac{\\hat{\\mu_a} - \\hat{\\mu_b}}{\\sqrt{\\left(\\frac{\\hat{\\sigma}^2_a}{N_a} + \\frac{\\hat{\\sigma}^2_b}{N_b}\\right)}}\n\\]t-statistic known follow Student’s t-distribution degrees freedom:\\[\nd.f. = \\frac{(\\frac{\\hat{\\sigma}^2_a}{N_a} + \\frac{\\hat{\\sigma}^2_b}{N_b})^2}{\\frac{(\\hat{\\sigma}^2_a / N_a)^2}{N_a - 1} + \\frac{(\\hat{\\sigma}^2_b/N_b)^2}{N_b-1}}\n\\]Therefore, reported values t-statistic d.f. different estimated. Welch’s t-test covers cases equal variances; therefore, default, use Welch’s test.","code":"\nt.test(x, y, var.equal = FALSE)## \n##  Welch Two Sample t-test\n## \n## data:  x and y\n## t = -0.7699, df = 501.15, p-value = 0.4417\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -0.3544786  0.1548791\n## sample estimates:\n##     mean of x     mean of y \n## -2.029679e-16  9.979976e-02"},{"path":"two-group-comparison.html","id":"laboratory-3","chapter":"10 Two-Group Comparison","heading":"10.5 Laboratory","text":"","code":""},{"path":"two-group-comparison.html","id":"influence-of-sample-size","chapter":"10 Two-Group Comparison","heading":"10.5.1 Influence of Sample Size","text":"Create following vectors rnorm():xs mean \\(10\\), SD \\(5\\), sample size \\(10\\)ys mean \\(12\\), SD \\(5\\), sample size \\(10\\)xl mean \\(10\\), SD \\(5\\), sample size \\(100\\)yl mean \\(12\\), SD \\(5\\), sample size \\(100\\)Perform Welch’s t-test xs vs. ys, xl vs. yl compare p-values. Set var.equal = TRUE t.test().","code":""},{"path":"two-group-comparison.html","id":"difference-and-uncertainty","chapter":"10 Two-Group Comparison","heading":"10.5.2 Difference and Uncertainty","text":"four vectors - a1 a2 & b1 b2.Perform following analysis:Estimate sample means SDs vector. , create tibble() object group (consist characters a1, a2, b1, b2) value columns (consist values ). use group_by() summarize() functions estimate means SDs group.Estimate sample means SDs vector. , create tibble() object group (consist characters a1, a2, b1, b2) value columns (consist values ). use group_by() summarize() functions estimate means SDs group.Create figure similar Figure 10.1 using data a1 a2.Create figure similar Figure 10.1 using data a1 a2.Perform Welch’s t-test pair compare results.Perform Welch’s t-test pair compare results.","code":"\na1 <- c(13.9, 14.9 ,13.4, 14.3, 11.8, 13.9, 14.5, 15.1, 13.3, 13.9)\na2 <- c(17.4, 17.3, 20.1, 17.2, 18.4, 19.6, 16.8, 18.7, 17.8, 18.9)\n\nb1 <- c(10.9, 20.3, 9.6, 8.3, 14.5, 12.3, 14.5, 16.7, 9.3, 22.0)\nb2 <- c(26.9, 12.9, 11.1, 16.7, 20.0, 20.9, 16.6, 15.4, 16.2, 16.2)"},{"path":"two-group-comparison.html","id":"bonus-exercise-welchs-t-test","chapter":"10 Two-Group Comparison","heading":"10.5.3 Bonus Exercise: Welch’s t-test","text":"Reproduce results t.test(x, y, var.equal = FALSE) without using function (report t-statistic, degrees freedom, p-value).","code":""},{"path":"multiple-group-comparison.html","id":"multiple-group-comparison","chapter":"11 Multiple-Group Comparison","heading":"11 Multiple-Group Comparison","text":"t-test used compare two groups, two groups, ANOVA (Analysis Variance) employed. ANOVA allows simultaneous comparison means among multiple groups determine statistically significant differences. ANOVA uses following steps:Partition total variability -group within-group components.Define test statistic ratio -group variability within-group variability (F statistic).Define probability distribution F-statistic null hypothesis.Estimate probability yielding greater observed test statistic.appropriate, post-hoc tests can conducted identify specific differing groups.Key words: F-statistic, Sum Squares","code":""},{"path":"multiple-group-comparison.html","id":"partition-the-variability","chapter":"11 Multiple-Group Comparison","heading":"11.1 Partition the Variability","text":"first step determine aspect examine. case t-test, focus difference sample means groups. One might initially consider conducting t-tests possible combinations. However, approach leads problem known multiple comparisons problem. Hence, viable option. Therefore, need explore data different perspective.facilitate learning, let’s utilize lake fish data (download ), time three lakes analysis:Visualization can helpful understanding data distributed. mean \\(\\pm\\) SD perfectly fine , let use violin plot show different way visualization.\nFigure 11.1: Violin plot fish length three lakes.\nappears Lake b exhibits larger average body size compared lakes. One approach quantifying difference groups examining ratio -group variability within-group variability. observe greater -group variability relative within-group variability, suggests differences among groups substantial. words, much observed variation explained group structure (lake).Let denote -group within-group variability \\(S_b\\) \\(S_w\\), respectively, defined follows:\\[\n\\begin{aligned}\nS_b &= \\sum_g \\sum_i (\\hat{\\mu}_{g()} - \\hat{\\mu})^2\\\\\nS_w &= \\sum_g \\sum_i (x_{} - \\hat{\\mu}_{g()})^2\n\\end{aligned}\n\\]double summation \\(\\sum_g \\sum_i\\) might scare , worries. can decompose equation two steps.","code":"\ndf_anova <- read_csv(\"data_raw/data_fish_length_anova.csv\")\ndistinct(df_anova, lake)## # A tibble: 3 × 1\n##   lake \n##   <chr>\n## 1 a    \n## 2 b    \n## 3 c\n# geom_violin() - function for violin plots\n# geom_jitter() - jittered points\n\ndf_anova %>% \n  ggplot(aes(x = lake,\n             y = length)) +\n  geom_violin(draw_quantiles = 0.5, # draw median horizontal line\n              alpha = 0.2) + # transparency\n  geom_jitter(alpha = 0.2) # transparency"},{"path":"multiple-group-comparison.html","id":"between-group-variability","chapter":"11 Multiple-Group Comparison","heading":"11.1.1 Between-group variability","text":"Let first consider \\(S_b = \\sum_g \\sum_i (\\hat{\\mu}_{g()} - \\hat{\\mu})^2\\). equation, \\(\\hat{\\mu}\\) overall mean fish length \\(\\hat{\\mu}_{g()}\\) group-mean given lake (\\(g \\\\{, b, c\\}\\)). Let’s perform estimation R:column dev_g, estimated \\((\\hat{\\mu}_{g()} - \\hat{\\mu})^2\\). must sum \\(\\) (fish individual) get variability lake (\\(\\sum_i (\\hat{\\mu}_{g()} - \\hat{\\mu})^2\\)). Since \\(\\hat{\\mu}_{g()}\\) constant lake, can simply multiply dev_g sample size n lake:Sum \\(g\\) (lake) get \\(S_b\\).","code":"\n# estimate overall mean\nmu <- mean(df_anova$length)\n\n# estimate group means and sample size each\ndf_g <- df_anova %>% \n  group_by(lake) %>% \n  summarize(mu_g = mean(length), # mean for each group\n            dev_g = (mu_g - mu)^2, # squared deviation for each group\n            n = n()) # sample size for each group\n\nprint(df_g)## # A tibble: 3 × 4\n##   lake   mu_g   dev_g     n\n##   <chr> <dbl>   <dbl> <int>\n## 1 a      13.4 1.12       50\n## 2 b      15.4 0.997      50\n## 3 c      14.5 0.00344    50\ndf_g <- df_g %>% \n  mutate(ss = dev_g * n)\n\nprint(df_g)## # A tibble: 3 × 5\n##   lake   mu_g   dev_g     n     ss\n##   <chr> <dbl>   <dbl> <int>  <dbl>\n## 1 a      13.4 1.12       50 55.9  \n## 2 b      15.4 0.997      50 49.9  \n## 3 c      14.5 0.00344    50  0.172\ns_b <- sum(df_g$ss)\nprint(s_b)## [1] 105.9365"},{"path":"multiple-group-comparison.html","id":"within-group-variability","chapter":"11 Multiple-Group Comparison","heading":"11.1.2 Within-group variability","text":"can follow steps estimate within-group variability \\(S_w = \\sum_g \\sum_i (x_{} - \\hat{\\mu}_{g()})^2\\). Let’s estimate \\((x_{} - \\hat{\\mu}_{g()})^2\\) first:can take look group-level data following code; column mu_g contains group-specific means fish length, dev_i contains \\((x_i - \\hat{\\mu}_{g()})^2\\):Sum \\(\\) lake \\(g\\) (\\(\\sum_i (x_i - \\hat{\\mu}_{g()})^2\\)):sum \\(g\\) get \\(S_w\\):","code":"\ndf_i <- df_anova %>% \n  group_by(lake) %>% \n  mutate(mu_g = mean(length)) %>% # use mutate() to retain individual rows\n  ungroup() %>% \n  mutate(dev_i = (length - mu_g)^2) # deviation from group mean for each fish\n# filter() & slice(): show first 3 rows each group\nprint(df_i %>% filter(lake == \"a\") %>% slice(1:3))\n\nprint(df_i %>% filter(lake == \"b\") %>% slice(1:3))\n\nprint(df_i %>% filter(lake == \"c\") %>% slice(1:3))\ndf_i_g <- df_i %>% \n  group_by(lake) %>% \n  summarize(ss = sum(dev_i))\n\nprint(df_i_g)## # A tibble: 3 × 2\n##   lake     ss\n##   <chr> <dbl>\n## 1 a      417.\n## 2 b      565.\n## 3 c      486.\ns_w <- sum(df_i_g$ss)\nprint(s_w)## [1] 1467.645"},{"path":"multiple-group-comparison.html","id":"variability-to-variance","chapter":"11 Multiple-Group Comparison","heading":"11.1.3 Variability to Variance","text":"referred \\(S_b\\) \\(S_w\\) “variability,” essentially represents summation squared deviations. convert variances, can divide appropriate numbers. Chapter 8, mentioned denominator variance sample size minus one. principle applies , caution.-group variability, denoted \\(S_b\\), realized sample size number groups, \\(N_g\\), case three (representing number lakes). Therefore, divide three minus one obtain unbiased estimate -group variance, denoted \\(\\hat{\\sigma}_b^2\\):\\[\n\\hat{\\sigma}^2_b = \\frac{S_b}{N_g-1}\n\\]Meanwhile, need careful estimating within-group variance. Since within-group variance measured individual level, number data used equal number fish individuals. Yet, subtract number groups – rationale behind beyond scope, essentially accounting fact degrees freedom “used ” estimating group means. , estimate within-group variance \\(\\hat{\\sigma}^2_w\\) follows:\\[\n\\hat{\\sigma}^2_w = \\frac{S_w}{N-N_g}\n\\]","code":"\n# n_distinct() count the number of unique elements\nn_g <- n_distinct(df_anova$lake)\ns2_b <- s_b / (n_g - 1)\nprint(s2_b)## [1] 52.96827\ns2_w <- s_w / (nrow(df_anova) - n_g)\nprint(s2_w)## [1] 9.983982"},{"path":"multiple-group-comparison.html","id":"test-statistic-1","chapter":"11 Multiple-Group Comparison","heading":"11.2 Test Statistic","text":"","code":""},{"path":"multiple-group-comparison.html","id":"f-statistic","chapter":"11 Multiple-Group Comparison","heading":"11.2.1 F-statistic","text":"ANOVA, use F-statistic – ratio -group variability within-group variability. exercise essentially performed yield test statistic:\\[\nF = \\frac{\\text{-group variance}}{\\text{within-group variance}} = \\frac{\\hat{\\sigma}^2_b}{\\hat{\\sigma}^2_w}\n\\]F-statistic data calculated 5.31. indicates -group variance approximately five times higher within-group variance. difference appears significant, important determine whether statistically substantial. make claim, can use Null Hypothesis reference.","code":"\nf_value <- s2_b / s2_w\nprint(f_value)## [1] 5.305325"},{"path":"multiple-group-comparison.html","id":"null-hypothesis-1","chapter":"11 Multiple-Group Comparison","heading":"11.2.2 Null Hypothesis","text":"F-statistic follows F-distribution difference means among groups. Therefore, null hypothesis considering example means groups equal, represented \\(\\mu_a = \\mu_b = \\mu_c\\). ’s worth noting alternative hypotheses can take different forms, \\(\\mu_a \\ne \\mu_b = \\mu_c\\), \\(\\mu_a = \\mu_b \\ne \\mu_c\\), \\(\\mu_a \\ne \\mu_b \\ne \\mu_c\\). ANOVA, however, unable distinguish alternative hypotheses.degrees freedom F-distribution determined two parameters: \\(N_g - 1\\) \\(N - N_g\\). visualize distribution, can utilize thedf() function. Similar t-test, can plot F-distribution draw vertical line represent observed F-statistic. approach allows us assess position observed F-statistic within distribution determine associated p-value.\nFigure 11.2: F-distribution. vertical red line denotes observed F-statistic.\nUnlike t-statistics, F-statistics can take positive values (F-statistics ratio positive values). p-value \\(\\Pr(F_0 > F)\\), \\(F_0\\) possible F-statistics null hypothesis. Let’s estimate probability using pf():","code":"\nx <- seq(0, 10, by = 0.1)\ny <- df(x = x, df1 = n_g - 1, df2 = nrow(df_anova) - n_g)\n\ntibble(x = x, y = y) %>% \n  ggplot(aes(x = x,\n             y = y)) + \n  geom_line() + # F distribution\n  geom_vline(xintercept = f_value,\n             color = \"salmon\") # observed F-statistic\n# pf() estimate the probability of less than q\n# Pr(F0 > F) is 1 - Pr(F0 < F)\np_value <- 1 - pf(q = f_value, df1 = n_g - 1, df2 = nrow(df_anova) - n_g)\nprint(p_value)## [1] 0.00596054"},{"path":"multiple-group-comparison.html","id":"anova-in-r","chapter":"11 Multiple-Group Comparison","heading":"11.3 ANOVA in R","text":"Like t-test, R provides functions perform ANOVA easily. case, utilize aov() function. first argument function “formula,” used describe structure model. scenario, aim explain fish body length grouping according lakes. formula expression, represent relationship length ~ lake, using tilde symbol (~) indicate stochastic nature relationship.function returns Sum Squares Deg. Freedom – value matches calculated. get deeper insights, wrap object summary():F-statistic p-value identical manual estimate.","code":"\n# first argument is formula\n# second argument is data frame for reference\n# do not forget specify data = XXX! aov() refer to columns in the data frame\nm <- aov(formula = length ~ lake,\n         data = df_anova)\n\nprint(m)## Call:\n##    aov(formula = length ~ lake, data = df_anova)\n## \n## Terms:\n##                      lake Residuals\n## Sum of Squares   105.9365 1467.6454\n## Deg. of Freedom         2       147\n## \n## Residual standard error: 3.159744\n## Estimated effects may be unbalanced\nsummary(m)##              Df Sum Sq Mean Sq F value  Pr(>F)   \n## lake          2  105.9   52.97   5.305 0.00596 **\n## Residuals   147 1467.6    9.98                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"multiple-group-comparison.html","id":"post-hoc-tests","chapter":"11 Multiple-Group Comparison","heading":"11.4 Post-hoc Tests","text":"conducting ANOVA finding significant difference among group means, post-hoc test often performed determine specific groups differ significantly . Post-hoc tests help identify pairwise comparisons contribute observed overall difference.several post-hoc tests available ANOVA, including:Tukey’s Honestly Significant Difference (HSD): test compares possible pairs group means provides adjusted p-values control family-wise error rate. commonly used reliable post-hoc test.Tukey’s Honestly Significant Difference (HSD): test compares possible pairs group means provides adjusted p-values control family-wise error rate. commonly used reliable post-hoc test.Bonferroni correction: method adjusts significance level individual comparison control overall Type error rate. adjusted p-value obtained dividing desired significance level (e.g., 0.05) number comparisons.Bonferroni correction: method adjusts significance level individual comparison control overall Type error rate. adjusted p-value obtained dividing desired significance level (e.g., 0.05) number comparisons.Scheffe’s test: test controls family-wise error rate considering possible pairwise comparisons. conservative Tukey’s HSD, suitable cases specific comparisons particular interest.Scheffe’s test: test controls family-wise error rate considering possible pairwise comparisons. conservative Tukey’s HSD, suitable cases specific comparisons particular interest.Dunnett’s test: test useful comparing multiple groups control group. controls overall error rate conducting multiple t-tests control group groups.Dunnett’s test: test useful comparing multiple groups control group. controls overall error rate conducting multiple t-tests control group groups.choice post-hoc test depends specific research question, assumptions, desired control Type error rate. important select appropriate test interpret results accordingly draw valid conclusions group differences ANOVA analysis.","code":""},{"path":"multiple-group-comparison.html","id":"laboratory-4","chapter":"11 Multiple-Group Comparison","heading":"11.5 Laboratory","text":"","code":""},{"path":"multiple-group-comparison.html","id":"application-to-plantgrowth","chapter":"11 Multiple-Group Comparison","heading":"11.5.1 Application to PlantGrowth","text":"R, built-data set called PlantGrowth. analyze data set, carry following tasks:data set consists two columns: weight, group. Create figures similar Figure 11.1.data set consists two columns: weight, group. Create figures similar Figure 11.1.Conduct ANOVA examine whether differences weight among different group.Conduct ANOVA examine whether differences weight among different group.Discuss values reported scientific article?Discuss values reported scientific article?","code":""},{"path":"multiple-group-comparison.html","id":"power-analysis","chapter":"11 Multiple-Group Comparison","heading":"11.5.2 Power analysis","text":"’re planning compare mean plant biomass among three different wetlands using ANOVA. expect large effect size (Cohen’s f = 0.5) want achieve 80% power 0.05 significance level. can calculate required sample size per group R? may use pwr::pwr.anova.test() function R (see arguments ).","code":""},{"path":"regression.html","id":"regression","chapter":"12 Regression","heading":"12 Regression","text":"previous examples focused cases involved distinct group structure. However, always case structure exists. Instead, might examine relationship continuous variables. chapter, introduce technique called linear regression.Key words: intercept, slope (coefficient), least squares, residual, coefficient determination (\\(\\mbox{R}^2\\))","code":""},{"path":"regression.html","id":"explore-data-structure-1","chapter":"12 Regression","heading":"12.1 Explore Data Structure","text":"exercise, use algae biomass data streams. Download data locate data_raw/.dataset comprises data collected 50 sampling sites. variable biomass represents standing biomass, indicating dry mass algae time collection. hand, conductivity serves proxy water quality, higher values usually indicating higher nutrient content water. Let now proceed draw scatter plot help visualize relationship two variables.\nFigure 12.1: relationship algae biomass conductivity.\nseems noticeable positive correlation increased conductivity results higher algal biomass. Nevertheless, can accurately represent relationship line?","code":"\nlibrary(tidyverse)\ndf_algae <- read_csv(\"data_raw/data_algae.csv\")\nprint(df_algae)## # A tibble: 50 × 4\n##    biomass unit_biomass conductivity unit_cond\n##      <dbl> <chr>               <dbl> <chr>    \n##  1   19.8  mg_per_m2            30.2 ms       \n##  2   24.4  mg_per_m2            40.4 ms       \n##  3   27.4  mg_per_m2            59.4 ms       \n##  4   48.2  mg_per_m2            91.3 ms       \n##  5   19.2  mg_per_m2            24.2 ms       \n##  6   57.0  mg_per_m2            90.4 ms       \n##  7   51.9  mg_per_m2            94.7 ms       \n##  8   40.8  mg_per_m2            67.8 ms       \n##  9   37.1  mg_per_m2            64.8 ms       \n## 10    3.55 mg_per_m2            10.9 ms       \n## # ℹ 40 more rows\ndf_algae %>% \n  ggplot(aes(x = conductivity,\n             y = biomass)) +\n  geom_point()"},{"path":"regression.html","id":"drawing-the-fittest-line","chapter":"12 Regression","heading":"12.2 Drawing the “fittest” line","text":"","code":""},{"path":"regression.html","id":"linear-formula","chapter":"12 Regression","heading":"12.2.1 Linear formula","text":"first step represent relationship formula; let ASSUME relationship can described following linear formula:\\[\ny_i = \\alpha + \\beta x_i\n\\]formula, algal biomass, denoted \\(y_i\\), site \\(\\), expressed function conductivity, represented \\(x_i\\). variable explained (\\(y_i\\)) referred response (dependent) variable, variable used predict response variable (\\(x_i\\)) referred explanatory (independent) variable. However, two additional constants, namely \\(\\alpha\\) \\(\\beta\\). \\(\\alpha\\) commonly known intercept, \\(\\beta\\) referred slope coefficient. intercept represents value \\(y\\) \\(x\\) equal zero, slope indicates change \\(y\\) associated unit increase \\(x\\) (refer Figure 12.2).\nFigure 12.2: Intercept slope.\nHowever, formula, “model,” incomplete. must add error term \\(\\varepsilon_i\\) (residual) consider uncertainty associated observation process.\\[\ny_i = \\alpha + \\beta x_i + \\varepsilon_i\\\\\n\\varepsilon_i \\sim \\text{Normal}(0, \\sigma^2)\n\\]Notice \\(y_i\\), \\(x_i\\), \\(\\varepsilon_i\\) subscripts, \\(\\alpha\\) \\(\\beta\\) . means \\(y_i\\), \\(x_i\\), \\(\\varepsilon_i\\) vary data point \\(\\), \\(\\alpha\\) \\(\\beta\\) constants. Although \\(\\alpha + \\beta x_i\\) reproduce \\(y_i\\) perfectly, “fill” gaps error term \\(\\varepsilon_i\\), assumed follow Normal distribution.R, finding best \\(\\alpha\\) \\(\\beta\\) – .e., parameters model – easy. Function lm() everything . Let apply function example data set:Coefficients:, says (Intercept) 5.3 conductivity 0.5. values corresponds \\(\\alpha\\) \\(\\beta\\). Meanwhile, Residual standard error: indicates SD error term \\(\\sigma\\). Thus, substituting values formula yields:\\[\ny_i = 5.30 + 0.50 x_i + \\varepsilon_i\\\\\n\\varepsilon_i \\sim \\text{Normal}(0, 4.6^2)\n\\]can draw “fittest” line figure:\nFigure 12.3: Drawing fitted model prediction.\nHowever, lm() find \\(\\alpha\\) \\(\\beta\\)?","code":"\n# lm() takes a formula as the first argument\n# don't forget to supply your data\nm <- lm(biomass ~ conductivity,\n        data = df_algae)\n\nsummary(m)## \n## Call:\n## lm(formula = biomass ~ conductivity, data = df_algae)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.5578 -2.8729 -0.7307  2.5479 11.4700 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   5.29647    1.57178    3.37  0.00149 ** \n## conductivity  0.50355    0.02568   19.61  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.649 on 48 degrees of freedom\n## Multiple R-squared:  0.889,  Adjusted R-squared:  0.8867 \n## F-statistic: 384.5 on 1 and 48 DF,  p-value: < 2.2e-16\n# coef() extracts estimated coefficients\n# e.g., coef(m)[1] is (Intercept)\n\nalpha <- coef(m)[1]\nbeta <- coef(m)[2]\n\ndf_algae %>% \n  ggplot(aes(x = conductivity,\n             y = biomass)) +\n  geom_point() +\n  geom_abline(intercept = alpha,\n              slope = beta) # draw the line"},{"path":"regression.html","id":"minimizing-the-errors","chapter":"12 Regression","heading":"12.2.2 Minimizing the errors","text":"Notice error term difference \\(y_i\\) \\(\\alpha + \\beta x_i\\):\\[\n\\varepsilon_i = y_i - (\\alpha + \\beta x_i)\n\\]words, term represents portion observation \\(y_i\\) explained conductivity \\(x_i\\). Therefore, logical approach find values \\(\\alpha\\) \\(\\beta\\) minimize unexplained portion across data points \\(\\).Least squares methods widely employed statistical approach achieve objective. methods aim minimize sum squared errors, denoted \\(\\sum_i \\varepsilon_i^2\\), across data points. deviation expected value \\(\\alpha + \\beta x_i\\) increases, quantity also increases. Consequently, determining parameter values (\\(\\alpha\\) \\(\\beta\\)) minimize sum squared errors yields best-fitting formula (see Section 12.2.3 details).","code":""},{"path":"regression.html","id":"least-squares","chapter":"12 Regression","heading":"12.2.3 Least Squares","text":"Introducing matrix representation indeed helpful way explain least square method. , represent vector observed values \\(y_i\\) \\(Y\\) (\\(Y = \\{y_1, y_2,...,y_n\\}^T\\)), vector parameters \\(\\Theta\\) (\\(\\Theta = \\{\\alpha, \\beta\\}^T\\)), matrix composed \\(1\\)s \\(x_i\\) values \\(X\\). matrix \\(X\\) can written :\\[\nX =\n\\begin{pmatrix}\n  1 & x_1\\\\\n  1 & x_2\\\\\n  1 & x_3\\\\\n  \\vdots & \\vdots\\\\\n  1 & x_n\n\\end{pmatrix}\n\\]Using matrix notation, can express error vector \\(\\pmb{\\varepsilon}\\) :\\[\n\\pmb{\\varepsilon} = Y - X\\Theta\n\\]column \\(1\\)s required represent intercept; \\(X\\Theta\\) reads:\\[\nX\\Theta =\n\\begin{pmatrix}\n  1 & x_1\\\\\n  1 & x_2\\\\\n  1 & x_3\\\\\n  \\vdots & \\vdots\\\\\n  1 & x_n\n\\end{pmatrix}\n\\begin{pmatrix}\n  \\alpha\\\\\n  \\beta\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  \\alpha + \\beta x_1\\\\\n  \\alpha + \\beta x_2\\\\\n  \\alpha + \\beta x_3\\\\\n  \\vdots\\\\\n  \\alpha + \\beta x_n\n\\end{pmatrix}\n\\]Minimizing squared magnitude \\(\\pmb{\\varepsilon}\\) (solve partial derivative \\(||\\pmb{\\varepsilon}||^2\\) \\(\\Theta\\)) leads solution (detailed derivation, refer Wikipedia):\\[\n\\hat{\\Theta} = (X^TX)^{-1}X^{T}Y\n\\]Let see reproduces result lm():lm() output reference:","code":"\n# create matrix X\nv_x <- df_algae %>% pull(conductivity)\nX <- cbind(1, v_x)\n\n# create a vector of y\nY <- df_algae %>% pull(biomass)\n\n# %*%: matrix multiplication\n# t(): transpose a matrix\n# solve(): computes the inverse matrix\ntheta <- solve(t(X) %*% X) %*% t(X) %*% Y\nprint(theta)##          [,1]\n##     5.2964689\n## v_x 0.5035548\nm <- lm(biomass ~ conductivity,\n        data = df_algae)\n\ncoef(m)##  (Intercept) conductivity \n##    5.2964689    0.5035548"},{"path":"regression.html","id":"standard-errors-and-t-values","chapter":"12 Regression","heading":"12.2.4 Standard Errors and t-values","text":"Ensuring reliability estimated point estimates \\(\\alpha\\) \\(\\beta\\) crucial. summary(m) output, two statistical quantities, namely Std. Error t-value, play significant role assessing uncertainty associated estimates.standard error (Std. Error; “SE”) represents estimated standard deviation parameter estimates (\\(\\hat{\\theta}\\) either \\(\\hat{\\alpha}\\) \\(\\hat{\\beta}\\)). smaller value standard error indicates higher degree statistical reliability estimate. hand, t-value concept covered Chapter 10. value defined :\\[\nt = \\frac{\\hat{\\theta} - \\theta_0}{\\text{SE}(\\hat{\\theta})}\n\\]t-statistic akin t-test; however, context regression analysis, another group compare (.e., \\(\\theta_0\\)). Typically, regression analysis, use zero reference (\\(\\theta_0 = 0\\)). Therefore, higher t-values lm() indicate greater deviation zero. Consequently, Null Hypothesis regression analysis \\(\\beta = 0\\). hypothesis sensible specific example since interested quantifying effect conductivity. \\(\\beta = 0\\), implies conductivity effect algal biomass. Since \\(\\theta_0 = 0\\), following code reproduces reported t-values:defining Null Hypothesis t-values, can compute probability observing estimated parameters Null Hypothesis. Similar t-test, lm() utilizes Student’s t-distribution purpose. However, difference lies estimate degrees freedom. case, 50 data points need subtract number parameters, \\(\\alpha\\) \\(\\beta\\). Therefore, degrees freedom \\(50 - 2 = 48\\). value employed calculating p-value:","code":"\n# extract coefficients\ntheta <- coef(m)\n\n# extract standard errors\nse <- sqrt(diag(vcov(m)))\n\n# t-value\nt_value <- theta / se\nprint(t_value)##  (Intercept) conductivity \n##     3.369732    19.609446\n# for intercept\n# (1 - pt(t_value[1], df = 48)) calculates pr(t > t_value[1])\n# pt(-t_value[1], df = 48) calculates pr(t < -t_value[1])\np_alpha <- (1 - pt(t_value[1], df = 48)) + pt(-t_value[1], df = 48)\n\n# for slope\np_beta <- (1 - pt(t_value[2], df = 48)) + pt(-t_value[2], df = 48)\n\nprint(p_alpha)## (Intercept) \n## 0.001492023\nprint(p_beta)## conductivity \n## 7.377061e-25"},{"path":"regression.html","id":"unexplained-variation","chapter":"12 Regression","heading":"12.3 Unexplained Variation","text":"","code":""},{"path":"regression.html","id":"retrieve-errors","chapter":"12 Regression","heading":"12.3.1 Retrieve Errors","text":"function lm() provides estimates \\(\\alpha\\) \\(\\beta\\), directly provide values \\(\\varepsilon_i\\). gain deeper understanding statistical model, necessary examine residuals \\(\\varepsilon_i\\). using resid() function, can obtain values \\(\\varepsilon_i\\). allows explore extract insights developed statistical model.element retains order data point original data frame, eps[1] (\\(\\varepsilon_1\\)) identical \\(y_1 - (\\alpha + \\beta x_1)\\). Let confirm:ensure two columns indeed identical, visual check may prone overlooking small differences. perform precise comparison, can use () function check data aligns (, evaluated five decimal points using round()). comparing two columns manner, can confirm whether identical .","code":"\n# eps - stands for epsilon\neps <- resid(m)\nhead(eps)##          1          2          3          4          5          6 \n## -0.6838937 -1.2149034 -7.8576927 -3.0109473  1.7076481  6.1773586\n# pull vector data\nv_x <- df_algae %>% pull(conductivity)\nv_y <- df_algae %>% pull(biomass)\n\n# theta[1] = alpha\n# theta[2] = beta\nerror <- v_y - (theta[1] + theta[2] * v_x)\n\n# cbind() combines vectors column-wise\n# head() retrieves the first 6 rows\nhead(cbind(eps, error))##          eps      error\n## 1 -0.6838937 -0.6838937\n## 2 -1.2149034 -1.2149034\n## 3 -7.8576927 -7.8576927\n## 4 -3.0109473 -3.0109473\n## 5  1.7076481  1.7076481\n## 6  6.1773586  6.1773586\n# round values at 5 decimal\neps <- round(eps, 5)\nerror <- round(error, 5)\n\n# eps == error return \"TRUE\" or \"FALSE\"\n# all() returns TRUE if all elements meet eps == error\nall(eps == error)## [1] TRUE"},{"path":"regression.html","id":"visualize-errors","chapter":"12 Regression","heading":"12.3.2 Visualize Errors","text":"visualize errors residuals, represent distance best-fitted line data point, can make use geom_segment() function:\nFigure 12.4: Vertical segments indicate errors.\n","code":"\n# add error column\ndf_algae <- df_algae %>% \n  mutate(eps = eps)\n\ndf_algae %>% \n  ggplot(aes(x = conductivity,\n             y = biomass)) +\n  geom_point() +\n  geom_abline(intercept = alpha,\n              slope = beta) + \n  geom_segment(aes(x = conductivity, # start-coord x\n                   xend = conductivity, # end-coord x\n                   y = biomass, # start-coord y\n                   yend = biomass - eps), # end-coord y\n               linetype = \"dashed\")"},{"path":"regression.html","id":"coefficient-of-determination","chapter":"12 Regression","heading":"12.3.3 Coefficient of Determination","text":"One crucial motivation developing regression model, statistical model, assess extent explanatory variable (\\(x_i\\)) explains variation response variable (\\(y_i\\)). fitting regression model, can quantify amount variability response variable can attributed explanatory variable. assessment provides valuable insights strength significance relationship variables consideration.coefficient determination, denoted R², statistical measure assesses proportion variance response variable can explained explanatory variable(s) regression model. provides indication well regression model fits observed data. formula R² :\\[\n\\text{R}^2 = 1 - \\frac{SS}{SS_0}\n\\]\\(SS\\) summed squares residuals (\\(\\sum_i \\varepsilon_i^2\\)), \\(SS_0\\) summed squares response variable (\\(SS_0 = \\sum_i(y_i - \\hat{\\mu}_y)^2\\)). term \\(\\frac{SS}{SS_0}\\) represents proportion variability “unexplained” – therefore, \\(1 - \\frac{SS}{SS_0}\\) gives proportion variability “explained.” R² value 0 1. R² value 0 indicates explanatory variable(s) explain variability response variable, R² value 1 indicates explanatory variable(s) can fully explain variability response variable.R2 provided default output lm(), let’s confirm equation reproduces reported value:Compare lm() output:Multiple R-squared: corresponds calculated coefficient determination.","code":"\n# residual variance\nss <- sum(resid(m)^2)\n\n# null variance\nss_0 <- sum((v_y - mean(v_y))^2)\n\n# coefficient of determination\nr2 <- 1 - ss / ss_0\n\nprint(r2)## [1] 0.8890251\nsummary(m)## \n## Call:\n## lm(formula = biomass ~ conductivity, data = df_algae)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.5578 -2.8729 -0.7307  2.5479 11.4700 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   5.29647    1.57178    3.37  0.00149 ** \n## conductivity  0.50355    0.02568   19.61  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.649 on 48 degrees of freedom\n## Multiple R-squared:  0.889,  Adjusted R-squared:  0.8867 \n## F-statistic: 384.5 on 1 and 48 DF,  p-value: < 2.2e-16"},{"path":"regression.html","id":"laboratory-5","chapter":"12 Regression","heading":"12.4 Laboratory","text":"","code":""},{"path":"regression.html","id":"develop-regression-models","chapter":"12 Regression","heading":"12.4.1 Develop regression models","text":"R provides built-data set called iris. iris data contain data points three different species (Species column). Split data set species (create three separate data frames) perform regression species separately analyze relationship Sepal.Width (response variable) Petal.Width (explanatory variable).","code":"\nhead(iris)##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa"},{"path":"regression.html","id":"multiple-explanatory-variables","chapter":"12 Regression","heading":"12.4.2 Multiple explanatory variables","text":"Regression analysis can involve multiple explanatory variables. explore , consider utilizing Petal.Length additional explanatory variable species. , investigate (1) variations estimates regression coefficients (2) differences coefficients determination compared model single explanatory variable.","code":""},{"path":"linear-model.html","id":"linear-model","chapter":"13 Linear Model","heading":"13 Linear Model","text":"extensively covered three important statistical analyses: t-test, ANOVA, regression analysis. methods may seem distinct, fall umbrella Linear Model framework5.Linear Model encompasses models depict connection response variable one explanatory variables. assumes error term follows normal distribution. chapter, elucidate framework illustrate relationship t-test, ANOVA, regression analysis.Key words: level measurement, dummy variable","code":""},{"path":"linear-model.html","id":"the-frame","chapter":"13 Linear Model","heading":"13.1 The Frame","text":"apparent distinctiveness t-test, ANOVA, regression analysis arises applied different types data:t-test: comparing differences two groups.ANOVA: examining differences among two groups.Regression: exploring relationship response variable one explanatory variables.Despite differences, analyses can unified single formula. discussed regression formula Chapter 12, :\\[\n\\begin{aligned}\ny_i &= \\alpha + \\beta_1 x_{1,} + \\varepsilon_i\\\\\n\\varepsilon_i &\\sim \\text{Normal}(0, \\sigma^2)\n\\end{aligned}\n\\]\\(y_i\\) represents response variable (fish body length plant height), \\(x_{1,}\\) denotes continuous explanatory variable, \\(\\alpha\\) represents intercept, \\(\\beta_1\\) corresponds slope. equivalent model can expressed differently follows:\\[\n\\begin{aligned}\ny_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2)\\\\\n\\mu_i &= \\alpha + \\beta_1 x_{1,}\n\\end{aligned}\n\\]structure provides insight relationship t-test, ANOVA, regression analysis. fundamental purpose regression analysis model mean \\(\\mu_i\\) changes increasing decreasing values \\(x_{1,}\\). Thus, t-test, ANOVA, regression analyze mean – expected value Normal distribution. primary distinction t-test, ANOVA, regression lies nature explanatory variable, can either continuous categorical (group).","code":""},{"path":"linear-model.html","id":"two-group-case","chapter":"13 Linear Model","heading":"13.1.1 Two-Group Case","text":"establish connection among approaches, one can employ “dummy indicator” variables represent group variables. case t-test, categorical variable consists two groups, typically denoted b (although can represented numbers well), one can convert categories numerical values. instance, assigning 0 b 1 enables following conversion:\\[\n\\pmb{x'}_2 =\n\\begin{pmatrix}\n   \\\\\n   \\\\\n   b\\\\\n   \\vdots\\\\\n   b\\end{pmatrix}\n\\rightarrow\n\\pmb{x}_2 =\n\\begin{pmatrix}\n0\\\\\n0\\\\\n1\\\\\n\\vdots\\\\\n1\n\\end{pmatrix}\n\\]model can written :\\[\n\\begin{aligned}\ny_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2)\\\\\n\\mu_i &= \\alpha + \\beta_2 x_{2,}\n\\end{aligned}\n\\]incorporating variable model, interesting outcomes arise. Since \\(x_{2,} = 0\\) observation belongs group , mean group (\\(\\mu_i = \\mu_a\\)) determined \\(\\mu_a = \\alpha + \\beta \\times 0 = \\alpha\\). Consequently, intercept represents mean value first group. hand, observation group b, mean group b (\\(\\mu_i = \\mu_b\\)) given \\(\\mu_b = \\alpha + \\beta \\times 1 = \\alpha + \\beta\\). ’s important recall \\(\\mu_a = \\alpha\\). substituting \\(\\alpha\\) \\(\\mu_a\\), equation \\(\\beta = \\mu_b - \\mu_a\\) obtained, indicating slope represents difference means two groups – key statistic t-test.Let confirm using dataset Chapter 10 (fish body length two lakes):lm() function, provide “categorical” variable character factor form, automatically converted binary (0/1) variable internally. means can include variable formula conducting standard regression analysis. lm() function takes care conversion process, allowing seamlessly incorporate categorical variables lm() function.estimated coefficient Lake b (lakeb) identical difference group means. can compare statistics (t value Pr(>|t|) output t.test() well:t-statistic p-value match lm() output.","code":"\nlibrary(tidyverse)\ndf_fl <- read_csv(\"data_raw/data_fish_length.csv\")\nprint(df_fl)## # A tibble: 100 × 3\n##    lake  length unit \n##    <chr>  <dbl> <chr>\n##  1 a       10.8 cm   \n##  2 a       13.6 cm   \n##  3 a       10.1 cm   \n##  4 a       18.6 cm   \n##  5 a       14.2 cm   \n##  6 a       10.1 cm   \n##  7 a       14.7 cm   \n##  8 a       15.6 cm   \n##  9 a       15   cm   \n## 10 a       11.9 cm   \n## # ℹ 90 more rows\n# group means\nv_mu <- df_fl %>% \n  group_by(lake) %>% \n  summarize(mu = mean(length)) %>% \n  pull(mu)\n\n# mu_a: should be identical to intercept\nv_mu[1]## [1] 13.35\n# mu_b - mu_a: should be identical to slope\nv_mu[2] - v_mu[1]## [1] 2.056\n# in lm(), letters are automatically converted to 0/1 binary variable.\n# alphabetically ordered (in this case, a = 0, b = 1)\nm <- lm(length ~ lake,\n        data = df_fl)\n\nsummary(m)## \n## Call:\n## lm(formula = length ~ lake, data = df_fl)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -8.150 -2.156  0.022  2.008  7.994 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  13.3500     0.4477  29.819   <2e-16 ***\n## lakeb         2.0560     0.6331   3.247   0.0016 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.166 on 98 degrees of freedom\n## Multiple R-squared:  0.09715,    Adjusted R-squared:  0.08794 \n## F-statistic: 10.55 on 1 and 98 DF,  p-value: 0.001596\nlake_a <- df_fl %>% \n  filter(lake == \"a\") %>% \n  pull(length)\n\nlake_b <- df_fl %>% \n  filter(lake == \"b\") %>% \n  pull(length)\n\nt.test(x = lake_b, y = lake_a)## \n##  Welch Two Sample t-test\n## \n## data:  lake_b and lake_a\n## t = 3.2473, df = 95.846, p-value = 0.001606\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  0.7992071 3.3127929\n## sample estimates:\n## mean of x mean of y \n##    15.406    13.350"},{"path":"linear-model.html","id":"multiple-group-case","chapter":"13 Linear Model","heading":"13.1.2 Multiple-Group Case","text":"argument applies ANOVA. ANOVA, deal two groups explanatory variable. handle , can convert group variable multiple dummy variables. instance, variable \\(\\pmb{x'}_2 = \\{, b, c\\}\\), can convert \\(x_2 = \\{0, 1, 0\\}\\) (\\(b \\rightarrow 1\\) others \\(0\\)) \\(\\pmb{x}_3 = \\{0, 0, 1\\}\\) (\\(c \\rightarrow 1\\) others \\(0\\)). Thus, model formula :\\[\n\\begin{aligned}\ny_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2)\\\\\n\\mu_i &= \\alpha + \\beta_{2} x_{2,}  + \\beta_{3} x_{3,}\n\\end{aligned}\n\\]substitute dummy variables, :\\[\n\\begin{aligned}\n\\mu_a &= \\alpha + \\beta_2 \\times 0 + \\beta_3 \\times 0 = \\alpha &&\\text{}~x_{2,}~\\text{}~x_{3,}~\\text{zero}\\\\\n\\mu_b &= \\alpha + \\beta_2 \\times 1 + \\beta_3 \\times 0 = \\alpha + \\beta_2 &&x_{2,} = 1~\\text{}~x_{3,}=0\\\\\n\\mu_c &= \\alpha + \\beta_2 \\times 0 + \\beta_3 \\times 1 = \\alpha + \\beta_3 &&x_{2,} = 0~\\text{}~x_{3,}=1\\\\\n\\end{aligned}\n\\]Therefore, group serves reference, \\(\\beta\\)s represent deviations reference group. Now, let attempt ANOVA dataset provided Chapter 11:, lm() function converts categorical variable dummy variables internally. Compare mean differences estimated parameters:Also, report F-statistic (5.305) p-value (0.005961). Compare aov() output:results identical.","code":"\ndf_anova <- read_csv(\"data_raw/data_fish_length_anova.csv\")\nprint(df_anova)## # A tibble: 150 × 3\n##    lake  length unit \n##    <chr>  <dbl> <chr>\n##  1 a       10.8 cm   \n##  2 a       13.6 cm   \n##  3 a       10.1 cm   \n##  4 a       18.6 cm   \n##  5 a       14.2 cm   \n##  6 a       10.1 cm   \n##  7 a       14.7 cm   \n##  8 a       15.6 cm   \n##  9 a       15   cm   \n## 10 a       11.9 cm   \n## # ℹ 140 more rows\n# group means\nv_mu <- df_anova %>% \n  group_by(lake) %>% \n  summarize(mu = mean(length)) %>% \n  pull(mu)\n\nprint(c(v_mu[1], # mu_a: should be identical to intercept\n        v_mu[2] - v_mu[1], # mu_b - mu_a: should be identical to the slope for lakeb\n        v_mu[3] - v_mu[1])) # mu_c - mu_a: should be identical to the slope for lakec## [1] 13.350  2.056  1.116\n# lm() output\nm <- lm(length ~ lake,\n        data = df_anova)\n\nsummary(m)## \n## Call:\n## lm(formula = length ~ lake, data = df_anova)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -8.150 -1.862 -0.136  1.846  7.994 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  13.3500     0.4469  29.875  < 2e-16 ***\n## lakeb         2.0560     0.6319   3.253  0.00141 ** \n## lakec         1.1160     0.6319   1.766  0.07948 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.16 on 147 degrees of freedom\n## Multiple R-squared:  0.06732,    Adjusted R-squared:  0.05463 \n## F-statistic: 5.305 on 2 and 147 DF,  p-value: 0.005961\nm_aov <- aov(length ~ lake,\n             data = df_anova)\n\nsummary(m_aov)##              Df Sum Sq Mean Sq F value  Pr(>F)   \n## lake          2  105.9   52.97   5.305 0.00596 **\n## Residuals   147 1467.6    9.98                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"linear-model.html","id":"the-common-structure","chapter":"13 Linear Model","heading":"13.1.3 The Common Structure","text":"examples show t-test, ANOVA, regression model structure:\\[\ny_i = \\text{(deterministic component)} + \\text{(stochastic component)}\n\\] framework Linear Model, deterministic component expressed \\(\\alpha + \\sum_k \\beta_k x_{k,}\\) stochastic component (random error) expressed Normal distribution. structure makes several assumptions. main assumptions:Linearity: relationship explanatory variables response variable linear. means effect explanatory variable response variable additive constant.Linearity: relationship explanatory variables response variable linear. means effect explanatory variable response variable additive constant.Independence: observations independent . assumption implies values response variable one observation influence values observations.Independence: observations independent . assumption implies values response variable one observation influence values observations.Homoscedasticity: variance response variable constant across levels explanatory variables. words, spread dispersion response variable values explanatory variables.Homoscedasticity: variance response variable constant across levels explanatory variables. words, spread dispersion response variable values explanatory variables.Normality: error term follows normal distribution level explanatory variables. assumption important many statistical tests estimators used linear model based assumption normality.Normality: error term follows normal distribution level explanatory variables. assumption important many statistical tests estimators used linear model based assumption normality.multicollinearity: explanatory variables highly correlated . Multicollinearity can lead problems estimating coefficients accurately can make interpretation difficult.multicollinearity: explanatory variables highly correlated . Multicollinearity can lead problems estimating coefficients accurately can make interpretation difficult.Violations assumptions can affect validity reliability results obtained Linear Model.","code":""},{"path":"linear-model.html","id":"combine-multiple-types-of-variables","chapter":"13 Linear Model","heading":"13.2 Combine Multiple Types of Variables","text":"","code":""},{"path":"linear-model.html","id":"iris-example---additive-effects","chapter":"13 Linear Model","heading":"13.2.1 iris Example - additive effects","text":"previous section clarified t-test, ANOVA, regression common model structure; therefore, categorical continuous variables can included model. , let use iris data (available R default) show example. data set comprises continuous (Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) categorical variables (Species):, let model Petal.Length function Petal.Width Species:Please note model output include setosa used reference group ((Intercept)).want obtain predicted values, can use predict() function. make predictions, need provide new data frame containing values explanatory variables prediction.plotting predicted values observed data points, can visually evaluate accuracy model assess goodness fit.\nFigure 13.1: Example model fitting iris data. Points observations, lines model predictions.\n","code":"\n# convert the data format to tibble\niris <- as_tibble(iris)\nprint(iris)## # A tibble: 150 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n##  1          5.1         3.5          1.4         0.2 setosa \n##  2          4.9         3            1.4         0.2 setosa \n##  3          4.7         3.2          1.3         0.2 setosa \n##  4          4.6         3.1          1.5         0.2 setosa \n##  5          5           3.6          1.4         0.2 setosa \n##  6          5.4         3.9          1.7         0.4 setosa \n##  7          4.6         3.4          1.4         0.3 setosa \n##  8          5           3.4          1.5         0.2 setosa \n##  9          4.4         2.9          1.4         0.2 setosa \n## 10          4.9         3.1          1.5         0.1 setosa \n## # ℹ 140 more rows\ndistinct(iris, Species)## # A tibble: 3 × 1\n##   Species   \n##   <fct>     \n## 1 setosa    \n## 2 versicolor\n## 3 virginica\n# develop iris model\nm_iris <- lm(Petal.Length ~ Petal.Width + Species,\n             data = iris)\n\nsummary(m_iris)## \n## Call:\n## lm(formula = Petal.Length ~ Petal.Width + Species, data = iris)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.02977 -0.22241 -0.01514  0.18180  1.17449 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)        1.21140    0.06524  18.568  < 2e-16 ***\n## Petal.Width        1.01871    0.15224   6.691 4.41e-10 ***\n## Speciesversicolor  1.69779    0.18095   9.383  < 2e-16 ***\n## Speciesvirginica   2.27669    0.28132   8.093 2.08e-13 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.3777 on 146 degrees of freedom\n## Multiple R-squared:  0.9551, Adjusted R-squared:  0.9542 \n## F-statistic:  1036 on 3 and 146 DF,  p-value: < 2.2e-16\n# create a data frame for prediction\n# variable names must be identical to the original dataframe for analysis\nn_rep <- 100\ndf_pred <- iris %>% \n  group_by(Species) %>% \n  reframe(Petal.Width = seq(min(Petal.Width),\n                            max(Petal.Width),\n                            length = n_rep))\n\n# make prediction based on supplied values of explanatory variables\ny_pred <- predict(m_iris,\n                  newdata = df_pred)\n\ndf_pred <- df_pred %>% \n  mutate(y_pred = y_pred)\n\nprint(df_pred)## # A tibble: 300 × 3\n##    Species Petal.Width y_pred\n##    <fct>         <dbl>  <dbl>\n##  1 setosa        0.1     1.31\n##  2 setosa        0.105   1.32\n##  3 setosa        0.110   1.32\n##  4 setosa        0.115   1.33\n##  5 setosa        0.120   1.33\n##  6 setosa        0.125   1.34\n##  7 setosa        0.130   1.34\n##  8 setosa        0.135   1.35\n##  9 setosa        0.140   1.35\n## 10 setosa        0.145   1.36\n## # ℹ 290 more rows\niris %>% \n  ggplot(aes(x = Petal.Width,\n             y = Petal.Length,\n             color = Species)) +\n  geom_point(alpha = 0.5) +\n  geom_line(data = df_pred,\n            aes(y = y_pred)) # redefine y values for lines; x and color are inherited from ggplot()"},{"path":"linear-model.html","id":"iris-example---interactive-effects","chapter":"13 Linear Model","heading":"13.2.2 iris Example - interactive effects","text":"statistical models, interaction occurs effect one factor depends level another factor. iris dataset, example, relationship Petal.Length Petal.Width may depend species. can represent scenario following model:model:Petal.Width represents effect petal width petal length reference species (setosa).Petal.Width represents effect petal width petal length reference species (setosa).Species captures differences petal length among species Petal.Width = 0.Species captures differences petal length among species Petal.Width = 0.interaction term (Petal.Width:Species) allows slope Petal.Length vs. Petal.Width vary species, meaning effect petal width can differ across species.interaction term (Petal.Width:Species) allows slope Petal.Length vs. Petal.Width vary species, meaning effect petal width can differ across species.interaction term significant, indicates relationship Petal.Length Petal.Width depends species, visualizing fitted lines species helpful interpretation.case, interaction significant versicolor, slope estimated clearly different two.","code":"\n# develop iris model with an interaction\nm_iris_int <- lm(Petal.Length ~ Petal.Width * Species,\n                 data = iris)\n\nsummary(m_iris_int)## \n## Call:\n## lm(formula = Petal.Length ~ Petal.Width * Species, data = iris)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.84099 -0.19343 -0.03686  0.16314  1.17065 \n## \n## Coefficients:\n##                               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)                     1.3276     0.1309  10.139  < 2e-16 ***\n## Petal.Width                     0.5465     0.4900   1.115   0.2666    \n## Speciesversicolor               0.4537     0.3737   1.214   0.2267    \n## Speciesvirginica                2.9131     0.4060   7.175 3.53e-11 ***\n## Petal.Width:Speciesversicolor   1.3228     0.5552   2.382   0.0185 *  \n## Petal.Width:Speciesvirginica    0.1008     0.5248   0.192   0.8480    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.3615 on 144 degrees of freedom\n## Multiple R-squared:  0.9595, Adjusted R-squared:  0.9581 \n## F-statistic: 681.9 on 5 and 144 DF,  p-value: < 2.2e-16\n# recreate prdiction dataframe\nn_rep <- 100\ndf_pred <- iris %>% \n  group_by(Species) %>% \n  reframe(Petal.Width = seq(min(Petal.Width),\n                            max(Petal.Width),\n                            length = n_rep))\n\n# prediction with the interaction model\ny_pred_int <- predict(m_iris_int,\n                      newdata = df_pred)\n\ndf_pred <- df_pred %>% \n  mutate(y_pred_int = y_pred_int)\n\n# visualization\niris %>% \n  ggplot(aes(x = Petal.Width,\n             y = Petal.Length,\n             color = Species)) +\n  geom_point(alpha = 0.5) +\n  geom_line(data = df_pred,\n            aes(y = y_pred_int))"},{"path":"linear-model.html","id":"level-of-measurament","chapter":"13 Linear Model","heading":"13.2.3 Level of Measurament","text":"Categorical Variables: Qualitative discrete characteristics fall specific categories groups. numerical value order associated . Examples: gender (male/female), marital status (single/married/divorced), color (red/green/blue).Categorical Variables: Qualitative discrete characteristics fall specific categories groups. numerical value order associated . Examples: gender (male/female), marital status (single/married/divorced), color (red/green/blue).Ordinal Variables: Categories levels natural order ranking. Examples: survey ratings (e.g., Likert scale), education levels (e.g., high school, bachelor’s, master’s), socioeconomic status (e.g., low, medium, high).Ordinal Variables: Categories levels natural order ranking. Examples: survey ratings (e.g., Likert scale), education levels (e.g., high school, bachelor’s, master’s), socioeconomic status (e.g., low, medium, high).Interval Variables: Numerical values representing continuous scale intervals values meaningful consistent. Unlike ordinal variables, interval variables equally spaced intervals adjacent values, allowing meaningful arithmetic operations like addition subtraction. However, interval variables contain natural zero point; zero arbitrary merely reference point. Examples: temperatureInterval Variables: Numerical values representing continuous scale intervals values meaningful consistent. Unlike ordinal variables, interval variables equally spaced intervals adjacent values, allowing meaningful arithmetic operations like addition subtraction. However, interval variables contain natural zero point; zero arbitrary merely reference point. Examples: temperatureRatio Variables: Numerical variable shares characteristics interval variables also meaningful absolute zero point. Thus, type variables complete absence measured attribute/quantity. Examples: height (centimeters), weight (kilograms), time (seconds), distance (meters), income (dollars).Ratio Variables: Numerical variable shares characteristics interval variables also meaningful absolute zero point. Thus, type variables complete absence measured attribute/quantity. Examples: height (centimeters), weight (kilograms), time (seconds), distance (meters), income (dollars).carry meaningful arithmetic operations categorical ordinal variables; therefore, treat variables “character” “factors” R. R recognizes intervals different groups levels variables meaningful numerical sense.contrast, interval ratio variables considered “numerical” variables R. variables possess meaningful numerical scale equal intervals, case ratio variables, true zero point.","code":""},{"path":"linear-model.html","id":"non-parametric-method","chapter":"13 Linear Model","heading":"13.3 Non-parametric method","text":"assumptions linear models (e.g., normality, equal variances) met, nonparametric methods can used. methods assume specific probability distribution data can provide valid inference even skewed ordinal data.Examples common cases:Wilcoxon rank-sum test (Mann–Whitney U test)wilcox.test()Kruskal–Wallis testkruskal.test()","code":""},{"path":"linear-model.html","id":"laboratory-6","chapter":"13 Linear Model","heading":"13.4 Laboratory","text":"","code":""},{"path":"linear-model.html","id":"normality-assumption","chapter":"13 Linear Model","heading":"13.4.1 Normality Assumption","text":"dataset ToothGrowth comes Guinea pig experiment tested Vitamin C affects tooth growth. observation represents single guinea pig.experiment compared:Two supplement types: orange juice (OJ) ascorbic acid (VC)Two supplement types: orange juice (OJ) ascorbic acid (VC)Three dose levels: 0.5, 1, 2 mg/dayThree dose levels: 0.5, 1, 2 mg/dayUsing dataset,Develop linear model examining effect supplement type (supp), dose (dose), interaction tooth length (len). Assign developed model m_tooth.Develop linear model examining effect supplement type (supp), dose (dose), interaction tooth length (len). Assign developed model m_tooth.Linear Model framework assumes normality. verify validity normality assumption, typically employ Shapiro-Wilk test (shapiro.test() R). Discuss whether test applied (1) response variable (2) model residuals. , apply Shapiro-Wilk. Type ?shapiro.test R console learn usage.Linear Model framework assumes normality. verify validity normality assumption, typically employ Shapiro-Wilk test (shapiro.test() R). Discuss whether test applied (1) response variable (2) model residuals. , apply Shapiro-Wilk. Type ?shapiro.test R console learn usage.","code":""},{"path":"linear-model.html","id":"model-interpretation","chapter":"13 Linear Model","heading":"13.4.2 Model Interpretation","text":"Using m_tooth, create figure illustrating relationship tooth length (len; y-axis) dose (dose; x-axis), color indicating supplement type (supp). Include observed predicted values, similar Figure 13.1.Use following data frame prediction:","code":"\ndf_pred <- ToothGrowth %>%\n  group_by(supp) %>%\n  reframe(dose = seq(min(dose),\n                     max(dose),\n                     length = 100))"},{"path":"linear-model.html","id":"multicollinearity","chapter":"13 Linear Model","heading":"13.4.3 Multicollinearity","text":"Copy run following code create simulated dataset:Create Figures Response vs. Predictors.Using df_y, create following figures:scatter plot y (y-axis) versus x1 (x-axis)scatter plot y (y-axis) versus x1 (x-axis)scatter plot y (y-axis) versus x2 (x-axis)scatter plot y (y-axis) versus x2 (x-axis)Based plots, interpret effects x1 x2 y. x1 x2 appear positive negative influences y?Examining statistical effects x1 x2 y.Examine statistical effects x1 x2 y using lm() function.\nSuppose x1 x2 factors vary across samples. case, want account influence one variable assessing effect . , include predictors single linear model using + operator. Discuss whether statistical estimates x1 x2’s effects make sense.Create Figure Predictor vs. Predictor.Using df_y,Create scatter plot x1 x2.Create scatter plot x1 x2.Examine Pearson’s correlation x1 x2 using function cor(). (?cor() usage).Examine Pearson’s correlation x1 x2 using function cor(). (?cor() usage).","code":"\n## variance-covariance matrix\nmv <- rbind(c(1, 0.9),\n            c(0.9, 1))\n\n## true regression coefficients\nb <- c(0.05, 1.00)\n\n## produce simulated data\nset.seed(523)\nX <- MASS::mvrnorm(100,\n                   mu = c(0, 0),\n                   Sigma = mv)\n\ndf_y <- tibble(x1 = X[,1],\n               x2 = X[,2]) %>% \n  mutate(y = rnorm(nrow(.),\n                   mean = 1 + b[1] * x1 + b[2] * x2))"},{"path":"linear-model.html","id":"experimental-design","chapter":"13 Linear Model","heading":"13.4.4 Experimental Design","text":"designing experiment examine effects nutrient level predation prey fish growth. proposed treatment combinations :Low nutrient predator presentLow nutrient predator presentHigh nutrient predator presentHigh nutrient predator presentHigh nutrient predator absentHigh nutrient predator absentIn addition, plan record initial length prey fish (continuous variable), may influence subsequent growth.Question: Select appropriate statement regarding experiment can assess:experiment can capture effects nutrient, predation, interaction.experiment can capture effects nutrient, predation, interaction.experiment can capture effects nutrient predation. However, nutrient effect may limited presence predators, predator effect limited high nutrient condition.experiment can capture effects nutrient predation. However, nutrient effect may limited presence predators, predator effect limited high nutrient condition.experiment can capture effects nutrient predation. Main effects can assessed across nutrient conditions.experiment can capture effects nutrient predation. Main effects can assessed across nutrient conditions.experiment capture either main effects interaction.experiment capture either main effects interaction.","code":""},{"path":"generalized-linear-model.html","id":"generalized-linear-model","chapter":"14 Generalized Linear Model","heading":"14 Generalized Linear Model","text":"One key assumptions Linear Model framework Normality – error term follows normal distribution (Chapter 13). However, assumption frequently leads situations models predict unrealistic values response variable. chapter, introduce Generalized Linear Model (GLM) framework, enables flexible modeling.Key words: link function","code":""},{"path":"generalized-linear-model.html","id":"count-data","chapter":"14 Generalized Linear Model","heading":"14.1 Count Data","text":"","code":""},{"path":"generalized-linear-model.html","id":"plant-density","chapter":"14 Generalized Linear Model","heading":"14.1.1 Plant Density","text":"Recall data set garden plant counts (Chapter 9):\nFigure 14.1: Garden view sampling plots. White squares represent plots. Red dots indicate plant individuals counted.\nvariable plant count following characteristics:possible values discrete (count data) since counting number plant individuals plot.possible values discrete (count data) since counting number plant individuals plot.possible values always positive, counts negative.possible values always positive, counts negative.distribution appears non-symmetric around sample mean.distribution appears non-symmetric around sample mean.happens apply simple linear model assumes Normal distribution? Let model plant count function nitrate follows:\\[\n\\begin{aligned}\ny_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2)\\\\\n\\mu_i &= \\alpha + \\beta~\\text{nitrate}_i\n\\end{aligned}\n\\]seems satisfactory, anomalous behavior occurs plotting process figure. Plot data predicted values together.intersection predicted line x-axis suggests presence negative predicted values plant counts. However, logically implausible plant counts negative.occurrence stems nature model employed.Linear Model Normal error distribution, allow mean encompass wide range values, including negative values.Even mean predicted values intersect x-axis, consider possibility observations taking negative values, dictated nature Normal distribution (\\(\\mu_i + \\varepsilon_i\\) can negative even \\(\\mu_i > 0\\)), reality can never occur.","code":"\ndf_count <- read_csv(\"data_raw/data_garden_count.csv\")\nprint(df_count)## # A tibble: 30 × 3\n##     plot count nitrate\n##    <dbl> <dbl>   <dbl>\n##  1     1     3    31.3\n##  2     2     1    27.4\n##  3     3     4    31.9\n##  4     4     1    30.1\n##  5     5     4    29.8\n##  6     6     1    30.4\n##  7     7     2    32  \n##  8     8     0    18.3\n##  9     9     2    31.7\n## 10    10     2    34  \n## # ℹ 20 more rows\nm_normal <- lm(count ~ nitrate,\n               df_count)\n\nsummary(m_normal)## \n## Call:\n## lm(formula = count ~ nitrate, data = df_count)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.2782 -0.6020 -0.1625  0.5094  3.1272 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -4.17690    1.20371  -3.470   0.0017 ** \n## nitrate      0.21234    0.04068   5.219 1.52e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9975 on 28 degrees of freedom\n## Multiple R-squared:  0.4931, Adjusted R-squared:  0.475 \n## F-statistic: 27.24 on 1 and 28 DF,  p-value: 1.523e-05\n# extract estimates\nalpha <- coef(m_normal)[1] # intercept\nbeta <- coef(m_normal)[2] # slope\n\ndf_count %>% \n  ggplot(aes(x = nitrate,\n             y = count)) +\n  geom_point() +\n  geom_abline(intercept = alpha,\n              slope = beta)"},{"path":"generalized-linear-model.html","id":"poisson-model","chapter":"14 Generalized Linear Model","heading":"14.1.2 Poisson Model","text":"One possible approach consider assumption error term follows Poisson distribution. purpose discussion, let’s assume model based Poisson distribution appropriate. Unlike Normal distribution, Poisson distribution generates non-negative discrete values, makes suitable fit plant count variable. incorporate , can make following modifications model:\\[\n\\begin{aligned}\ny_i &\\sim \\text{Poisson}(\\lambda_i)\\\\\n\\log\\lambda_i &= \\alpha + \\beta~\\text{nitrate}_i\n\\end{aligned}\n\\]changes crucial:plant count, denoted \\(y_i\\), now assumed follow Poisson distribution parameter \\(\\lambda_i\\). ensures negative values \\(y_i\\) possible (.e., \\(\\Pr(y_i < 0) = 0\\)).mean \\(\\lambda_i\\) log-transformed expressing function nitrate. Given mean Poisson distribution negative, must restrict range \\(\\lambda_i\\) positive values. log-transformation guarantees positivity \\(\\lambda_i\\) (\\(\\lambda_i = \\exp(\\alpha + \\beta~\\text{nitrate}_i)\\)) irrespective values \\(\\alpha\\), \\(\\beta\\), \\(\\text{nitrate}_i\\).implementation Poisson model quite simple. following example, use function glm():major difference lm() argument family = \"poisson\". argument specifies probability distribution used; example, used Poisson distribution model count data. However, looking estimates…estimated intercept still negative…’s wrong? sweat, estimated parameters log scale; result translates :\\[\n\\lambda_i = \\exp(-4.05 + 0.17~\\text{nitrate}_i)\n\\]Therefore, even intercept estimated negative, mean \\(\\lambda_i\\) guaranteed non-negative.Another important difference lm() test-statistic long t-statistic – instead, glm() reports z-statistic. However, z-statistic similar t-statistic, defined \\(z=\\frac{\\hat{\\theta} - \\theta_0}{\\text{SE}(\\hat{\\theta})}\\) (\\(\\hat{\\theta} \\\\{\\hat{\\alpha}, \\hat{\\beta}\\}\\), \\(\\theta_0 = 0\\)). Therefore, can reproduce z-statistic dividing parameter estimates (\\(\\hat{\\alpha}\\) \\(\\hat{\\beta}\\)) standard errors:call statistic “z-statistic” known follow z-distribution (also known standardized Normal distribution \\(\\text{Normal}(0, 1)\\)), opposed Student’s t-distribution. reported p-value (Pr(>|z|)) estimated assumption, can seen follows.Notice p-values identical output Normal model – therefore, choice probability distribution critically affects results statistical analysis, often times, qualitatively (particular example though).Visualization provides convincing difference Normal Poisson models (Figure 14.2).\nFigure 14.2: Comparison predicted values Normal (broken, black) Poisson models (solid, red).\nClearly, Poisson model better job.","code":"\nm_pois <- glm(count ~ nitrate,\n              data = df_count,\n              family = \"poisson\")\nsummary(m_pois)## \n## Call:\n## glm(formula = count ~ nitrate, family = \"poisson\", data = df_count)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -4.70258    1.58317  -2.970 0.002974 ** \n## nitrate      0.17751    0.05035   3.525 0.000423 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 32.015  on 29  degrees of freedom\n## Residual deviance: 13.628  on 28  degrees of freedom\n## AIC: 86.661\n## \n## Number of Fisher Scoring iterations: 5\n# parameter estimates and their SEs\ntheta <- coef(m_pois)\nse <- sqrt(diag(vcov(m_pois)))\nz_value <- theta / se\n\nprint(z_value)## (Intercept)     nitrate \n##   -2.970364    3.525377\n# estimate Pr(>|z|) using a standardized normal distribution\np_value <- (1 - pnorm(abs(z_value), mean = 0, sd = 1)) * 2\nprint(p_value)##  (Intercept)      nitrate \n## 0.0029744722 0.0004228806\n# make predictions\ndf_pred <- tibble(nitrate = seq(min(df_count$nitrate),\n                                max(df_count$nitrate),\n                                length = 100))\n\n# y_pois is exponentiated because predict() returns values in log-scale\ny_normal <- predict(m_normal, newdata = df_pred)\ny_pois <- predict(m_pois, newdata = df_pred) %>% exp()\n\ndf_pred <- df_pred %>% \n  mutate(y_normal,\n         y_pois)\n\n# figure\ndf_count %>% \n  ggplot(aes(x = nitrate,\n             y = count)) +\n  geom_point() +\n  geom_line(data = df_pred,\n            aes(y = y_normal),\n            linetype = \"dashed\") +\n  geom_line(data = df_pred,\n            aes(y = y_pois),\n            color = \"salmon\")"},{"path":"generalized-linear-model.html","id":"proportional-data","chapter":"14 Generalized Linear Model","heading":"14.2 Proportional Data","text":"","code":""},{"path":"generalized-linear-model.html","id":"mussel-egg-fertilization","chapter":"14 Generalized Linear Model","heading":"14.2.1 Mussel Egg Fertilization","text":"another type data Normal distribution suitable - proportional data. Let’s consider example dataset investigation conducted number fertilized eggs 30 eggs examined 100 female freshwater mussels. can download dataset .Male freshwater mussels release sperm water, drawn female mussels downstream. Considering nature fertilization process, female mussels expected higher probability fertilization surrounded greater number males. explore relationship, let’s load data R visualize .data frame contains variables ind_id (mussel individual ID), n_fertilized (number fertilized eggs), n_examined (number eggs examined), density (number mussels 1 m\\(^2\\) quadrat). visualize relationship proportion eggs fertilized density gradient, plot proportion fertilized eggs density gradient (Figure 14.3).\nFigure 14.3: Relationship proportion fertilized eggs mussel density surround.\nexample, response variable, represents number eggs fertilized, possesses following characteristics:Discreteness: variable takes discrete values rather continuous ones.Discreteness: variable takes discrete values rather continuous ones.Upper limit: number eggs fertilized bounded upper limit (case, 30).Upper limit: number eggs fertilized bounded upper limit (case, 30).discrete nature variable might initially lead one consider Poisson distribution potential candidate. However, due presence upper limit number fertilized eggs exceed number eggs examined, Poisson distribution suitable option.","code":"\ndf_mussel <- read_csv(\"data_raw/data_mussel.csv\")\nprint(df_mussel)## # A tibble: 100 × 4\n##    ind_id n_fertilized n_examined density\n##     <dbl>        <dbl>      <dbl>   <dbl>\n##  1      1            0         30       6\n##  2      2           17         30      15\n##  3      3           21         30      15\n##  4      4            3         30       9\n##  5      5            1         30       9\n##  6      6            7         30      11\n##  7      7            2         30       9\n##  8      8            0         30       2\n##  9      9            1         30       9\n## 10     10            0         30       7\n## # ℹ 90 more rows\n# calculate the proportion of fertilized eggs\ndf_mussel <- df_mussel %>% \n  mutate(prop_fert = n_fertilized / n_examined)\n\n# plot\ndf_mussel %>% \n  ggplot(aes(x = density,\n             y = prop_fert)) +\n  geom_point() +\n  labs(y = \"Proportion of eggs fertilized\",\n       x = \"Mussel density\")"},{"path":"generalized-linear-model.html","id":"binomial-model","chapter":"14 Generalized Linear Model","heading":"14.2.2 Binomial Model","text":"Binomial distribution natural appropriate choice modeling variable. Binomial distribution characterized two parameters: number trials (\\(N\\)) probability success (\\(p\\)). trial certain probability success, outcome trial either assigned value 1 (indicating success) 0 (indicating failure). given example, can consider “fertilization” successful outcome among 30 trials. Consequently, number eggs fertilized (\\(y_i\\)) individual mussel (\\(\\)) can described :\\[\ny_i \\sim \\text{Binomial}(N_i, p_i)\n\\]formulation adequately captures nature response variable, outcome Binomial distribution constrained within range \\(0\\) \\(N_i\\), aligning upper limit imposed number examined eggs.prediction fertilization probability \\(p_i\\) increase increasing mussel’s density – relate \\(p_i\\) mussel density? value \\(p_i\\) must constrained within range \\(0.0-1.0\\) “probability.” Given , log-transformation, used Poisson model, suitable choice. Instead, can use logit-transformation convert linear formula fall within range \\(0.0 - 1.0\\).\\[\n\\begin{aligned}\ny_i &\\sim \\text{Binomial}(N_i, p_i)\\\\\n\\log(\\frac{p_i}{1-p_i}) &= \\alpha + \\beta~\\text{density}_i\n\\end{aligned}\n\\]logit transformation, represented \\(\\log\\left(\\frac{p_i}{1-p_i}\\right)\\), guarantees values \\(p_i\\) confined within range 0.0 1.0. relationship expressed following equation:\\[\np_i = \\frac{\\exp(\\alpha + \\beta~\\text{density}_i)}{1 + \\exp(\\alpha + \\beta~\\text{density}_i)}\n\\]straightforward coding can verify .\nFigure 14.4: Logit-transformation ensures transformed variable fall range zero one.\nglm() function can used implement modeling approach. However, certain modifications need made response variable order ensure compatibility binomial distribution.contrast Normal Poisson models, response variable encoded using cbind(n_fertilized, n_examined - n_fertilized) function. cbind() function combines number successes (fertilized) failures (fertilized) single matrix. approach allows modeling data using binomial distribution, taking account successes total number trials within observation.Similar previous examples, summary() function provides estimates model parameters. applying summary() function fitted model, can obtain information coefficient estimates, standard errors, p-values, relevant statistics.result translates :\\[\n\\begin{aligned}\ny_i &\\sim \\text{Binomial}(N_i, p_i)\\\\\n\\log(\\frac{p_i}{1 - p_i}) &= -8.06 + 0.34~\\text{density}_i\n\\end{aligned}\n\\]make predictions, necessary back-transform predicted fertilization probability since estimated logit scale. back-transformation convert predicted values back original probability scale (\\(0.0-1.0\\)).Draw figure (Figure 14.5).\nFigure 14.5: Prediction Binomial model.\nPerfect.","code":"\n# x: produce 100 numbers from -100 to 100 (assume logit scale)\n# y: convert with inverse-logit transformation (ordinary scale)\ndf_test <- tibble(logit_x = seq(-10, 10, length = 100),\n                  x = exp(logit_x) / (1 + exp(logit_x)))\n\ndf_test %>% \n  ggplot(aes(x = logit_x,\n             y = x)) +\n  geom_point() +\n  geom_line() +\n  labs(y = \"x\",\n       x = \"logit(x)\")\nm_binom <- glm(cbind(n_fertilized, n_examined - n_fertilized) ~ density,\n               data = df_mussel,\n               family = \"binomial\")\ncbind(df_mussel$n_fertilized, df_mussel$n_examined - df_mussel$n_fertilized) %>% \n  head()##      [,1] [,2]\n## [1,]    0   30\n## [2,]   17   13\n## [3,]   21    9\n## [4,]    3   27\n## [5,]    1   29\n## [6,]    7   23\nsummary(m_binom)## \n## Call:\n## glm(formula = cbind(n_fertilized, n_examined - n_fertilized) ~ \n##     density, family = \"binomial\", data = df_mussel)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -7.85097    0.27150  -28.92   <2e-16 ***\n## density      0.58219    0.02142   27.18   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1676.42  on 99  degrees of freedom\n## Residual deviance:  104.66  on 98  degrees of freedom\n## AIC: 317.64\n## \n## Number of Fisher Scoring iterations: 5\n# make prediction\ndf_pred <- tibble(density = seq(min(df_mussel$density),\n                                max(df_mussel$density),\n                                length = 100))\n\n# y_binom is inv.logit-transformed because predict() returns values in logit-scale\ny_binom <- predict(m_binom, newdata = df_pred) %>% boot::inv.logit()\n\ndf_pred <- df_pred %>% \n  mutate(y_binom)\ndf_mussel %>% \n  ggplot(aes(x = density,\n             y = prop_fert)) +\n  geom_point() +\n  geom_line(data = df_pred,\n            aes(y = y_binom)) +\n  labs(y = \"Proportion of eggs fertilized\",\n       x = \"Mussel density\")"},{"path":"generalized-linear-model.html","id":"the-glm-framework","chapter":"14 Generalized Linear Model","heading":"14.3 The GLM Framework","text":"models, including assuming Normal distribution, fall category Generalized Linear Model (GLM) framework. framework follows common structure:\\[\n\\begin{aligned}\ny_i &\\sim \\text{D}(\\Theta)\\\\\n\\text{Link}(\\mbox{E}(y_i)) &= \\alpha + \\sum_k \\beta_k x_{k, }\n\\end{aligned}\n\\]structure, \\(\\Theta\\) represents parameter vector probability distribution, \\(\\text{D}(\\cdot)\\) \\(\\text{Link}(\\cdot)\\) denote chosen probability distribution associated link function, respectively. example, select Poisson distribution \\(\\text{D}(\\cdot)\\), corresponding \\(\\text{Link}(\\cdot)\\) function natural logarithm (log) (see Section 14.1.2). expected value \\(\\mbox{E}(y_i)\\) expressed function parameter(s) probability distribution \\(f(\\Theta)\\), related explanatory variables \\(x\\). instance, Normal Poisson models, model mean, Binomial model, model success probability. certain distributions, \\(f(\\Theta)\\) may function multiple parameters.GLM framework differs Linear Model framework offering greater flexibility choice probability distribution, achieved introduction Link function. function allows us constrain modeled parameter within specific range. non-normal distributions common natural phenomena, framework plays critical role modern statistical analysis.However, determine appropriate probability distribution? single correct answer question, clear factors consider—specifically, characteristics response variable. following criteria can particularly helpful:variable discrete?upper bound?sample variance far greater mean?assist selecting appropriate probability distribution modeling, provided concise decision tree graph. Please note choices presented frequently encountered ecological modeling, rather exhaustive list probability distributions (see Table 14.1). full list probability distributions can found (Wikipedia page)\nFigure 14.6: Tree chart choose probability distribution GLM analysis.\nTable 14.1:  Common distributions used GLM analysis","code":""},{"path":"generalized-linear-model.html","id":"laboratory-7","chapter":"14 Generalized Linear Model","heading":"14.4 Laboratory","text":"","code":""},{"path":"generalized-linear-model.html","id":"glm-exercise","chapter":"14 Generalized Linear Model","heading":"14.4.1 GLM exercise","text":"publicly available stream fish distribution dataset can accessed online repository:\n\nurl <- \"https://raw.githubusercontent.com/aterui/public-proj_fish-richness-shubuto/master/data/data_vpart.csv\"\ndf_fish <- read_csv(url)\ndataset comprises information number species found site (n_sp) several associated environmental variables, including distance sea (distance), catchment area (cat_area), environmental heterogeneity (hull_area). environmental variables factors known influence fish distributions. analyze dataset, develop Generalized Linear Model (GLM) follows:\nUse fish species richness n_sp response variable model appropriate probability distribution (family). Identify predictor variable(s) significantly influence response variable.\npublicly available stream fish distribution dataset can accessed online repository:dataset comprises information number species found site (n_sp) several associated environmental variables, including distance sea (distance), catchment area (cat_area), environmental heterogeneity (hull_area). environmental variables factors known influence fish distributions. analyze dataset, develop Generalized Linear Model (GLM) follows:Use fish species richness n_sp response variable model appropriate probability distribution (family). Identify predictor variable(s) significantly influence response variable.mtcars dataset contains information 32 car models, including performance design characteristics. variable indicates type transmission, 0 = automatic 1 = manual.\nUsing dataset, develop GLM explore:\nvariables significantly influence probability car manual transmission? Choose appropriate probability distribution (family). Consider including following predictors: miles per gallon (mpg), horsepower (hp), weight (wt).\nDevelop GLM normal distribution (family = \"gaussian\"). Compare results model appropriate probability distribution.\nmtcars dataset contains information 32 car models, including performance design characteristics. variable indicates type transmission, 0 = automatic 1 = manual.Using dataset, develop GLM explore:variables significantly influence probability car manual transmission? Choose appropriate probability distribution (family). Consider including following predictors: miles per gallon (mpg), horsepower (hp), weight (wt).variables significantly influence probability car manual transmission? Choose appropriate probability distribution (family). Consider including following predictors: miles per gallon (mpg), horsepower (hp), weight (wt).Develop GLM normal distribution (family = \"gaussian\"). Compare results model appropriate probability distribution.Develop GLM normal distribution (family = \"gaussian\"). Compare results model appropriate probability distribution.","code":"\nurl <- \"https://raw.githubusercontent.com/aterui/public-proj_fish-richness-shubuto/master/data/data_vpart.csv\"\ndf_fish <- read_csv(url)"},{"path":"generalized-linear-model.html","id":"effect-size","chapter":"14 Generalized Linear Model","heading":"14.4.2 Effect size","text":"Often, interested comparing effect sizes different explanatory variables. However, use raw environmental values, regression coefficients directly comparable due differing units. Recall regression coefficients represent increment \\(y\\) per unit increase \\(x\\). facilitate comparison, standardize explanatory variables subtracting means dividing standard deviations:\\[ \\mbox{E}(y) = \\alpha + \\beta x \\rightarrow \\mbox{E}(y) = \\alpha' + \\beta' \\frac{(x - \\mu_x)}{\\sigma_x} \\], \\(\\alpha'\\) \\(\\beta'\\) represent intercept slope standardizing \\(x\\), respectively.function scale() perform standardization. Create columns standardized distance (std_dist), cat_area (std_cat), hull_area (std_hull).function scale() perform standardization. Create columns standardized distance (std_dist), cat_area (std_cat), hull_area (std_hull).Perform GLM analysis fish species richness standardized variables, identify influential variable among terms effect size.Perform GLM analysis fish species richness standardized variables, identify influential variable among terms effect size.","code":""},{"path":"generalized-linear-model.html","id":"offset-term","chapter":"14 Generalized Linear Model","heading":"14.4.3 Offset term","text":"Poisson distribution (distributions used modeling count data) assumes observations collected comparable sampling efforts. practice, however, survey data often differ effort — example, counts may come surveys conducted longer search times larger areas. see issue, copy paste following code:\n\nurl <- \"https://raw.githubusercontent.com/aterui/biostats/master/data_raw/data_offset.csv\"\ndf_offset <- read_csv(url)\nUsing df_offset, perform following:\nPlot relationship count nitrate.\nPlot relationship count area.\nPlot relationship count/area nitrate.\nPoisson distribution (distributions used modeling count data) assumes observations collected comparable sampling efforts. practice, however, survey data often differ effort — example, counts may come surveys conducted longer search times larger areas. see issue, copy paste following code:Using df_offset, perform following:Plot relationship count nitrate.Plot relationship count area.Plot relationship count/area nitrate.Directly comparing counts misleading higher effort naturally increases expected number detections. One might consider dividing response variable effort standardize , changes outcome discrete count continuous rate, violating assumptions count models. Confirm following code returns numerous wranings – density discrete variable:\n\ndf_offset <- df_offset %>% \n  mutate(density = count / area)\n\nglm(density ~ nitrate,\n    data = df_offset,\n    family = \"poisson\")\nbetter approach incorporate sampling effort offset term glm(), allows model account differences effort preserving count nature response variable. Perform following exercise:\nDevelop GLM response variable count predictor nitrate.\nDevelop GLM response variable count, predictor nitrate, offset term offset(log(area)) (~ nitrate + offset(log(area)) work).\nCompare results two models.\nDirectly comparing counts misleading higher effort naturally increases expected number detections. One might consider dividing response variable effort standardize , changes outcome discrete count continuous rate, violating assumptions count models. Confirm following code returns numerous wranings – density discrete variable:better approach incorporate sampling effort offset term glm(), allows model account differences effort preserving count nature response variable. Perform following exercise:Develop GLM response variable count predictor nitrate.Develop GLM response variable count, predictor nitrate, offset term offset(log(area)) (~ nitrate + offset(log(area)) work).Compare results two models.","code":"\nurl <- \"https://raw.githubusercontent.com/aterui/biostats/master/data_raw/data_offset.csv\"\ndf_offset <- read_csv(url)\ndf_offset <- df_offset %>% \n  mutate(density = count / area)\n\nglm(density ~ nitrate,\n    data = df_offset,\n    family = \"poisson\")"},{"path":"generalized-linear-model.html","id":"overdispersion","chapter":"14 Generalized Linear Model","heading":"14.4.4 Overdispersion","text":"Copy paste following code:dataset contains data tadpole counts (tadpole), aquatic vegetation cover (aqveg), pond permanence (number days persist; permanence).Plot relationships () tadpole aqveg (b) tadpole permanence.Plot relationships () tadpole aqveg (b) tadpole permanence.Discuss group variable may significant influence.Discuss group variable may significant influence.Develop GLM (choose appropriate distribution) explaining tadpole aqveg permanence.Develop GLM (choose appropriate distribution) explaining tadpole aqveg permanence.","code":"\nurl <- \"https://raw.githubusercontent.com/aterui/biostats/master/data_raw/data_tadpole.csv\"\ndf_tadpole <- read_csv(url)"},{"path":"likelihood.html","id":"likelihood","chapter":"15 Likelihood","heading":"15 Likelihood","text":"Chapter 14, introduced non-normal distributions represent count proportional data. glm() function used obtain parameter estimates, function achieve ? reality, least squares approach applicable normal distributions. Therefore, require alternative approach estimate parameters within GLM framework. section, introduce concept likelihood fundamental principle parameter estimation GLM framework.","code":""},{"path":"likelihood.html","id":"finding-the-more-likely","chapter":"15 Likelihood","heading":"15.1 Finding the “More Likely”","text":"least squares method, goal minimize sum squared errors, ensuring fitted model smallest deviation observed data points. However, approach applicable non-normal variables. cases, need rely “probability” observing data point numerical value.Let’s consider example count data, denoted \\(\\pmb{y} = \\{y_1, y_2, ..., y_N\\}\\). simplify matter, let’s focus specific data point, say \\(y_1 = 3\\). Assuming Poisson distribution, can calculate probability observing particular value using arbitrary parameter value mean, denoted \\(\\lambda\\). refresh memory, can refer back Poisson distribution discussed Chapter 9.\\[\n\\Pr(y = k) = \\frac{\\lambda^{k}\\exp(-\\lambda)}{k!}\n\\]equation represents probability variable \\(y\\) taking specific value \\(k\\). current example, interested case \\(k = 3\\). specifying value mean parameter \\(\\lambda\\), can compute probability associated value. Let’s try using \\(\\lambda = 3.5\\) calculation:\\[\n\\Pr(y_1 = 3) = \\frac{3.5^3 \\exp(-3.5)}{3!} \\approx 0.215\n\\]can calculate dpois() R:assume mean \\(3.5\\) Poisson distribution, probability observing value \\(3\\) approximately \\(0.216\\). can experiment different values \\(\\lambda\\) see potentially “better” value. exploring different values \\(\\lambda\\), can observe probability observing specific value changes. allows us assess value \\(\\lambda\\) provides better fit captures data accurately. following example explores \\(\\lambda = 0 - 10\\) \\(0.1\\) interval.Make plot:\nFigure 15.1: Relationship parameter \\(\\lambda\\) probability observing value three.\nexamining figure data, appears probability highest around \\(\\lambda = 3\\). confirm observation, let’s arrange data frame df_pois.reasoning sound probability \\(y_1\\) equal \\(3\\) maximized \\(\\lambda = 3\\). represents simplest form likelihood function denoted \\(L(\\lambda~|~y_1)\\), \\((\\cdot~|~y_1)\\) indicates \\(y_1\\) fixed given. evaluated likelihood various \\(\\lambda\\) considering given value \\(y_1\\).However, multiple values \\(y_i\\)? Let’s consider scenario \\(\\pmb{y} = \\{3, 2, 5\\}\\). cases, must account probability simultaneously observing values. events independent, probability observing multiple events can expressed product individual probabilities. Consequently, likelihood function takes following form:\\[\n\\begin{aligned}\nL(\\lambda~|~\\pmb{y}) &= \\Pr(y_1 = 3) \\times \\Pr(y_2 = 2) \\times \\Pr(y_3 = 5)\\\\\n&= \\frac{\\lambda^{3}\\exp(-\\lambda)}{3!} \\times \\frac{\\lambda^{2}\\exp(-\\lambda)}{2!} \\times \\frac{\\lambda^{5}\\exp(-\\lambda)}{5!}\\\\\n&= \\prod_i^3 \\frac{\\lambda^{y_i} \\exp(-\\lambda)}{y_i!}\n\\end{aligned}\n\\]Implement R:Similar previous example, let’s search suitable value \\(\\lambda\\) better captures observed data.\nFigure 15.2: Likelihood observing three count data points \\(\\pmb{y}\\) simultaneously.\nexercise interval \\(0.01\\), \\(\\lambda \\approx 3.33\\) identified suitable value. Interestingly, sample mean matches value:? See .","code":"\n# dpois()\n# the first argument is \"k\"\n# the second argument is \"lambda\"\ndpois(3, lambda = 3.5)## [1] 0.2157855\n# write the formula to confirm\n(p <- (3.5^3 * exp(-3.5)) / factorial(3))## [1] 0.2157855\n# change lambda from 0 to 10 by 0.1\nlambda <- seq(0, 10, by = 0.1)\n\n# probability\npr <- dpois(3, lambda = lambda)\n\n# create a data frame\ndf_pois <- tibble(y = 3,\n                  lambda = lambda,\n                  pr = pr)\n\nprint(df_pois)## # A tibble: 101 × 3\n##        y lambda       pr\n##    <dbl>  <dbl>    <dbl>\n##  1     3    0   0       \n##  2     3    0.1 0.000151\n##  3     3    0.2 0.00109 \n##  4     3    0.3 0.00333 \n##  5     3    0.4 0.00715 \n##  6     3    0.5 0.0126  \n##  7     3    0.6 0.0198  \n##  8     3    0.7 0.0284  \n##  9     3    0.8 0.0383  \n## 10     3    0.9 0.0494  \n## # ℹ 91 more rows\ndf_pois %>% \n  ggplot(aes(x = lambda,\n             y = pr)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"lambda\",\n       y = \"Pr(k = 3)\")\n# arrange() re-orders the dataframe based on the speficied column in an ascending order\n# desc() flips the order (descending)\n\ndf_pois %>% \n  arrange(desc(pr)) %>% \n  print()## # A tibble: 101 × 3\n##        y lambda    pr\n##    <dbl>  <dbl> <dbl>\n##  1     3    3   0.224\n##  2     3    3.1 0.224\n##  3     3    2.9 0.224\n##  4     3    3.2 0.223\n##  5     3    2.8 0.222\n##  6     3    3.3 0.221\n##  7     3    2.7 0.220\n##  8     3    3.4 0.219\n##  9     3    2.6 0.218\n## 10     3    3.5 0.216\n## # ℹ 91 more rows\n# try lambda = 3 for y = 3, 2, 5\npr <- dpois(c(3, 2, 5), lambda = 3)\nprint(pr)## [1] 0.2240418 0.2240418 0.1008188\n# probability of observing 3, 2, 5 simultaneously\n# with lambda = 3\nprod(pr)## [1] 0.005060573\n# lambda = 0 - 10 by 0.01\ny <- c(3, 2, 5)\nlambda <- seq(0, 10, by = 0.01)\n\n# sapply repeats the task in FUN\n# each element in \"X\" will be sequencially substituted in \"z\"\npr <- sapply(X = lambda,\n             FUN = function(z) prod(dpois(y, lambda = z)))\n\n# make a data frame and arrange by pr (likelihood)\ndf_pois <- tibble(lambda = lambda,\n                  pr = pr)\n\ndf_pois %>% \n  arrange(desc(pr)) %>% \n  print()## # A tibble: 1,001 × 2\n##    lambda      pr\n##     <dbl>   <dbl>\n##  1   3.33 0.00534\n##  2   3.34 0.00534\n##  3   3.32 0.00534\n##  4   3.35 0.00534\n##  5   3.31 0.00534\n##  6   3.36 0.00534\n##  7   3.3  0.00534\n##  8   3.37 0.00534\n##  9   3.29 0.00533\n## 10   3.38 0.00533\n## # ℹ 991 more rows\n# visualize\ndf_pois %>% \n  ggplot(aes(x = lambda,\n             y = pr)) +\n  geom_line() +\n  labs(y = \"Likelihood\")\nmean(c(3, 2, 5))## [1] 3.333333"},{"path":"likelihood.html","id":"maximum-likelihood-method","chapter":"15 Likelihood","heading":"15.2 Maximum Likelihood Method","text":"","code":""},{"path":"likelihood.html","id":"simple-case","chapter":"15 Likelihood","heading":"15.2.1 Simple case","text":"Finding parameter value maximizes probability observing set observed values known Maximum Likelihood Estimate (MLE). estimation method commonly employed GLM framework various statistical inference techniques. MLE approach applicable long likelihood can defined, making versatile method many statistical analyses.However, procedure employed previous section (Section 15.1) mathematically rigorous optimal value \\(\\lambda\\) dependent resolution interval. interval becomes smaller, possible discover even better value \\(\\lambda\\) ad infinitum due continuous nature.reliable approach identifying “peak” utilizing first-order derivative. first-order derivative represents slope likelihood function given \\(\\lambda\\) value. derivative equals zero, indicates reached peak likelihood function. can leverage mathematical principle determine maximum likelihood estimate \\(\\lambda\\). Since computing derivative product mathematically challenging, can apply logarithm transformation likelihood function. Recall \\(\\log ab = \\log + \\log b\\). taking logarithm, can convert product operation \\(\\prod\\) summation operation \\(\\sum\\). Specifically:\\[\n\\begin{aligned}\n\\log L(\\lambda | \\pmb{y}) &= \\log \\prod_i^N \\frac{\\lambda^{y_i} \\exp(-\\lambda)}{y_i!}\\\\\n&= \\log \\frac{\\lambda^{y_1} \\exp(-\\lambda)}{y_1!} + \\log \\frac{\\lambda^{y_2} \\exp(-\\lambda)}{y_2!} + ... + \\log \\frac{\\lambda^{y_N} \\exp(-\\lambda)}{y_N!}\\\\\n&= \\sum_i^N \\log \\frac{\\lambda^{y_i} \\exp(-\\lambda)}{y_i!}\\\\\n&= \\sum_i^N (y_i \\log \\lambda -\\lambda -\\log y_i!)\n\\end{aligned}\n\\]Taking first derivative \\(\\log L(\\lambda | \\pmb{y})\\):\\[\n\\begin{aligned}\n\\frac{\\partial\\log L(\\lambda|\\pmb{y})}{\\partial \\lambda} &= \\sum_i^N (\\frac{y_i}{\\lambda} -1)\\\\\n&= \\frac{\\sum_i^N y_i}{\\lambda} - \\sum_i^N 1\\\\\n&= \\frac{\\sum_i^N y_i}{\\lambda} - N\\\\\n\\end{aligned}\n\\] obtain value \\(\\lambda\\) maximizes likelihood, set derivative equal zero:\\[\n\\begin{aligned}\n\\frac{\\partial\\log L(\\lambda|\\pmb{y})}{\\partial \\lambda} &= \\frac{\\sum_i^N y_i}{\\lambda} - N = 0\\\\\n\\lambda &= \\frac{\\sum_i^N y_i}{N}\n\\end{aligned}\n\\]Interesting – maximum likelihood estimate \\(\\lambda\\) equal sample mean. reason use sample mean estimate \\(\\lambda\\) Chapter 9.","code":""},{"path":"likelihood.html","id":"general-case","chapter":"15 Likelihood","heading":"15.2.2 General case","text":"practice, obtaining MLE analytically can highly complex often infeasible. Consequently, GLM framework relies numerical search algorithms approximate MLE estimates (see Ben Bolker’s book chapter full details; particular, quasi-Newton methods). Within GLMs, common establish relationships mean \\(\\lambda\\) explanatory variables order investigate influences. instance:\\[\n\\begin{aligned}\ny_i &\\sim \\mbox{Poisson}(\\lambda_i)\\\\\n\\log(\\lambda_i) &= \\alpha + \\sum_k \\beta_k x_k\n\\end{aligned}\n\\]\\(x_k\\) represents \\(k\\)th explanatory variable. translates :\\[\n\\log L(\\alpha, \\beta_1,...\\beta_k|\\pmb{y}) = \\sum_i \\left[y_i (\\alpha + \\sum_k\\beta_k x_k) - \\exp(\\alpha + \\sum_k \\beta_k x_k) - \\log y_i! \\right]\n\\]solving \\(k+1\\) partial derivatives:\\[\n\\begin{aligned}\n\\frac{\\partial \\log L(\\alpha, \\beta_1,...\\beta_k|\\pmb{y})}{\\partial \\alpha} &= 0\\\\\n\\frac{\\partial \\log L(\\alpha, \\beta_1,...\\beta_k|\\pmb{y})}{\\partial \\beta_1} &= 0\\\\\n&...\\\\\n\\frac{\\partial \\log L(\\alpha, \\beta_1,...\\beta_k|\\pmb{y})}{\\partial \\beta_k} &= 0\\\\\n\\end{aligned}\n\\]Thus, function glm() lot us! log likelihood fitted model can extracted logLik():","code":"\n# load garden plant data\n# df_count <- read_csv(\"data_raw/data_garden_count.csv\")\n\nm_pois <- glm(count ~ nitrate,\n              data = df_count,\n              family = \"poisson\")\n\nlogLik(m_pois)## 'log Lik.' -41.33051 (df=2)"},{"path":"likelihood.html","id":"laboratory-8","chapter":"15 Likelihood","heading":"15.3 Laboratory","text":"","code":""},{"path":"likelihood.html","id":"binomial-distribution","chapter":"15 Likelihood","heading":"15.3.1 Binomial Distribution","text":"binomial distribution probability distribution describes number successes fixed number independent Bernoulli trials, trial probability success, denoted \\(p\\). probability mass function (PMF) binomial distribution given :\\[\n\\Pr(y = k) = \\begin{pmatrix} N\\\\ k \\end{pmatrix} p^k(1 - p)^{N-k}\n\\]\\(\\begin{pmatrix} N\\\\ k \\end{pmatrix}\\) binomial coefficient, representing number ways choose \\(k\\) successes \\(N\\) trials.function dbinom() calculates \\(\\Pr(y = k)\\) binomial distribution. Using function, calculate likelihood observing vector \\(\\pmb{y} = \\{2, 2, 0, 0, 3, 1, 3, 3, 4, 3\\}\\) following values \\(p\\): \\(p \\\\{0, 0.01, 0.02, ..., 1.0\\}\\), \\(N = 10\\).function dbinom() calculates \\(\\Pr(y = k)\\) binomial distribution. Using function, calculate likelihood observing vector \\(\\pmb{y} = \\{2, 2, 0, 0, 3, 1, 3, 3, 4, 3\\}\\) following values \\(p\\): \\(p \\\\{0, 0.01, 0.02, ..., 1.0\\}\\), \\(N = 10\\).set \\(p\\) examined, find parameter value maximizes likelihood.set \\(p\\) examined, find parameter value maximizes likelihood.Using \\(p\\) maximizes likelihood, calculate \\(N \\times p\\). Compare value sample mean vector \\(\\pmb{y}\\).Using \\(p\\) maximizes likelihood, calculate \\(N \\times p\\). Compare value sample mean vector \\(\\pmb{y}\\).","code":""},{"path":"likelihood.html","id":"normal-distribution-1","chapter":"15 Likelihood","heading":"15.3.2 Normal Distribution","text":"normal distribution continuous probability distribution describes distribution continuous random variable. characterized mean \\(\\mu\\) standard deviation \\(\\sigma\\). probability density function (PDF) normal distribution given :\\[\nf(y) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\left(-\\frac{(y - \\mu)^2}{2 \\sigma^2}\\right)\n\\]normal distribution deals continuous variables, PDF returns probability density instead probability. However, can define likelihood observing entire vector \\(\\pmb{y}\\) similar way:\\[\nL(\\mu, \\sigma | \\pmb{y}) = \\prod_{}^N f(y_i)\n\\]Yield logarithm \\(L(\\mu, \\sigma | \\pmb{y})\\).Yield logarithm \\(L(\\mu, \\sigma | \\pmb{y})\\).Derive first-order derivative log-likelihood function respect \\(\\mu\\) solve \\(\\mu\\) setting \\(\\frac{\\partial \\log L(\\mu, \\sigma)}{\\partial \\mu} = 0\\). Assume \\(\\sigma\\) constant.Derive first-order derivative log-likelihood function respect \\(\\mu\\) solve \\(\\mu\\) setting \\(\\frac{\\partial \\log L(\\mu, \\sigma)}{\\partial \\mu} = 0\\). Assume \\(\\sigma\\) constant.","code":""},{"path":"model-comparison.html","id":"model-comparison","chapter":"16 Model Comparison","heading":"16 Model Comparison","text":"acquired knowledge constructing models evaluating performance. However, challenge lies selecting “optimal” model pool potential candidates. may think can employ metrics like Coefficient Determination (\\(\\mbox{R}^2\\)) compare choose superior models; however, predicament arises metrics tend favor models larger number explanatory variables, even variables actually irrelevant contribute meaningfully model’s effectiveness. Chapter, introduce popular methods balance trade-goodness fit model’s complexity.Key words: adjusted \\(\\mbox{R}^2\\), likelihood ratio test, Akaike’s Information Criterion","code":""},{"path":"model-comparison.html","id":"model-fit-and-complexity","chapter":"16 Model Comparison","heading":"16.1 Model Fit and Complexity","text":"field biological sciences, common encounter multiple hypotheses need exploratory analysis. instances, becomes necessary develop various models, corresponding specific hypothesis, subsequently compare respective performances. crucial aspect lies quantify “performance” models, acknowledging term can somewhat ambiguous. determination model performance ultimately dictates model superior, profoundly impacts conclusions drawn research. Thus, significance process overstated.One potential approach utilize Coefficient Determination (Chapter 12) measure model performance. metrics provide indication proportion variation response variable explained model, making reasonable options. However, situations measures meaningful model comparison.illustrate , let’s consider following simulated data. advantage using simulated data already know correct outcomes, enabling us validate verify analysis results.\nFigure 16.1: Simulated data\ncode generated response variable, denoted \\(y\\), function variable \\(x_1\\) (\\(y_i = \\beta_0 + \\beta_1 x_1 + \\varepsilon_i\\)). values intercept slope set 0.1 0.5 respectively. Given knowledge data generated, can establish “correct” model. Now, let’s assess performance model:estimated values intercept slope model approximately 0.08 0.5 respectively, reasonably close true values. coefficient determination, indicating proportion variation response variable explained model, 0.467.Now, let’s consider scenario include another variable, x2, completely irrelevant.Surprisingly, \\(\\mbox{R}^2\\) value increased 0.469 even though model includes unnecessary variable.outcome occurs \\(\\mbox{R}^2\\) value consider complexity model number parameters employed. fact, can increase number parameters indefinitely, ’s easy construct model \\(\\mbox{R}^2\\) value \\(1.00\\). achieved assigning \\(N\\) parameters \\(N\\) data points, effectively making parameters equivalent response variable . However, approach undesirable since model essentially explains response variable response variable (\\(y = y\\)), self-evident. Therefore, aim develop meaningful model, essential employ alternative measures model comparisons.","code":"\nset.seed(1) # for reproducibility\n\n# hypothetical sample size\nn <- 100\n\n# true intercept and slope\nb <- c(0.1, 0.5)\n\n# hypothetical explanatory variable\nx1 <- rnorm(n = n, mean = 0, sd = 1)\n\n# create a design matrix\nX <- model.matrix(~x1)\n\n# expected values of y is a function of x\n# %*% means matrix multiplication\n# y = X %*% b equals y = b[1] + b[2] * x\n# recall linear algebra\ny_hat <- X %*% b\n\n# add normal errors\ny <- rnorm(n = n, mean = y_hat, sd = 0.5)\n\n# plot\ndf0 <- tibble(y = y, x1 = x1)\n\ndf0 %>% \n  ggplot(aes(y = y,\n             x = x1)) + \n  geom_point()\n# correct model used to generate the data\nm1 <- lm(y ~ x1, data = df0)\nsummary(m1)## \n## Call:\n## lm(formula = y ~ x1, data = df0)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.93842 -0.30688 -0.06975  0.26970  1.17309 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.08115    0.04849   1.673   0.0974 .  \n## x1           0.49947    0.05386   9.273 4.58e-15 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.4814 on 98 degrees of freedom\n## Multiple R-squared:  0.4674, Adjusted R-squared:  0.4619 \n## F-statistic: 85.99 on 1 and 98 DF,  p-value: 4.583e-15\n# add a column x2 which is irrelevant for y\ndf0 <- df0 %>% \n  mutate(x2 = rnorm(n))\n\n# add x2 to the model\nm2 <- lm(y ~ x1 + x2, data = df0)\nsummary(m2)## \n## Call:\n## lm(formula = y ~ x1 + x2, data = df0)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.92274 -0.32325 -0.07538  0.26640  1.16629 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.08178    0.04870   1.679   0.0963 .  \n## x1           0.49996    0.05408   9.244 5.75e-15 ***\n## x2          -0.02294    0.04697  -0.488   0.6264    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.4833 on 97 degrees of freedom\n## Multiple R-squared:  0.4687, Adjusted R-squared:  0.4577 \n## F-statistic: 42.78 on 2 and 97 DF,  p-value: 4.793e-14"},{"path":"model-comparison.html","id":"comparison-metrics","chapter":"16 Model Comparison","heading":"16.2 Comparison Metrics","text":"various measures exist evaluate model performance, discuss three fundamental ones: Adjusted \\(\\mbox{R}^2\\), likelihood ratio test, Akaike’s Information Criterion (AIC). first two focus well model fits available dataset appropriately considering model complexity. hand, AIC assesses model’s capability make predictions unseen data (“--sample” prediction). universally optimal measure — depends specific research objective. goal assess model’s fit current data, first two measures similar ones can used. However, evaluating model’s predictive ability objective, AIC employed. crucial factor consider comparing models, essential ensure :models use dataset. Models fitted different data sets compared means.assume identical probability distribution among candidate models. example, normal model compared Poisson model.now describe rationale behind method.","code":""},{"path":"model-comparison.html","id":"adjusted-mboxr2","chapter":"16 Model Comparison","heading":"16.2.1 Adjusted \\(\\mbox{R}^2\\)","text":"Adjusted \\(\\mbox{R}^2\\) natural extension ordinary \\(\\mbox{R}^2\\). original formula \\(R^2\\) :\\[\nR^2 = 1 - \\frac{SS}{SS_0}\n\\], \\(SS\\) represents sum squares residuals (\\(\\sum \\varepsilon_i\\)), \\(SS_0\\) represents sum squares response variable (\\(\\sum (y_i - \\mu_y)\\), \\(\\mu_y\\) sample mean \\(y\\)). Adjusted \\(R^2\\) modifies formula account number parameters used:\\[\n\\begin{align*}\n\\text{Adj. }R^2 &= 1 - \\frac{SS/(N-k)}{SS_0/(N-1)} \\\\\n&= 1 - \\frac{\\sigma^2_{\\varepsilon}}{\\sigma^2_0}\n\\end{align*}\n\\]equation , \\(k\\) denotes number parameters utilized model. Hence, numerator indicates residual variance model, \\(\\sigma^2_{\\varepsilon}\\), denominator represents variance response variable, \\(y\\). can obtain values model objects m1 m2:Now can observe model m2 exhibits lower value adjusted \\(\\mbox{R}^2\\). discrepancy arises adjusted \\(\\mbox{R}^2\\) incorporates penalty term inclusion additional explanatory variables (\\(k\\)). order adjusted \\(\\mbox{R}^2\\) decrease, variable x2 must significantly decrease \\(\\sigma^2_{\\varepsilon}\\) beyond expected chance alone. words, x2 needs provide substantial improvement explaining response variable counterbalance penalty imposed inclusion extra variable.Note \\(\\mbox{R}^2\\) \\(\\mbox{Adj. R}^2\\) applicable normal models. case GLM framework (Chapter 14), utilize different measure called \\(\\mbox{R}^2_D\\), \\(D\\) represents “deviance.” Deviance (\\(D\\)) defined negative twice log-likelihood. estimate \\(\\mbox{R}^2_D\\) using deviance (\\(D\\)), comparable traditional \\(\\mbox{R}^2\\) tailored GLMs.\\[\n\\begin{align}\n\\mbox{R}^2_D &= 1 - \\frac{-2 \\ln L}{-2 \\ln L_0}\\\\\n&= 1 - \\frac{D}{D_0}\n\\end{align}\n\\]formula, \\(L\\) (\\(L_0\\)) represents likelihood fitted (null) model, \\(D\\) (\\(D_0\\)) represents deviance. several substitutes available \\(\\mbox{Adjusted R}^2\\). One method, known McFadden’s method 6, incorporates model complexity calculation using following formula:\\[\n\\mbox{Adj. R}^2_{D} = 1 - \\frac{\\ln L - k}{\\ln L_0}\n\\]","code":"\n# Adjusted R-square for m1 without x2\nsm1 <- summary(m1)\nprint(sm1$adj.r.squared)## [1] 0.4619164\n# Adjusted R-square for m2 with x2\nsm2 <- summary(m2)\nprint(sm2$adj.r.squared)## [1] 0.4577026"},{"path":"model-comparison.html","id":"likelihood-ratio-test","chapter":"16 Model Comparison","heading":"16.2.2 Likelihood Ratio Test","text":"Another possible approach likelihood ratio test, statistical test used compare fit two nested models. calculates log ratio likelihoods two models test statistic:\\[\n\\begin{align}\n\\mbox{LR} &= -2 \\ln \\frac{L_0}{L_1}\\\\\n&= -2 (\\ln L_0 - \\ln L_1)\n\\end{align}\n\\]likelihoods null alternative models denoted \\(L_0\\) \\(L_1\\) respectively. null hypothesis, test statistic \\(\\mbox{LR}\\) follows chi-square distribution, sample size approaches infinity. enables us determine whether inclusion new variable (variables) significantly enhanced model’s likelihood beyond expected chance.employ approach likelihood encounters problem similar \\(\\mbox{R}^2\\). log likelihood models can obtained using logLik() function:model m2 higher likelihood likelihood absolute measure model performance. However, likelihood ratio test overcomes limitation comparing observed increase likelihood expected chance. test statistic \\(\\mbox{LR}\\) follows chi-square distribution7, serves reference distribution. chi-square distribution single parameter - degrees freedom - , case, corresponds difference number model parameters used.model m1 utilized two parameters (intercept one slope), whereas model m2 employed three parameters (intercept two slopes). Consequently, test statistic assumed follow chi-square distribution one degree freedom. possible calculate p-value manually, anova() function can perform analysis:Pr(>Chi) value, represents p-value, significantly greater 0.05. indicates inclusion variable x2 improve model’s likelihood expected chance.likelihood ratio test can applied wide range models long capable estimating likelihood. Therefore, method highly versatile. However, one limitation competing models must nested, meaning null model subset alternative model. example provided, m1 includes explanatory variable x1, m2 incorporates x1 x2. However, m1 can viewed special case m2 slope x2 fixed zero. Thus, m1 can considered “nested” subset m2.","code":"\n# log likelihood: correct model\nlogLik(m1)## 'log Lik.' -67.77319 (df=3)\n# log likelihood: incorrect model\nlogLik(m2)## 'log Lik.' -67.6504 (df=4)\n# test = \"Chisq\" specifies a chi-square distribution\n# as a distribution of LR\nanova(m1, m2, test = \"Chisq\")## Analysis of Variance Table\n## \n## Model 1: y ~ x1\n## Model 2: y ~ x1 + x2\n##   Res.Df    RSS Df Sum of Sq Pr(>Chi)\n## 1     98 22.709                      \n## 2     97 22.653  1  0.055702   0.6253"},{"path":"model-comparison.html","id":"aic","chapter":"16 Model Comparison","heading":"16.2.3 AIC","text":"AIC takes different approach evaluating model performance compared measures. measures like \\(\\mbox{R}^2\\) primarily assess goodness fit available dataset used model fitting, AIC rooted information theoretic perspective. evaluates model’s ability predict unseen data population.mathematical details AIC beyond scope discussion, interested, reference Burnham Anderson 2002. However, crucial understand AIC differs fundamentally measures assesses model’s robustness new data added.Despite underlying mathematical complexity, AIC formula remarkably simple:\\[\n\\mbox{AIC} = 2k - 2\\ln L\n\\], \\(k\\) represents number model parameters, \\(L\\) likelihood. Lower AIC values indicate better predictability model. formula consists two terms: first term twice number parameters, second term model’s deviance. Thus, lower AIC value preferred. one might perceive variant \\(\\mbox{R}^2\\) similar measures, important note penalty term AIC derived Kullback–Leibler divergence.evident formula, AIC can estimated model valid likelihood, making method widely applicable, including GLMs. R, model’s AIC can computed using AIC() function.AIC value m2 higher, indicating second model lower capability predict unseen data. AIC several valuable features make useful criterion. instance, unlike likelihood tests, can used compare two models. Moreover, field biological sciences, prediction often holds significant interest, AIC particularly relevant. example, helps determine model performs best predicting unobserved sites within study region. AIC suggests selected “parsimonious” model lowest AIC provide best performance among competing models. Consequently, AIC finds widespread use biology, especially ecology.However, important exercise caution interpreting results. Specifically:model lowest AIC necessarily imply “true” model. limited explanatory variables collected, AIC can indicate model better among choices available. possible candidate models incorrect.model lowest AIC necessarily imply “true” model. limited explanatory variables collected, AIC can indicate model better among choices available. possible candidate models incorrect.AIC designed infer causal mechanisms. Variables causally related may enhance model’s ability predict unseen data. goal causal inference, different criterion, backdoor criterion, employed. example, AIC suitable analyzing controlled experiments aimed uncovering causal mechanisms biology.AIC designed infer causal mechanisms. Variables causally related may enhance model’s ability predict unseen data. goal causal inference, different criterion, backdoor criterion, employed. example, AIC suitable analyzing controlled experiments aimed uncovering causal mechanisms biology.often misuse metric, particularly overlooking second component (see arguments Arif MacNeil 2022). Therefore, crucial clearly define objective analysis ensure appropriate choice statistical methods.","code":"\n# AIC: correct model\nAIC(m1)## [1] 141.5464\n# AIC: incorrect model\nAIC(m2)## [1] 143.3008"},{"path":"model-comparison.html","id":"laboratory-9","chapter":"16 Model Comparison","heading":"16.3 Laboratory","text":"","code":""},{"path":"model-comparison.html","id":"format-penguin-data","chapter":"16 Model Comparison","heading":"16.3.1 Format Penguin Data","text":"R package palmerpenguins provides penguin data collected made available Dr. Kristen Gorman Palmer Station, Antarctica LTER, member Long Term Ecological Research Network. get started, install package onto computer:exercise, use penguins_raw dataset. However, dataset well-organized may prone errors current format. delving data analysis, let’s perform following tasks:Column names mix upper lower cases, contain white spaces. need format column names follows: (1) convert characters lowercase, (2) replace white spaces underscores _, (3) remove parentheses (e.g., (mm)). accomplish , can use clean_names() function janitor package. can check usage typing ?function_name R console.Column names mix upper lower cases, contain white spaces. need format column names follows: (1) convert characters lowercase, (2) replace white spaces underscores _, (3) remove parentheses (e.g., (mm)). accomplish , can use clean_names() function janitor package. can check usage typing ?function_name R console.following exercise, focus data Clutch Completion column, records data Yes . make suitable analysis, need convert information binary values: 1 Yes 0 . Use ifelse() function combination mutate().following exercise, focus data Clutch Completion column, records data Yes . make suitable analysis, need convert information binary values: 1 Yes 0 . Use ifelse() function combination mutate().Species column dataset contains species names Adelie Penguin (Pygoscelis adeliae), Chinstrap penguin (Pygoscelis antarctica), Gentoo penguin (Pygoscelis papua). need convert species names standardized ones, specifically: adelie, chinstrap, gentoo. Use dplyr::mutate() dplyr::case_when() functions task.Species column dataset contains species names Adelie Penguin (Pygoscelis adeliae), Chinstrap penguin (Pygoscelis antarctica), Gentoo penguin (Pygoscelis papua). need convert species names standardized ones, specifically: adelie, chinstrap, gentoo. Use dplyr::mutate() dplyr::case_when() functions task.Lastly, remove rows contain missing values (NA) columns Culmen Length (mm), Culmen Depth (mm), Flipper Length (mm), Body Mass (g), Sex. achieve , can utilize dplyr::drop_na() function.Lastly, remove rows contain missing values (NA) columns Culmen Length (mm), Culmen Depth (mm), Flipper Length (mm), Body Mass (g), Sex. achieve , can utilize dplyr::drop_na() function.","code":"\n# Perform only once\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)"},{"path":"model-comparison.html","id":"analyze-penguin-data","chapter":"16 Model Comparison","heading":"16.3.2 Analyze Penguin Data","text":"formatting data, next step perform following:Develop statistical model explains Clutch Completion using variables Species, Culmen Length (mm), Culmen Depth (mm), Flipper Length (mm), Body Mass (g), Sex. Use appropriate probability distribution model.Develop statistical model explains Clutch Completion using variables Species, Culmen Length (mm), Culmen Depth (mm), Flipper Length (mm), Body Mass (g), Sex. Use appropriate probability distribution model.Perform AIC-based model selection using MuMIn::dredge() function. process help us identify variables important predicting clutch completion status.Perform AIC-based model selection using MuMIn::dredge() function. process help us identify variables important predicting clutch completion status.","code":"\nlibrary(MuMIn)\noptions(na.action = \"na.fail\")\nm <- #model object\nm_set <- dredge(m, rank = \"AIC\")\nsubset(m_set, delta < 2)"},{"path":"generalized-linear-mixed-effect-model.html","id":"generalized-linear-mixed-effect-model","chapter":"17 Generalized Linear Mixed Effect Model","heading":"17 Generalized Linear Mixed Effect Model","text":"previous chapters, developed foundational skills linear modeling, including generalized linear models (GLMs). GLMs powerful flexible framework data arise single, relatively homogeneous set observational units. However, ecological biological data often structured multiple groups differ underlying characteristics (e.g., sites, individuals, years). hierarchical grouped structure present, generalized linear mixed-effects models (GLMMs) provide natural extension GLMs explicitly accounting shared population-level effects group-specific variation.Learning Objectives:Understand group structure biological dataUnderstand group structure biological dataUnderstand random intercept slopeUnderstand random intercept slopeUnderstand random effects preferred fixed effectsUnderstand random effects preferred fixed effects","code":"\npacman::p_load(tidyverse,\n               lme4,\n               glmmTMB)"},{"path":"generalized-linear-mixed-effect-model.html","id":"group-structure","chapter":"17 Generalized Linear Mixed Effect Model","heading":"17.1 Group Structure","text":"ecological studies, data often collected hierarchical grouped structure rather independent observations. example, measurements may repeated across sites within watersheds, individuals within populations, years within study locations.Let’s use Owls dataset R package glmmTMB grasp idea works. dataset originally published Roulin Bersier 2007, Animal Behavior.dataset contains observations begging behavior barn owl nestlings across different experimental conditions. data come study investigating sibling negotiation—vocal communication system nestlings use calls peacefully resolve individual receive next food item delivered parents.experiment manipulated hunger levels either food-depriving satiating nestlings, recorded negotiation calls parental arrivals. allowed researchers test whether negotiation intensity honestly signals hunger whether nestlings adjust calling based parent (male female) arrives nest. dataset includes repeated observations across 27 nests time, researchers counted negotiation calls individual chick - thus, data group (nested) structure (599 observations individual level, nested within 27 nests).seven main columns:Nest, factor identifying 27 individual nests.Nest, factor identifying 27 individual nests.FoodTreatment, indicating whether nestlings experimentally “Deprived” (kept hungry observation) “Satiated” (fed observation).FoodTreatment, indicating whether nestlings experimentally “Deprived” (kept hungry observation) “Satiated” (fed observation).SexParent, identifying whether provisioning parent “Male” “Female.”SexParent, identifying whether provisioning parent “Male” “Female.”ArrivalTime, time parent arrived nest.ArrivalTime, time parent arrived nest.SiblingNegotiation, number negotiation calls made siblings (primary response variable).SiblingNegotiation, number negotiation calls made siblings (primary response variable).BroodSize, number chicks nest.BroodSize, number chicks nest.NegPerChick, number negotiations per chick.NegPerChick, number negotiations per chick.Since original data contains mix upper- lower-case letters, first clean format:, visualize data. primary interest whether food_treatment changed number negotiations per chick (neg_per_chick = sibling_negotiation / brood_size):first glance, figure appears straightforward: number negotiations per chick decreases nestlings satiated. However, closer examination reveals important additional complexity data—grouped structure. example highlights first nine nests, illustrating substantial variation neg_per_chick among nests.examine effect food treatment accounting differences among nests, GLM can specified follows:Although model works, advisable. 27 nests, model estimates 26 separate parameters, using first nest reference category. results unnecessarily complex model. importantly, nest-specific coefficients biologically interpretable: comparing one arbitrary nest (e.g., first) another provide meaningful biological insight, data encode specific biological differences among nests.","code":"\ndata(Owls) # from `glmmTMB`\n\n# convert Owls to tibble\ndf_owl_raw <- as_tibble(Owls)\n(df_owl <- df_owl_raw %>%                 # Start with raw owl dataset and assign cleaned version\n  janitor::clean_names() %>%              # Standardize column names (lowercase, underscores, etc.)\n  mutate(across(.cols = where(is.factor), # Select all factor-type columns\n                .fns = str_to_lower))     # Convert factor levels to lowercase\n )                     ## # A tibble: 599 × 8\n##    nest    food_treatment sex_parent arrival_time sibling_negotiation brood_size\n##    <chr>   <chr>          <chr>             <dbl>               <int>      <int>\n##  1 autava… deprived       male               22.2                   4          5\n##  2 autava… satiated       male               22.4                   0          5\n##  3 autava… deprived       male               22.5                   2          5\n##  4 autava… deprived       male               22.6                   2          5\n##  5 autava… deprived       male               22.6                   2          5\n##  6 autava… deprived       male               22.6                   2          5\n##  7 autava… deprived       male               22.8                  18          5\n##  8 autava… satiated       female             22.9                   4          5\n##  9 autava… deprived       male               23.0                  18          5\n## 10 autava… satiated       female             23.1                   0          5\n## # ℹ 589 more rows\n## # ℹ 2 more variables: neg_per_chick <dbl>, log_brood_size <dbl>\ndf_owl %>%                          # Use cleaned owl dataset\n  ggplot(aes(x = food_treatment,    # Map food treatment to x-axis\n             y = neg_per_chick)) +  # Map number of negatives per chick to y-axis\n  geom_boxplot(outliers = FALSE) +  # Draw boxplots without plotting outliers\n  geom_jitter(alpha = 0.25) +       # Add jittered raw data points with transparency\n  theme_bw()                        # Apply a clean black-and-white theme\nv_g9 <- unique(df_owl$nest)[1:9]    # Extract first 9 nest IDs\n\ndf_owl %>%                          # Start with owl dataset\n  filter(nest %in% v_g9) %>%        # Keep only observations from selected nests\n  ggplot(aes(x = food_treatment,    \n             y = neg_per_chick)) +  \n  geom_jitter(alpha = 0.25) +       # Draw jitter plots for each treatment\n  facet_wrap(facets =~ nest,        # Create separate panels for each nest\n             ncol = 3,              # Arrange panels into 3 columns\n             nrow = 3) +            # Arrange panels into 3 rows\n  theme_bw()                        # Apply clean black-and-white theme\n# Response: sibling_negotiation — raw count of negotiation calls\n# Predictors: food_treatment and nest\n# Offset: log(brood_size)\n# Note: Including this offset models the log rate of negotiations per chick,\n#       i.e., log(sibling_negotiation / brood_size), which is equivalent to neg_per_chick\n\n# check mean and variance of the response\n# (mu_y <- mean(df_owl$sibling_negotiation)) # 6.72\n# (var_y <- var(df_owl$sibling_negotiation)) # 44.50\n\n# the response variable is count data\n# variance >> mean\n# choice of error distribution = negative-binomal\n(m_glm <- MASS::glm.nb(sibling_negotiation ~ food_treatment + nest + offset(log(brood_size)),\n                       data = df_owl))## \n## Call:  MASS::glm.nb(formula = sibling_negotiation ~ food_treatment + \n##     nest + offset(log(brood_size)), data = df_owl, init.theta = 0.911083123, \n##     link = log)\n## \n## Coefficients:\n##            (Intercept)  food_treatmentsatiated              nestbochet  \n##                0.20011                -0.78129                 0.20020  \n##        nestchampmartin             nestchesard            nestchevroux  \n##                0.14911                 1.02631                 0.44853  \n##    nestcorcellesfavres            nestetrabloz               nestforel  \n##                1.27423                -0.20826                -2.55492  \n##             nestfranex                nestgdlv          nestgletterens  \n##                0.31528                 0.24989                 0.74728  \n##            nesthenniez               nestjeuss         nestlesplanches  \n##                0.52213                -0.63986                 0.31026  \n##             nestlucens               nestlully             nestmarnand  \n##                0.53563                 0.83107                 0.56212  \n##             nestmontet              nestmurist              nestoleyes  \n##                0.66764                 1.01809                 0.71780  \n##            nestpayerne              nestrueyes               nestseiry  \n##                0.48422                 1.15510                 0.62465  \n##              nestsevaz             neststaubin                nesttrey  \n##                1.73558                 0.06776                 1.25416  \n##           nestyvonnand  \n##                0.62720  \n## \n## Degrees of Freedom: 598 Total (i.e. Null);  571 Residual\n## Null Deviance:       821.1 \n## Residual Deviance: 705.4     AIC: 3477"},{"path":"generalized-linear-mixed-effect-model.html","id":"random-intercept","chapter":"17 Generalized Linear Mixed Effect Model","heading":"17.2 Random Intercept","text":"","code":""},{"path":"generalized-linear-mixed-effect-model.html","id":"developing-a-model","chapter":"17 Generalized Linear Mixed Effect Model","heading":"17.2.1 Developing a model","text":"Variables included explicit predictors explanatory variables typically referred fixed effects. previous example, food_treatment nest treated fixed effects. Fixed effects usually primary focus analysis, represent factors whose effects wish estimate directly interpret biological context.However, noted , differences among nests biologically interpretable, making uninformative include fixed effects. time, ignoring variation among nests altogether excluding model bias estimated effect food_treatment.Random effects well suited situation. represent categorical grouping variables assign individual observations smaller number groups. example , nest ideal random effect groups individual chicks within nests, capturing among-nest variation without requiring explicit estimation nest-specific effects.Let’s develop model includes nest random effect. use glmmTMB() function glmmTMB package, specifying random effect syntax (1 | nest).Fixed effects still estimate (Intercept) effect food_treatment. key distinction standard GLM nest-specific coefficients GLMM; instead, model estimates standard deviation nest effect (estimated Std.Dev. 0.361), capturing among-nest variation.“trick” GLMMs model variation groups single random effect (one parameter), rather estimating separate parameter nest (26 parameters). substantially reduces number parameters, improving model efficiency increasing reliability estimated fixed effects.","code":"\nm_ri <- glmmTMB(\n  sibling_negotiation ~       # Response variable\n    food_treatment +          # Fixed effect of food treatment\n    (1 | nest) +              # Random intercept for each nest\n    offset(log(brood_size)),  # Offset to model rate per chick\n  data = df_owl,              \n  family = nbinom2()          # Negative binomial distribution\n)\n\nsummary(m_ri)                 # Display model results and parameter estimates##  Family: nbinom2  ( log )\n## Formula:          \n## sibling_negotiation ~ food_treatment + (1 | nest) + offset(log(brood_size))\n## Data: df_owl\n## \n##       AIC       BIC    logLik -2*log(L)  df.resid \n##    3492.4    3510.0   -1742.2    3484.4       595 \n## \n## Random effects:\n## \n## Conditional model:\n##  Groups Name        Variance Std.Dev.\n##  nest   (Intercept) 0.1271   0.3565  \n## Number of obs: 599, groups:  nest, 27\n## \n## Dispersion parameter for nbinom2 family (): 0.841 \n## \n## Conditional model:\n##                        Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)             0.69378    0.09902   7.006 2.44e-12 ***\n## food_treatmentsatiated -0.67769    0.11028  -6.145 7.98e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"generalized-linear-mixed-effect-model.html","id":"visualize-model-output","chapter":"17 Generalized Linear Mixed Effect Model","heading":"17.2.2 Visualize model output","text":"GLMM, random effects (e.g., nests, plots, subjects) modeled variation among groups, summarized standard deviation intercept rather separate fixed-effect parameters group. means model estimates much groups vary around overall mean, individual group intercepts directly. call specification random-intercept model allows group-specific intercepts.can obtain group-specific intercepts using coef() function8 – note estimates show log-scale negative-binomial distribution uses log link function:returned values (Intercept) represent group-specific intercepts, showing GLMMs account group-level variation allowing intercept vary across groups. coefficient food_treatmentsatiated constant across nests allow variation parameter among groups.Since plotting 27 nests tedious hard track visually, use random nine nests exercise:\nFigure 17.1: Visualization random‐intercept GLMM fitted owl nesting data subset nine nests. Points show observed counts negotiation calls per chick. Dashed horizontal lines indicate nest‐specific intercepts (back‐transformed response scale log link), reflecting baseline differences among nests. Solid line segments connect predicted values unfed satiated treatments, illustrating common (fixed) effect food treatment, assumed identical across nests random‐intercept model. solid blue horizontal line represents global intercept (population‐level mean). Panels faceted nest emphasize ‐nest variation baseline response.\n","code":"\nhead(coef(m_ri)$cond$nest)##                 (Intercept) food_treatmentsatiated\n## autavauxtv        0.3214851             -0.6776889\n## bochet            0.4792640             -0.6776889\n## champmartin       0.4384421             -0.6776889\n## chesard           1.0449504             -0.6776889\n## chevroux          0.6627038             -0.6776889\n## corcellesfavres   1.0781649             -0.6776889\n# ------------------------------------------------------------\n# 1. Global (population-level) intercept on the response scale\n# ------------------------------------------------------------\n# The model uses a log link (negative binomial),\n# so the intercept is on the log scale.\n# We exponentiate it to return to the original response scale.\ng0 <- exp(fixef(m_ri)$cond[1])\n\n\n# ------------------------------------------------------------\n# 2. Select a subset of nests to visualize\n# ------------------------------------------------------------\n# Plotting all 27 nests would be visually overwhelming,\n# so we randomly select 9 nests for this example.\nset.seed(123)  # ensures reproducibility\nv_g9_ran <- sample(unique(df_owl$nest),\n                   size = 9)\n\n\n# ------------------------------------------------------------\n# 3. Extract nest-specific coefficients (random intercept model)\n# ------------------------------------------------------------\n# coef(m_ri) returns the sum of fixed + random effects for each nest.\n# These values are still on the log scale.\ndf_g9 <- coef(m_ri)$cond$nest %>% \n  as_tibble(rownames = \"nest\") %>%      # convert to tibble and keep nest ID\n  filter(nest %in% v_g9_ran) %>%        # keep only the selected nests\n  rename(\n    log_g = `(Intercept)`,              # nest-specific intercept (log scale)\n    b = food_treatmentsatiated          # fixed slope for food treatment\n  ) %>% \n  mutate(\n    g = exp(log_g),                     # intercept on response scale\n    s = exp(log_g + b)                  # predicted value under satiated treatment\n  )\n\n\n# ------------------------------------------------------------\n# 4. Create the figure\n# ------------------------------------------------------------\ndf_owl %>% \n  filter(nest %in% v_g9_ran) %>%        # plot only the selected nests\n  ggplot(aes(x = food_treatment,\n             y = neg_per_chick)) +\n  # Raw data points (jittered to reduce overlap)\n  geom_jitter(width = 0.1,\n              alpha = 0.5) +\n  # Dashed horizontal lines:\n  # nest-specific intercepts (baseline differences among nests)\n  geom_hline(data = df_g9,\n             aes(yintercept = g),\n             alpha = 0.5,\n             linetype = \"dashed\") +\n  # Solid line segments:\n  # predicted change from unfed to satiated treatment\n  # using a common (fixed) slope across nests\n  geom_segment(data = df_g9,\n               aes(y = g,\n                   yend = s,\n                   x = 1,\n                   xend = 2),\n               linewidth = 0.5,\n               linetype = \"solid\") +\n  # Solid blue horizontal line:\n  # global (population-level) intercept\n  geom_hline(yintercept = g0,\n             alpha = 0.5,\n             linewidth = 1,\n             linetype = \"solid\",\n             color = \"steelblue\") +\n  # Facet by nest to emphasize group-level structure\n  facet_wrap(facets =~ nest,\n             nrow = 3,\n             ncol = 3) +\n  theme_bw()"},{"path":"generalized-linear-mixed-effect-model.html","id":"random-intercept-and-slope","chapter":"17 Generalized Linear Mixed Effect Model","heading":"17.3 Random Intercept and Slope","text":"","code":""},{"path":"generalized-linear-mixed-effect-model.html","id":"adding-a-random-slope-term","chapter":"17 Generalized Linear Mixed Effect Model","heading":"17.3.1 Adding a random slope term","text":"random-intercept model, written (1 | nest) example, allow nest baseline level (intercept) number negotiations per chick. words, nests can start different average values. However, saw coef(m_ri), effect food_treatment nests—slope change across nests.reflects assumption model: nests may differ overall level, effect food treatment assumed similar across nests. nest--nest differences food treatment works assumed small enough ignore.can relax assumption allowing effect food treatment vary among nests. done specifying random effect (1 + food_treatment | nest). random-intercept random-slope model, model still estimates average effect food treatment, also allows nest slope, capturing differences strongly nests respond treatment:Now, coef(m_ris)$cond$nest returns variable slopes among nests, reflecting assumption random slope:, visualization helps clarify model differs random-intercept model. random-intercept case, lines different nests parallel, effect food treatment across nests.following code highlights effect food treatment differs among nests allowing nest slope. result, lines longer parallel, making easy see nest-specific responses food treatment.\nFigure 17.2: Visualization random‐intercept random‐slope GLMM fitted owl nesting data subset nine nests. Points show observed counts negotiation calls per chick food treatment. Dotted line segments represent predictions random‐intercept model, effect food treatment assumed constant across nests. Solid line segments show predictions random‐slope model, allowing effect food treatment vary among nests; color indicates nest‐specific slope estimate. solid blue horizontal line denotes global (population‐level) intercept. Panels faceted nest emphasize differences baseline response treatment effects.\n","code":"\nm_ris <- glmmTMB(\n  sibling_negotiation ~             # Response variable\n    food_treatment +                # Fixed effect of food treatment\n    (1 + food_treatment | nest) +   # Random intercepts and slopes for each nest\n    offset(log(brood_size)),        # Offset to model rate per chick\n  data = df_owl,                    \n  family = nbinom2()                # Negative binomial distribution\n)\n\nsummary(m_ris)                      # Display model estimates and diagnostics##  Family: nbinom2  ( log )\n## Formula:          sibling_negotiation ~ food_treatment + (1 + food_treatment |  \n##     nest) + offset(log(brood_size))\n## Data: df_owl\n## \n##       AIC       BIC    logLik -2*log(L)  df.resid \n##    3447.0    3473.4   -1717.5    3435.0       593 \n## \n## Random effects:\n## \n## Conditional model:\n##  Groups Name                   Variance Std.Dev. Corr \n##  nest   (Intercept)            0.006385 0.0799        \n##         food_treatmentsatiated 1.215901 1.1027   1.00 \n## Number of obs: 599, groups:  nest, 27\n## \n## Dispersion parameter for nbinom2 family (): 0.978 \n## \n## Conditional model:\n##                        Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)             0.65330    0.06225  10.496  < 2e-16 ***\n## food_treatmentsatiated -0.97408    0.24512  -3.974 7.07e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nhead(coef(m_ris)$cond$nest)##                 (Intercept) food_treatmentsatiated\n## autavauxtv        0.6458989           -1.076110554\n## bochet            0.5818570           -1.960248712\n## champmartin       0.5051865           -3.018403203\n## chesard           0.7381005            0.196113330\n## chevroux          0.5486965           -2.417926598\n## corcellesfavres   0.7235323           -0.004790489\n# ------------------------------------------------------------\n# 1. Extract nest-specific coefficients from the random-slope model\n# ------------------------------------------------------------\n# coef(m_ris) returns the sum of fixed + random effects for each nest.\n# In this model, BOTH the intercept and the slope of food treatment\n# are allowed to vary among nests.\n# All coefficients are still on the log scale.\ndf_s9 <- coef(m_ris)$cond$nest %>% \n  as_tibble(rownames = \"nest\") %>%       # convert to tibble and keep nest ID\n  rename(\n    log_g = `(Intercept)`,               # nest-specific intercept (log scale)\n    b = \"food_treatmentsatiated\"          # nest-specific slope (log scale)\n  ) %>% \n  mutate(\n    g = exp(log_g),                      # intercept on response scale\n    s = exp(log_g + b)                   # predicted value under satiated treatment\n  ) %>% \n  filter(nest %in% v_g9_ran)              # keep only the selected nine nests\n\n\n# ------------------------------------------------------------\n# 2. Create the figure\n# ------------------------------------------------------------\ndf_owl %>% \n  filter(nest %in% v_g9_ran) %>%          # plot only the selected nests\n  ggplot(aes(x = food_treatment,\n             y = neg_per_chick)) +\n  # Raw data points (jittered to reduce overlap)\n  geom_jitter(width = 0.1,\n              alpha = 0.5) +\n  # DOTTED line segments:\n  # predictions from the random-intercept model\n  # (intercepts vary, but slopes are assumed identical across nests)\n  geom_segment(data = df_g9,\n               aes(y = g,\n                   yend = s,\n                   x = 1,\n                   xend = 2),\n               linewidth = 0.5,\n               linetype = \"dotted\") +\n  # SOLID line segments:\n  # predictions from the random-intercept + random-slope model\n  # both intercepts and slopes vary among nests\n  # color represents the nest-specific slope estimate\n  geom_segment(data = df_s9,\n               aes(y = g,\n                   yend = s,\n                   x = 1,\n                   xend = 2,\n                   color = b),\n               linewidth = 1,\n               linetype = \"solid\") +\n  # Solid blue horizontal line:\n  # global (population-level) intercept\n  geom_hline(yintercept = g0,\n             alpha = 0.5,\n             linewidth = 1,\n             linetype = \"solid\",\n             color = \"steelblue\") +\n  # Facet by nest to highlight group-level heterogeneity\n  facet_wrap(facets =~ nest,\n             nrow = 3,\n             ncol = 3) +\n  theme_bw() +\n  # Color scale for nest-specific slopes\n  scale_color_viridis_c()"},{"path":"generalized-linear-mixed-effect-model.html","id":"fixed-vs-random","chapter":"17 Generalized Linear Mixed Effect Model","heading":"17.4 Fixed vs Random","text":"statistical modeling, particularly ecology, factors dataset can treated fixed effects random effects. Choosing appropriate type affects interpret model coefficients account variation data. Fixed effects used specifically interested estimating comparing levels factor, random effects used levels represent sample larger population primarily interested quantifying variability among .","code":""},{"path":"generalized-linear-mixed-effect-model.html","id":"model-specification","chapter":"17 Generalized Linear Mixed Effect Model","heading":"17.5 Model specification","text":"","code":""},{"path":"generalized-linear-mixed-effect-model.html","id":"random-intercept-1","chapter":"17 Generalized Linear Mixed Effect Model","heading":"17.5.1 Random intercept","text":"\\(y_i\\) denoting number negotiation calls (sibling_negotiation), example formulated follows:\\[\ny_i \\sim \\mbox{NB}(\\mu_i, \\theta)\\\\\n\\ln \\mu_i = \\underbrace{(\\alpha + \\gamma_{j()})}_{\\text{intercept + random effect term}} + \\underbrace{\\beta}_{\\text{slope}} x_i + \\ln(\\mbox{offset term})\\\\\n\\gamma_j \\sim \\mbox{Normal}(0, \\sigma_{\\gamma}^2)\n\\], \\(\\) indexes individuals, \\(j()\\) denotes nest individual \\(\\) belongs. model estimates, global intercept \\(\\alpha\\) = 0.69, effect food treatment \\(\\beta\\) = -0.68, dispersion parameter \\(\\theta\\) = 0.84, SD random effect \\(\\sigma_{\\gamma}\\) = 0.13. Although example uses negative binomial model, overall structure similar probability distributions, provided appropriate link function chosen.","code":""},{"path":"generalized-linear-mixed-effect-model.html","id":"random-intercept-and-slope-1","chapter":"17 Generalized Linear Mixed Effect Model","heading":"17.5.2 Random intercept and slope","text":"random slope term, model specification becomes:\\[ y_i \\sim \\mbox{NB}(\\mu_i, \\theta)\\\\ \\ln \\mu_i = \\underbrace{(\\alpha + \\gamma_{j()})}_{\\text{intercept + random effect term}} + \\underbrace{(\\beta + \\delta_{j()})}_{\\text{slope + random effect term}} x_i + \\ln(\\mbox{offset term})\\\\\\]random slope term included, random intercept slope terms assumed follow multi-variate normal distribution, specified :\\[\n\\{\\gamma_{j}, \\delta_j\\} \\sim \\mbox{MVN}(0, \\Sigma)\n\\]\\(\\Sigma\\) denotes variance-covariance matrix random effect terms.","code":""},{"path":"generalized-additive-model.html","id":"generalized-additive-model","chapter":"18 Generalized Additive Model","heading":"18 Generalized Additive Model","text":"generalized linear models (GLMs) generalized linear mixed models (GLMMs), key structural assumption linearity scale linear predictor. , applying link function, expected value response assumed linear combination predictors associated parameters.biological ecological data, however, relationships often nonlinear smooth, reflecting processes seasonality, thresholds, saturation, diminishing returns. patterns forced strictly linear framework, GLMs GLMMs may yield biased parameter estimates, poor predictive performance, misleading inference.address limitations, chapter introduces generalized additive models (GAMs), extend GLMs allowing predictors enter model smooth functions rather assuming linear effects.Learning Objectives:Understand linearity assumptions underlying GLMs GLMMsUnderstand linearity assumptions underlying GLMs GLMMsUnderstand smoothing functions relax assumptionsUnderstand smoothing functions relax assumptionsEvaluate advantages limitations additive modelsEvaluate advantages limitations additive models","code":"\npacman::p_load(tidyverse,\n               ggeffects,\n               mgcv)"},{"path":"generalized-additive-model.html","id":"seasonal-data","chapter":"18 Generalized Additive Model","heading":"18.1 Seasonal Data","text":"","code":""},{"path":"generalized-additive-model.html","id":"water-temperature-example","chapter":"18 Generalized Additive Model","heading":"18.1.1 Water temperature example","text":"illustrate application nonlinear models, use water temperature data collected artificial wetlands campus University North Carolina, Greensboro. data publicly available form part Ibrahim et al. 2025.dataset loaded provided link contains water temperature observations wetlands includes three main columns. date_time column records timestamp measurement, typically month/day/year format. site column indicates wetland type location (e.g., “woody” vs. “open”), serving factor distinguish different wetland types. Finally, temp column provides numeric water temperature degrees Celsius, can used response variable statistical models.always good practice begin checking class column, misunderstanding data types can lead unnecessary confusion data analysis.potential issue dataset date_time column stored “character.” Although data_time contains temporal information, character data type allow R interpret date. following code, convert column Date object, explicitly encodes temporal information enables time-based operations.example , restricted dataset period 2022-03-01 2022-10-31 woody site contains missing data outside range. Next, aggregate data daily means original measurements recorded 15-minute intervals, fine-grained analysis.Since water temperature seasonal, rises spring summer falls summer fall. data collected two wetlands, colors distinguish different types wetlands.\nFigure 18.1: Daily water temperature measured two types artificial wetlands (“woody” “open”) UNCG campus. point represents daily average temperature, semi-transparent points reduce overplotting. Colors indicate wetland type, x-axis shows date monitoring period.\nhappens model seasonal dataset using linear model? can explore using glm() (lm()) function, visualizing predicted values highlight limitations treating inherently seasonal data linear.model suggests water temperature increases Julian date significant difference wetland types. However, conclusion may misleading strong seasonal (nonlinear) pattern data. Visualizing model-predicted values can help assess result.ggpredict() function ggeffects package provides convenient way generate predicted values model, internally using base predict() function. Factor variables (e.g., site) returned group, continuous variables returned x; can renamed match original column names clarity.\nFigure 18.2: Daily water temperature two types artificial wetlands (“woody” “open”) shown points, semi-transparent markers reduce overplotting. overlaid lines represent predicted values linear model (glm) using Julian day site predictors. plot highlights clear mismatch observed seasonal fluctuations linear predictions, illustrating limitations modeling inherently seasonal data linear approach.\n","code":"\nlink <- \"https://raw.githubusercontent.com/aterui/biostats/master/data_raw/data_water_temp.csv\"\n\n(df_wt_raw <- read_csv(link))## # A tibble: 77,596 × 4\n##       id date_time       temp site \n##    <dbl> <chr>          <dbl> <chr>\n##  1     1 8/6/2021 18:00  23.1 woody\n##  2     2 8/6/2021 18:15  23.1 woody\n##  3     3 8/6/2021 18:30  23.1 woody\n##  4     4 8/6/2021 18:45  23.2 woody\n##  5     5 8/6/2021 19:00  23.2 woody\n##  6     6 8/6/2021 19:15  23.2 woody\n##  7     7 8/6/2021 19:30  23.2 woody\n##  8     8 8/6/2021 19:45  23.2 woody\n##  9     9 8/6/2021 20:00  23.2 woody\n## 10    10 8/6/2021 20:15  23.3 woody\n## # ℹ 77,586 more rows\nsapply(df_wt_raw, class)##          id   date_time        temp        site \n##   \"numeric\" \"character\"   \"numeric\" \"character\"\n# Start from the raw water-temperature dataset\n(df_wt <- df_wt_raw %>% \n  mutate(\n    # Convert the character datetime column into a Date object.\n    # The format argument specifies month/day/year.\n    date = as.Date(date_time,\n                   format = \"%m/%d/%Y\"),\n    # Extract the calendar year from the Date object\n    year = year(date),\n    # Extract the month (1–12) from the Date object\n    month = month(date)\n  ) %>% \n  # Subset the data to observations from the year 2022 only\n  # March through October\n  filter(year == 2022,\n         between(month, 3, 10)))## # A tibble: 35,287 × 7\n##       id date_time      temp site  date        year month\n##    <dbl> <chr>         <dbl> <chr> <date>     <dbl> <dbl>\n##  1 19801 3/1/2022 0:00  7.67 woody 2022-03-01  2022     3\n##  2 19802 3/1/2022 0:15  7.62 woody 2022-03-01  2022     3\n##  3 19803 3/1/2022 0:30  7.57 woody 2022-03-01  2022     3\n##  4 19804 3/1/2022 0:45  7.52 woody 2022-03-01  2022     3\n##  5 19805 3/1/2022 1:00  7.44 woody 2022-03-01  2022     3\n##  6 19806 3/1/2022 1:15  7.39 woody 2022-03-01  2022     3\n##  7 19807 3/1/2022 1:30  7.34 woody 2022-03-01  2022     3\n##  8 19808 3/1/2022 1:45  7.29 woody 2022-03-01  2022     3\n##  9 19809 3/1/2022 2:00  7.27 woody 2022-03-01  2022     3\n## 10 19810 3/1/2022 2:15  7.22 woody 2022-03-01  2022     3\n## # ℹ 35,277 more rows\n# Aggregate raw water temperature data to daily averages\ndf_wt_daily <- df_wt %>% \n  # Group the data by date and site\n  # This ensures that averaging is done separately for each site on each day\n  group_by(date, site) %>% \n  # Compute summary statistics for each group\n  summarize(\n    # Take the mean of the 'temp' column within each group\n    # na.rm = TRUE ignores missing values (NA) in the calculation\n    # round(..., 3) rounds the resulting mean to 3 decimal places\n    temp = mean(temp, na.rm = TRUE) %>% round(3)\n  )\n# Visualize daily water temperature for each wetland site\ndf_wt_daily %>% \n  ggplot(aes(\n    x = date,      # Date on the x-axis\n    y = temp,      # Daily averaged temperature on the y-axis\n    color = site   # Color points by wetland type (woody vs open)\n  )) +\n  # Add points for each observation\n  # alpha = 0.25 makes points semi-transparent to reduce overplotting\n  geom_point(alpha = 0.25) +\n  # Apply a clean black-and-white theme for readability\n  theme_bw() +\n  # Customize axis labels and legend title\n  labs(\n    x = \"Date\",                # x-axis label\n    y = \"Water Temperature\",   # y-axis label\n    color = \"Wetland Type\"     # legend title for the color mapping\n  )\n# Add new variables and ensure proper data types\ndf_wt_daily <- df_wt_daily %>% \n  mutate(\n    # Convert the date column to Julian day (day of year)\n    # Useful for modeling seasonal trends\n    j_date = yday(date),\n    # Ensure 'site' is treated as a factor (categorical variable)\n    site = factor(site)\n  )\n\n# Fit a Generalized Linear Model (GLM) with Gaussian family\nm_glm <- glm(\n  temp ~ j_date + site,  # Model daily temperature as a function of Julian day and wetland site\n  data = df_wt_daily,    # Dataset to use\n  family = \"gaussian\"    # Specify Gaussian distribution (standard linear regression)\n)\n\nsummary(m_glm)## \n## Call:\n## glm(formula = temp ~ j_date + site, family = \"gaussian\", data = df_wt_daily)\n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 15.165281   0.688730  22.019  < 2e-16 ***\n## j_date       0.021495   0.003316   6.481 2.23e-10 ***\n## sitewoody   -0.640290   0.469106  -1.365    0.173    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for gaussian family taken to be 26.95744)\n## \n##     Null deviance: 14311  on 489  degrees of freedom\n## Residual deviance: 13128  on 487  degrees of freedom\n## AIC: 3009.7\n## \n## Number of Fisher Scoring iterations: 2\n# Generate model predictions across all Julian days and wetland sites\ndf_pred <- ggpredict(m_glm,\n                     terms = c(\n                       \"j_date [all]\",  # Use all observed values of Julian day\n                       \"site [all]\"     # Generate predictions for all levels of the factor 'site'\n                     )\n) %>% \n  # Rename the default columns to match the original dataset\n  rename(site = group,  # 'group' from ggpredict() corresponds to the factor variable 'site'\n         j_date = x     # 'x' from ggpredict() corresponds to the predictor 'j_date'\n  )\n\n# Plot daily water temperature and overlay model predictions\ndf_wt_daily %>% \n  ggplot(aes(\n    x = j_date,   # Julian day on x-axis\n    y = temp,     # Observed daily temperature on y-axis\n    color = site  # Color points by wetland type (factor)\n  )) +\n  geom_point(alpha = 0.25) +\n  # Overlay predicted values from the model\n  # df_pred contains predictions from ggpredict()\n  # aes(y = predicted) maps the model's predicted temperature to y\n  geom_line(data = df_pred,\n            aes(y = predicted)) +\n  theme_bw() +\n  labs(x = \"Julian Date\",         # x-axis label\n       y = \"Water Temperature\",   # y-axis label\n       color = \"Wetland Type\"     # Legend title for site color\n       )"},{"path":"generalized-additive-model.html","id":"smooth-function","chapter":"18 Generalized Additive Model","heading":"18.1.2 Smooth function","text":"Linear models assume straight-line relationship predictors response. seasonal data, like water temperature year, assumption often violated: temperatures rise spring, peak summer, fall autumn—nonlinear pattern simple linear term capture.Generalized Additive Model (GAM) extends linear models replacing (one ) linear terms smooth functions, allowing relationship bend flex according data:\\[\ny_i = \\alpha + f(x) + \\varepsilon_i\n\\], \\(f(\\cdot)\\) smooth function estimated data, often using splines. practice, means GAMs can model seasonal nonlinear trends without needing specify exact form curve advance.R, can use mgcv package implement approach. apply smooth function s() j_date capture clear seasonal pattern, keeping site (wetland type) linear term, since expect effect wetland type vary nonlinearly.interpretation parametric coefficients largely GLMs GLMMs. case, effect site (wetland type) becomes statistically significant. occurs nonlinear seasonal pattern modeled first smooth term, allowing site effect assessed without confounded seasonal structure data.smooth term s(j_date) GAM captures nonlinear seasonal variation water temperature across year. effective degrees freedom (edf =8.62) indicate relationship flexible rather straight line, allowing model follow rise temperature spring, peak summer, decline fall. edf becomes approximately 1 smooth term represents essentially linear effect - occurs, adding predictor smooth term meaningful.extremely low p-value (<2e-16) shows seasonal pattern highly significant, meaning smooth term explains large portion variation temperature linear term alone capture.GCV (generalized cross-validation) score (3.35) worth mentioning used determine wiggly curve . lower GCV indicates better balance model fit smoothness, penalizing excessive curvature avoid overfitting.ggpredict() function allows us generate visualize predicted values GAM.\nFigure 18.3: Daily water temperature two types artificial wetlands (“woody” “open”) shown points, semi-transparent markers reduce overplotting. overlaid lines represent predicted values linear model (glm) using Julian day site predictors. plot highlights clear mismatch observed seasonal fluctuations linear predictions, illustrating limitations modeling inherently seasonal data linear approach.\nKey insights practical use:Smooth terms (s(x)) allow flexible, data-driven curves.model automatically balances fit vs. smoothness avoid overfitting.Categorical continuous predictors can included linear predictors alongside smooth terms.GAM framework lets move linear approximations flexible, realistic modeling seasonal patterns, can illustrated simple water temperature example diving mathematical details.","code":"\nm_gam <- gam(temp ~ site + s(j_date),\n             data = df_wt_daily,\n             family = \"gaussian\")\n\nsummary(m_gam)## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## temp ~ site + s(j_date)\n## \n## Parametric coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  19.0774     0.1157 164.878  < 2e-16 ***\n## sitewoody    -0.6403     0.1636  -3.913 0.000104 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##             edf Ref.df     F p-value    \n## s(j_date) 8.617  8.958 431.1  <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.888   Deviance explained =   89%\n## GCV = 3.3527  Scale est. = 3.28      n = 490\ndf_pred_gam <- ggpredict(m_gam,\n                         terms = c(\n                           \"j_date [all]\", \n                           \"site [all]\")\n                         ) %>% \n  rename(site = group,\n         j_date = x)\n\ndf_wt_daily %>% \n  ggplot(aes(\n    x = j_date,\n    y = temp, \n    color = site\n  )) +\n  geom_point(alpha = 0.25) +\n  # Overlay predicted values from the GAM\n  geom_line(data = df_pred_gam,\n            aes(y = predicted)) +\n  theme_bw() +\n  labs(x = \"Julian Date\",         # x-axis label\n       y = \"Water Temperature\",   # y-axis label\n       color = \"Wetland Type\"     # Legend title for site color\n       )"},{"path":"generalized-additive-model.html","id":"estimating-smoothness","chapter":"18 Generalized Additive Model","heading":"18.2 Estimating Smoothness","text":"","code":""},{"path":"generalized-additive-model.html","id":"choice-of-smooth-function","chapter":"18 Generalized Additive Model","heading":"18.2.1 Choice of smooth function","text":"default, mgcv::s() employs thin plate regression splines smoothing function. one broadly applicable approaches, options may appropriate depending structure data. following table summarizes possible alternatives offered mgcv package, along typical applications.","code":""},{"path":"generalized-additive-model.html","id":"penalty-strength","chapter":"18 Generalized Additive Model","heading":"18.2.2 Penalty strength","text":"GAM, smooth terms flexible functions whose degree curvature (wiggliness) must estimated data. mgcv package, done automatically selecting smoothing parameter balances goodness fit model complexity.default, mgcv uses generalized cross-validation (GCV) determine smoothing parameter. words, GCV decides strength penalty applied smooth term: stronger penalty produces smoother, less wiggly curve, weaker penalty allows flexibility follow data fluctuations. Conceptually, GCV asks: well model predict new data slightly penalize excessive curvature? smoother fits data closely (overfitting) penalized, smoother rigid (underfitting) fails capture important structure. chosen smoothness minimizes GCV score, yielding curve flexible enough capture real patterns smooth enough generalize beyond observed data.However, options penalty selection method, can specified via method argument mgcv::gam():","code":""},{"path":"unconstrained-ordination.html","id":"unconstrained-ordination","chapter":"19 Unconstrained Ordination","heading":"19 Unconstrained Ordination","text":"far, modeling exercises focused univariate analyses, model single response variable using one predictors. practice, biological data often multivariate, multiple variables describing system. useful reduce many correlated variables smaller number axes capture main patterns variation. chapter, learn methods allow us summarize multivariate data fewer dimensions.Learning Objectives:Understand apply unconstrained ordination methods PCA NMDS summarize visualize multivariate data.Understand apply unconstrained ordination methods PCA NMDS summarize visualize multivariate data.Interpret ordination outputs, including principal component axes, NMDS coordinates, stress values.Interpret ordination outputs, including principal component axes, NMDS coordinates, stress values.Compare select appropriate ordination methods (PCA, NMDS, PCoA, CA, MCA) based data type, distribution, research questions.Compare select appropriate ordination methods (PCA, NMDS, PCoA, CA, MCA) based data type, distribution, research questions.","code":"\npacman::p_load(tidyverse,\n               GGally,\n               vegan)"},{"path":"unconstrained-ordination.html","id":"methods-for-unconstrained-ordination","chapter":"19 Unconstrained Ordination","heading":"19.1 Methods for Unconstrained Ordination","text":"explore unconstrained ordination methods summarizing multivariate data. methods differ assumptions, flexibility, interpretable axes , choice method depends goals analysis. , focus two extreme cases—PCA NMDS—demonstrate application examples. conclude briefly introducing ordination options, highlighting potential uses future analyses.","code":""},{"path":"unconstrained-ordination.html","id":"pca","chapter":"19 Unconstrained Ordination","heading":"19.1.1 PCA","text":"","code":""},{"path":"unconstrained-ordination.html","id":"flower-shape","chapter":"19 Unconstrained Ordination","heading":"Flower “shape”","text":"use iris dataset exercise. dataset contains multiple measurements flower individual, measurements (length width dimensions sepal petal) highly correlated, collectively capturing overall “shape” flower.begin, clean data simplify variable names ensure consistency, since mixed cases formatting can cumbersome.Next, explore relationships among numeric measurements iris dataset. Since variables likely correlated, use scatterplot matrix visualize pairwise relationships see three species differ. ggpairs() function GGally package makes easy.\nFigure 19.1: Pairwise scatterplots four iris traits (sepal length, sepal width, petal length, petal width) colored species. plots show relationships among traits, highlight species-specific trait distributions, reveal correlations patterns data.\nexceptions, variable combinations correlated , suggesting may contain overlapping information can summarized along fewer axes.variables approximately normally distributed show substantial correlations, Principal Component Analysis (PCA) provides effective way summarize multiple variables smaller set axes. PCA identifies orthogonal axes based variance–covariance structure variables, assumption multivariate normality.Without going much detail, PCA can performed R using prcomp() function. simplicity, example focuses two variables (petal_length petal_width). important point using prcomp() almost always set scale = TRUE explicitly, since PCA intended summarize variables measured different scales different variances.standard deviations principal components (PCs) indicate much data varies along axis. case, PC1 standard deviation 1.401, much higher PC2’s 0.193, meaning variation petal length width occurs along PC1. Squaring values gives eigenvalues, quantify variance captured component.proportion variance shows PC1 explains approximately 98% total variance, PC2 accounts 2%. cumulative proportion confirms together capture 100% variation. means PC1 effectively summarizes almost information dataset, PC2 adds little new insight.Visualization helps illustrate PCA summarizes variables. shown figure, principal component axes identified prcomp() represent orthogonal dimensions relationships, first axis capturing largest variance (eigenvalue) therefore information original variables.\nFigure 19.2: Scatter plot scaled petal length width three Iris species (subtract means, divide SDs). Points represent individual flowers colored species. Red lines represent contribution principal component variation petal length width. Black arrows indicate directions principal components visual reference. Text labels arrow tips denote corresponding principal component.\n","code":"\n# Convert the built-in iris dataset into a tibble and clean column names\ndf_iris <- iris %>% \n  as_tibble() %>%               # Convert from data.frame to tibble for tidyverse-friendly operations\n  janitor::clean_names()        # Standardize column names: lowercase, replace spaces/special characters\ndf_iris %>%\n  ggpairs(\n    progress = FALSE, # Disable progress bar during plotting\n    columns = c(\"sepal_length\", # Select the four numeric columns to include\n                \"sepal_width\",\n                \"petal_length\",\n                \"petal_width\"),\n    aes(\n      color = species, # Color points by species\n      alpha = 0.5 # Make points semi-transparent for better visibility\n    )\n  ) +\n  theme_bw() # Use a clean theme for better readability\n# Extract only the petal measurements from the iris dataset\ndf_petal <- df_iris %>% \n  # Select columns whose names start with \"petal_\" (petal length and petal width)\n  select(starts_with(\"petal_\")) \n\n# Perform Principal Component Analysis (PCA) on the selected petal data\nobj_pca <- prcomp(\n  x = df_petal,    # Input data for PCA\n  center = TRUE,   # Subtract the mean of each column so variables are centered at 0\n  scale = TRUE     # Divide by the standard deviation of each column so variables have unit variance\n)\n\n# obj_pca now contains:\n# - obj_pca$x        : PCA scores (coordinates of each sample in PC space)\n# - obj_pca$rotation : Eigenvectors (directions of principal components)\n# - obj_pca$sdev     : Standard deviations of the principal components (square = variance explained)\n\nprint(obj_pca)## Standard deviations (1, .., p=2):\n## [1] 1.4010230 0.1927033\n## \n## Rotation (n x k) = (2 x 2):\n##                    PC1        PC2\n## petal_length 0.7071068 -0.7071068\n## petal_width  0.7071068  0.7071068"},{"path":"unconstrained-ordination.html","id":"extracting-pc-values","chapter":"19 Unconstrained Ordination","heading":"Extracting PC values","text":"PCA performed, may wonder original data points positioned PCA space. prcomp() function returns principal component (PC) scores data point, allowing relate values back original data. coordinates stored $x element prcomp object, first second columns correspond PC1 PC2 (additional columns correspond many axes number variables included PCA).Since prcomp() preserves order original data, can safely combine $x scores original dataset using bind_cols(). Just careful alter row order, misalign PC scores corresponding original observations.example, focus PC1, since PC2 contributes little information petal shape. Converting PC1 ordinal code allows us compare values differ among species.","code":"\n# Combine the original iris dataset with PCA scores\ndf_pca <- bind_cols(\n  df_iris,              # Original dataset (all columns: species, sepal, and petal measurements)\n  as_tibble(obj_pca$x)  # PCA scores for each observation (PC1, PC2, ...)\n)\ndf_pca %>% \n  ggplot(aes(\n    x = species,   # x-axis: categorical variable (species)\n    y = PC1,       # y-axis: PCA scores for the first principal component\n    color = species # Color the boxplots by species\n  )) +\n  geom_boxplot() +\n  labs(x = \"Species\",\n       y = \"Petal shape (PC1)\")"},{"path":"unconstrained-ordination.html","id":"nmds","chapter":"19 Unconstrained Ordination","heading":"19.1.2 NMDS","text":"","code":""},{"path":"unconstrained-ordination.html","id":"dune-community-data","chapter":"19 Unconstrained Ordination","heading":"Dune community data","text":"explored PCA using iris dataset, saw linear ordination can summarize variation continuous, roughly normally distributed traits. However, many biological datasets meet assumptions. example, species abundances often zero-inflated, skewed, otherwise non-normal.Non-metric ordination methods, Non-metric Multidimensional Scaling (NMDS), overcome limitations linear ordination operating directly matrix pairwise dissimilarities sites (observational units). example, use dune dataset, Vegetation Environment Dutch Dune Meadows, illustrate strengths applications NMDS.dune dataset contains species abundance matrix 20 rows 30 columns. row represents unique sampling site dune meadows, column corresponds plant species. values indicate abundance species site, typically measured counts individuals percent cover. multivariate structure makes dataset well suited ordination methods, site can characterized overall species composition.Visualization reveals pronounced non-normality non-linearity data. example, plot first three species. distributions strongly right-skewed, clearly deviating normal distribution.\nFigure 19.3: Pairwise scatterplots first three plant species Dutch dune meadows dataset (dune). plots visualize relationships among species abundances across sites, highlighting correlations, patterns, potential non-normality data.\nPCA, representing data using reduced number axes, method identifies orthogonal dimensions capture main “cloud” variation data. However, underlying relationships non-linear, linear approach may fail capture important patterns.contrast, NMDS aims find set axes best preserve ranked dissimilarities sites. first step NMDS convert community matrix distance matrix. produces N × N matrix (N number sites), cell representing dissimilarity species composition pair sites. vegdist() function vegan package provides straightforward way compute distance (dissimilarity) matrix community multivariate dataset., used “Bray-Curtis” distance (method = \"bray), widely used measure dissimilarity ecology quantifies differences species composition two sites based abundances. ranges 0 (identical composition) 1 (completely different composition). Importantly, Bray-Curtis ignores joint absences—species absent sites contribute dissimilarity—making well-suited ecological community data.distance dissimilarity measures also available, depending type data ecological question. Examples include Jaccard, Euclidean, Manhattan, many others. full list metrics supported vegdist() vegan package can found .Next, use metaMDS() function vegan package perform NMDS distance matrix.metaMDS() function positions sites user-specified k-dimensional space (k = 2) ranked distances (observed distances!) reduced space closely match original dissimilarities. achieves iterative optimization algorithm.resulting NMDS object contains several important pieces information. includes coordinates sites NMDS space, can used visualization analysis. also reports stress value, quantifies well low-dimensional configuration represents original distance relationships, lower stress indicating better fit. analysis resulted stress value 0.12.Additionally, object stores information optimization process, including number iterations whether algorithm successfully converged.","code":"\n## call sample data from vegan package\ndata(dune)\n## the first 3 species\nhead(dune[, 1:3])##   Achimill Agrostol Airaprae\n## 1        1        0        0\n## 2        3        0        0\n## 3        0        4        0\n## 4        0        8        0\n## 5        2        0        0\n## 6        2        0        0\n# Take the 'dune' dataset from vegan\ndune %>% \n  as_tibble() %>%         # Convert 'dune' from matrix/data frame to tibble for tidyverse compatibility\n  select(1:3) %>%         # Select the first three species (columns) for visualization\n  ggpairs() +             # Create a pairwise scatterplot matrix (plots all pairwise relationships)\n  theme_bw()              # Apply a clean, minimal theme with a white background\n# Compute pairwise dissimilarities between sites using Bray-Curtis distance\nm_bray <- vegdist(dune,       # Input community matrix (sites × species)\n                  method = \"bray\")  # Use Bray-Curtis dissimilarity, commonly used in ecology\n\n# Inspect the first 3 rows and columns of the distance matrix\n# Convert 'm_bray' (a dist object) to a standard matrix for easy subsetting\ndata.matrix(m_bray)[1:3, 1:3]  # Show pairwise dissimilarities among the first 3 sites##           1         2         3\n## 1 0.0000000 0.4666667 0.4482759\n## 2 0.4666667 0.0000000 0.3414634\n## 3 0.4482759 0.3414634 0.0000000\n# Perform Non-metric Multidimensional Scaling (NMDS) \nobj_nmds <- metaMDS(\n  m_bray,   # Input distance matrix (e.g., Bray-Curtis dissimilarities between sites)\n  k = 2     # Number of dimensions (axes) to represent in the NMDS solution\n)## Run 0 stress 0.1192678 \n## Run 1 stress 0.1183186 \n## ... New best solution\n## ... Procrustes: rmse 0.02027061  max resid 0.06496334 \n## Run 2 stress 0.1886532 \n## Run 3 stress 0.1886532 \n## Run 4 stress 0.1192678 \n## Run 5 stress 0.1183186 \n## ... Procrustes: rmse 4.650431e-06  max resid 1.53307e-05 \n## ... Similar to previous best\n## Run 6 stress 0.1192678 \n## Run 7 stress 0.1812933 \n## Run 8 stress 0.1183186 \n## ... Procrustes: rmse 1.144619e-06  max resid 3.080589e-06 \n## ... Similar to previous best\n## Run 9 stress 0.1192679 \n## Run 10 stress 0.1192679 \n## Run 11 stress 0.2035424 \n## Run 12 stress 0.1192678 \n## Run 13 stress 0.2075713 \n## Run 14 stress 0.1192678 \n## Run 15 stress 0.1900906 \n## Run 16 stress 0.1183186 \n## ... Procrustes: rmse 1.098301e-05  max resid 3.764092e-05 \n## ... Similar to previous best\n## Run 17 stress 0.1192678 \n## Run 18 stress 0.1192678 \n## Run 19 stress 0.1192678 \n## Run 20 stress 0.1183186 \n## ... Procrustes: rmse 1.983547e-05  max resid 6.214347e-05 \n## ... Similar to previous best\n## *** Best solution repeated 4 times"},{"path":"unconstrained-ordination.html","id":"visualize-nmds","chapter":"19 Unconstrained Ordination","heading":"Visualize NMDS","text":"Often, interested external factors shape community structure. explore relationships, helpful relate positions communities NMDS space relevant grouping environmental variables, allowing us visualize interpret patterns community composition.dune dataset comes along environmental data dune.env can associate ordination output (whether NMDS, PCoA, methods) site attributes.dune.env dataset contains 5 environmental variables 20 dune meadow sites.A1 measures thickness topsoil horizon (top mineral layer).A1 measures thickness topsoil horizon (top mineral layer).Moisture ordered factor indicating soil moisture levels dry wet (levels 1, 2, 4, 5).Moisture ordered factor indicating soil moisture levels dry wet (levels 1, 2, 4, 5).Management categorical variable describing management type: Biological farming (BF), Hobby farming (HF), Nature Conservation Management (NCM), Standard Farming (SF).Management categorical variable describing management type: Biological farming (BF), Hobby farming (HF), Nature Conservation Management (NCM), Standard Farming (SF).Use indicates land-use intensity ordered factor Hayfield Haypastu Pasture.Use indicates land-use intensity ordered factor Hayfield Haypastu Pasture.Manure ordered factor showing manure application levels 0 4.Manure ordered factor showing manure application levels 0 4.exercise, use “Use” column investigate whether variations land-use intensity correspond differences plant community composition. NMDS output stores coordinates site NMDS space $points element, rows ordered according original distance matrix. can directly associate site attributes coordinates, important ensure row order remains consistent avoid mismatches.\nFigure 19.4: NMDS ordination plant communities Dutch dune meadows dataset (dune), sites colored land-use intensity (Use). plot visualizes differences community composition among sites along first two NMDS axes.\nNMDS plot, appears sites different land-use intensities might occupy slightly different regions ordination space, suggesting potential differences plant community composition. visual inspection provides useful first impression, need formal statistical test determine whether apparent group differences significant.","code":"\ndata(dune.env)\nhead(dune.env)##    A1 Moisture Management      Use Manure\n## 1 2.8        1         SF Haypastu      4\n## 2 3.5        1         BF Haypastu      2\n## 3 4.3        2         SF Haypastu      4\n## 4 4.2        2         SF Haypastu      4\n## 5 6.3        1         HF Hayfield      2\n## 6 4.3        1         HF Haypastu      2\n# Combine environmental data with NMDS coordinates\ndf_nmds <- dune.env %>%           # Start with the environmental data for each site\n  as_tibble() %>%                  # Convert to tibble for tidyverse-friendly operations\n  bind_cols(obj_nmds$points) %>%   # Add NMDS coordinates (site scores) as new columns\n  janitor::clean_names()           # Clean column names (lowercase, replace spaces/special characters)\n\n# Visualize NMDS site scores with points and 95% confidence ellipses by land-use intensity\ndf_nmds %>% \n  ggplot(aes(\n    x = mds1,           # NMDS axis 1 (first dimension from metaMDS)\n    y = mds2,           # NMDS axis 2 (second dimension from metaMDS)\n    color = use         # Color points by land-use intensity (or other grouping variable)\n  )) +\n  geom_point(size = 3) +               # Plot each site as a point, slightly larger for visibility\n  stat_ellipse(level = 0.95,           # Draw 95% confidence ellipses around each group\n               linetype = 2) +        # Dashed line for ellipse\n  theme_bw() +                         # Apply a clean black-and-white theme\n  labs(color = \"Land-use intensity\",   # Add a legend title\n       x = \"NMDS1\",                    # Label x-axis\n       y = \"NMDS2\")                    # Label y-axis"},{"path":"unconstrained-ordination.html","id":"test-with-permanova","chapter":"19 Unconstrained Ordination","heading":"Test with PERMANOVA","text":"formally test whether plant community composition differs among levels land-use intensity, can use PERMANOVA (Permutational Multivariate Analysis Variance).Unlike traditional MANOVA, PERMANOVA assume multivariate normality can applied directly distance dissimilarity matrix, Bray-Curtis distances used NMDS analysis.approach assesses whether centroids groups—, categories “Use” variable—differ significantly multivariate space. adonis2() function vegan package can used perform analysis, testing whether plant community composition differs among groups based chosen environmental factor.used PERMANOVA test whether plant communities differ among three land-use intensity categories (use). method looks overall differences species composition groups compares differences within groups. analysis, land-use intensity explains 13% variation plant communities, remaining 87% comes factors natural variation (column R2).test gave p-value 0.258, means differences see statistically significant. simple terms, even though NMDS plot shows separation sites different land-use intensity, test suggests differences easily happened chance. , strong evidence land-use intensity alone drives changes plant community composition dataset.","code":"\n# Perform PERMANOVA to test whether plant community composition differs by land-use intensity\nadonis2(\n  m_bray ~ use,   # Model formula: Bray-Curtis dissimilarities (m_bray) explained by 'use' factor\n  data = df_nmds  # Data frame containing the grouping variable 'use' aligned with the distance matrix\n)## Permutation test for adonis under reduced model\n## Terms added sequentially (first to last)\n## Permutation: free\n## Number of permutations: 999\n## \n## adonis2(formula = m_bray ~ use, data = df_nmds)\n##          Df SumOfSqs      R2      F Pr(>F)\n## use       2   0.5532 0.12867 1.2552  0.252\n## Residual 17   3.7459 0.87133              \n## Total    19   4.2990 1.00000"},{"path":"unconstrained-ordination.html","id":"other-options","chapter":"19 Unconstrained Ordination","heading":"19.1.3 Other Options","text":"many options unconstrained ordination, explored two extremes. PCA preserves quantitative information original variables, comes strict assumptions, including linearity normality. contrast, NMDS highly flexible applicable wide range multivariate data, preserves qualitative, ranked information reduced dimensions.ordination methods fall somewhere two approaches. example, Principal Coordinates Analysis (PCoA) can accommodate variety distance metrics; Correspondence Analysis (CA) designed categorical count data, rather continuous variables. Selecting appropriate method depends type distribution data well specific ecological questions addressed.","code":""},{"path":"unconstrained-ordination.html","id":"axis-interpretability","chapter":"19 Unconstrained Ordination","heading":"19.1.4 Axis Interpretability","text":"One key difference among ordination methods lies interpretability axes . Methods can broadly categorized directly interpretable axes abstract distance-based axes.PCA, CA, MCA produce axes meaningful transformations original data:PCA: axes (principal components) linear combinations original variables, coordinates along axis meaningful can interpreted terms original measurements. example, higher values PC1 may reflect larger trait values abundances across species. axis explains specific percentage total variance, loadings show original variables contribute component.PCA: axes (principal components) linear combinations original variables, coordinates along axis meaningful can interpreted terms original measurements. example, higher values PC1 may reflect larger trait values abundances across species. axis explains specific percentage total variance, loadings show original variables contribute component.CA: axes weighted averages species scores (sites) site scores (species), units standard deviations species turnover. Sites similar species composition cluster together, position along axis reflects species compositional change. Eigenvalues indicate strength axis capturing species turnover.CA: axes weighted averages species scores (sites) site scores (species), units standard deviations species turnover. Sites similar species composition cluster together, position along axis reflects species compositional change. Eigenvalues indicate strength axis capturing species turnover.MCA: Similar PCA categorical data, axes represent dimensions variation among categorical variables. Coordinates reflect association strength particular categories, contributions category dimension can quantified.MCA: Similar PCA categorical data, axes represent dimensions variation among categorical variables. Coordinates reflect association strength particular categories, contributions category dimension can quantified.PCoA NMDS create axes arbitrary units relative positions matter:PCoA: Axes preserve distances samples faithfully possible based chosen distance metric. axes arbitrary units, eigenvalues show much total distance variation axis captures. interpretation depends entirely distance metric used (e.g., Bray-Curtis, UniFrac).PCoA: Axes preserve distances samples faithfully possible based chosen distance metric. axes arbitrary units, eigenvalues show much total distance variation axis captures. interpretation depends entirely distance metric used (e.g., Bray-Curtis, UniFrac).NMDS: Axes purely abstract dimensions constructed best preserve rank order dissimilarities sites. numeric values along NMDS axes inherent meaning; relative distances points matter. Unlike PCoA, NMDS doesn’t even preserve exact distances—rank order. result, NMDS plots interpreted terms patterns similarity separation among sites, terms absolute values axis. quality fit measured stress values rather variance explained.NMDS: Axes purely abstract dimensions constructed best preserve rank order dissimilarities sites. numeric values along NMDS axes inherent meaning; relative distances points matter. Unlike PCoA, NMDS doesn’t even preserve exact distances—rank order. result, NMDS plots interpreted terms patterns similarity separation among sites, terms absolute values axis. quality fit measured stress values rather variance explained.Practical implication: methods interpretable axes (PCA, CA, MCA), can meaningfully describe high low values axis represent. distance-based methods (PCoA, NMDS), can describe samples similar dissimilar .","code":""},{"path":"constrained-ordination.html","id":"constrained-ordination","chapter":"20 Constrained Ordination","heading":"20 Constrained Ordination","text":"unconstrained ordination, axes derived solely multivariate response data, without reference external predictors. methods useful exploring dominant patterns, visualizing similarity among samples, generating hypotheses.many biological studies, however, explicitly interested external variables structure multivariate responses—example, species composition changes along gradients moisture, land use, management. Constrained ordination methods address question restricting ordination axes linear (unimodal) combinations measured explanatory variables. sense, constrained ordination can viewed multivariate extension regression.Learning Objectives:Understand conceptual difference unconstrained constrained ordination.Understand conceptual difference unconstrained constrained ordination.Apply constrained ordination methods (RDA, CCA, dbRDA) relate multivariate responses environmental predictors.Apply constrained ordination methods (RDA, CCA, dbRDA) relate multivariate responses environmental predictors.Interpret constrained ordination diagrams, including site scores, species scores, environmental vectors.Interpret constrained ordination diagrams, including site scores, species scores, environmental vectors.Quantify test proportion multivariate variation explained predictors using permutation tests.Quantify test proportion multivariate variation explained predictors using permutation tests.","code":"\npacman::p_load(tidyverse,\n               GGally,\n               vegan)"},{"path":"constrained-ordination.html","id":"methods-for-constrained-ordination","chapter":"20 Constrained Ordination","heading":"20.1 Methods for Constrained Ordination","text":"constrained ordination, term “constrained” refers fact ordination axes determined explanatory variables, rather emerging solely multivariate response data.axes represent linear (unimodal) combinations measured predictors.axes represent linear (unimodal) combinations measured predictors.Point coordinates ordination interpretable terms predictors.Point coordinates ordination interpretable terms predictors.points contrast unconstrained ordination, axes summarize total variation response data without reference external variables.effects predictors estimated identifying axes maximize variation multivariate response explained predictors. Mathematically, constrained ordination decomposes total variation response matrix portion explained predictors residual variation, successive axes capturing maximum remaining explained variation.","code":""},{"path":"constrained-ordination.html","id":"rda","chapter":"20 Constrained Ordination","heading":"20.1.1 RDA","text":"","code":""},{"path":"constrained-ordination.html","id":"lichen-pasture","chapter":"20 Constrained Ordination","heading":"Lichen pasture","text":"begin Redundancy Analysis (RDA), constrained version Principal Component Analysis (PCA). illustrate use case RDA, use varespec dataset. dataset contains observations 24 sampling sites (rows), column representing estimated cover one 44 plant species (lichen, bryophytes, vascular plants) sites. comes varechme data, include 14 variables summarize soil properties, nutrient concentrations physical characteristics.proceeding analysis, perform basic data cleaning ensure datasets easy work consistently formatted.Visualization highlights non-normal distributional patterns common biological datasets.Performing RDA straightforward, vegan package provides rda() function implementing analysis. example , set scale = FALSE (default rda()) response variables already standardized percent cover. variables measured different units (e.g., grams vs. meters) differ grearly variations, scale = TRUE necessary ensure variables contribute equally analysis.RDA results show total variance (inertia) species abundance data 1825.66, 28.3% (516.10) explained four soil chemistry predictors (n, p, k, ca) 71.7% (1309.56) remains unexplained (residual variation).first two axes RDA axes (constrained) captured explained variation, eigenvalues 244.71 (RDA1), 216.40 (RDA2), 30.61 (RDA3), 24.37 (RDA4). unconstrained axes, representing residual variation, larger eigenvalues (e.g., PC1 = 752.9, PC2 = 266.1), showing variation species composition accounted measured soil variables.Overall, indicates soil chemistry influences species distributions, substantial proportion multivariate variation structured unmeasured factors stochastic processes.However, may interested individual contribution predictor, particularly whether contributions statistically significant. assess , can use anova.cca() function vegan package, performs permutation-based significance tests constrained ordination models.important set = \"margin\" goal assess individual contribution predictor. alternative option = \"terms\", corresponds method analogous Type ANOVA, predictors tested sequentially order appear model formula. result, order predictors matters: example, specifying m_y ~ n + p + k + ca versus m_y ~ p + n + k + ca can lead different test results, predictor evaluated accounting entered .contrast, setting = \"margin\" invokes procedure analogous Type II ANOVA. , effect predictor tested accounting predictors model, regardless order formula. approach generally preferred predictors strictly hierarchical goal evaluate unique contribution environmental variable multivariate response.","code":"\n# Load example datasets from the vegan package\ndata(\"varespec\")  # Species abundance matrix: plant species measured at 44 sites\ndata(\"varechem\")  # Environmental variables (soil chemistry) for the same sites\n\n# Response matrix: species abundances\nm_y <- varespec\n\n# Convert column names to lowercase for consistency\n# This avoids issues with formulas or later data manipulation\ncolnames(m_y) <- str_to_lower(colnames(varespec))\n\n# Clean and prepare environmental data\n# - Convert to tibble for easier use with dplyr\n# - Use janitor::clean_names() to make column names lowercase, consistent, and safe for programming\ndf_env <- as_tibble(varechem) %>% \n  janitor::clean_names()\n# Pairwise scatterplot matrix of the first 3 species columns in m_y\nm_y %>%\n  ggpairs(\n    progress = FALSE,      # Disable the progress bar during plotting\n    columns = 1:3,         # Only use the first 3 species for the plot\n    aes(\n      alpha = 0.5           # Make points semi-transparent to reduce overplotting\n    )\n  ) +\n  theme_bw()               # Use a clean theme with white background\n# Run Redundancy Analysis (RDA)\n# - Response: m_y (species abundance matrix, varespec)\n#   Note: Hellinger transformation is recommended for raw species counts.\n# - Predictors: n, p, k, ca (numeric soil chemistry variables from df_env)\n#   These represent Nitrogen (n), Phosphorus (p), Potassium (k), and Calcium (ca) levels.\n# - Data: df_env (cleaned environmental variables)\n# The parentheses around obj_rda print the RDA summary immediately\n(obj_rda <- rda(m_y ~ n + p + k + ca,\n                data = df_env))## Call: rda(formula = m_y ~ n + p + k + ca, data = df_env)\n## \n##                 Inertia Proportion Rank\n## Total         1825.6594     1.0000     \n## Constrained    516.1041     0.2827    4\n## Unconstrained 1309.5553     0.7173   19\n## Inertia is variance \n## \n## Eigenvalues for constrained axes:\n##   RDA1   RDA2   RDA3   RDA4 \n## 244.71 216.40  30.61  24.37 \n## \n## Eigenvalues for unconstrained axes:\n##   PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8 \n## 752.9 266.1 109.3  65.4  33.2  27.1  19.4  12.4 \n## (Showing 8 of 19 unconstrained eigenvalues)\n# Perform permutation test on the RDA object\n# - obj_rda: RDA object containing species (response) and soil chemistry (predictors)\n# - by = \"margin\": test the marginal effect of each predictor\n#   This evaluates how much variation each predictor explains **after accounting for all other predictors**\n# - permutations = 999: number of random permutations used to generate the null distribution for the F-statistic\nanova.cca(obj_rda, \n          by = \"margin\", \n          permutations = 999)## Permutation test for rda under reduced model\n## Marginal effects of terms\n## Permutation: free\n## Number of permutations: 999\n## \n## Model: rda(formula = m_y ~ n + p + k + ca, data = df_env)\n##          Df Variance      F Pr(>F)  \n## n         1   189.63 2.7512  0.048 *\n## p         1    70.69 1.0256  0.370  \n## k         1    40.41 0.5863  0.649  \n## ca        1    80.83 1.1728  0.286  \n## Residual 19  1309.56                \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"constrained-ordination.html","id":"horseshoe-effect","chapter":"20 Constrained Ordination","heading":"Horseshoe effect","text":"Since RDA based PCA, implicitly assumes linear relationships among response variables. assumption often violated typical biological datasets, see . example illustrates RDA can inappropriate many biological response data, frequently non-normal non-linearly related. RDA tends perform better variables approximately normally distributed linearly related, physical measurements (e.g., temperature, water depth) certain biological traits (e.g., body size).applied non-linear data, visualization RDA axes can reveal artifacts horseshoe effect, mathematical distortion arises non-linear relationships inappropriately treated linear.\nFigure 20.1: RDA ordination plot sites based species composition. point represents site, positioned RDA scores (RDA1 RDA2), color gradient indicating soil nitrogen (n) concentration, illustrating environmental variation associated multivariate community patterns.\nplot clearly shows unimodal pattern (“horseshoe”) along RDA1 axis, reflect true biological pattern data. pattern appears, indicates RDA—ordination method assumes linear relationships—may inappropriate; statistical inference drawn results strongly biased.","code":"\n# Extract site (sample) scores from the RDA object\n# - display = \"sites\": returns the coordinates of sampling sites in RDA space\n# - scaling = 2: correlation scaling, which emphasizes relationships between sites\n#   and explanatory variables (i.e., how site positions correlate with predictors)\n# The site scores are then combined with the environmental data for visualization\ndf_rda <- scores(obj_rda, \n                 display = \"sites\",\n                 scaling = 2) %>% \n  bind_cols(df_env) %>%           # append environmental variables (e.g., soil chemistry)\n  janitor::clean_names()          # standardize column names for tidy workflows\n\n# Create a ggplot2 ordination plot\n# - Points represent sites positioned by their constrained community composition\n# - Color gradient reflects the nitrogen (n) concentration at each site\ndf_rda %>% \n  ggplot(aes(x = rda1,\n             y = rda2,\n             color = n)) +        # color sites by nitrogen level\n  geom_point() +\n  theme_bw() +\n  labs(x = \"RDA1\",\n       y = \"RDA2\",\n       color = \"Nitrogen\") +\n  scale_color_viridis_c()"},{"path":"constrained-ordination.html","id":"dbrda","chapter":"20 Constrained Ordination","heading":"20.1.2 dbRDA","text":"RDA provides important extension PCA include environmental predictors, also limitations, particularly species responses non-linear data non-normal. address , explore distance-based RDA (dbRDA) flexible alternative constrained ordination.Although dbRDA flexible NMDS, allows use distance dissimilarity matrix response input instead raw data, greatly expanding applicability biological datasets non-linear relationships.implementation requires minimal changes. can still use m_y previous example, instead rda(), apply dbrda() function distance = \"bray\". internally converts raw community matrix Bray-Curtis dissimilarity matrix, allowing distance-based constrained ordination.interpretation analogous RDA. total inertia represents overall variation Bray-Curtis dissimilarity among sites. constrained inertia (1.197) indicates 26% total variation species composition explained environmental predictors (n, p, k, ca), remaining variation (3.606) unconstrained residual.eigenvalues constrained axes (dbRDA1 – dbRDA4) indicate much explained variation axis captures, unconstrained axes (MDS1–MDS15) summarize residual variation distance space.Although output resembles RDA, important distinction dbRDA operates distance matrix, rather raw biological data (case, percent cover), allowing accommodate non-linear non-Euclidean relationships. way, dbRDA extends RDA complex biological datasets, providing flexible framework constrained ordination linear assumptions met.Re-running permutation test produces contrasting result soil nitrogen, now neither statistically significant marginally significant. highlights importance carefully assessing properties data applying linear ordination methods, assumptions violations can strongly affect statistical inference.can now examine whether dbRDA mitigated horseshoe effect, reducing distortions seen linear RDA species responses non-linear.\nFigure 20.2: Scatterplot sites dbRDA space. point represents site positioned according constrained ordination scores, summarizing variation species composition explained environmental predictors.\n","code":"\n# Perform distance-based Redundancy Analysis (dbRDA)\n# - Response: m_y (species abundance matrix)\n# - Predictors: n, p, k, ca (environmental variables: nitrogen, phosphorus, potassium, calcium)\n# - data = df_env: provides the environmental variables used as predictors\n# - distance = \"bray\": computes a Bray-Curtis dissimilarity matrix from the raw species abundances\n#   internally, before performing the constrained ordination\n# The result is a dbrda object representing the constrained ordination in distance space\n(obj_db <- dbrda(m_y ~ n + p + k + ca,\n                 data = df_env,\n                 distance = \"bray\"))## Call: dbrda(formula = m_y ~ n + p + k + ca, data = df_env, distance =\n## \"bray\")\n## \n##               Inertia Proportion Rank RealDims\n## Total           4.544      1.000              \n## Constrained     1.164      0.256    4        4\n## Unconstrained   3.381      0.744   19       14\n## Inertia is squared Bray distance \n## \n## Eigenvalues for constrained axes:\n## dbRDA1 dbRDA2 dbRDA3 dbRDA4 \n## 0.5896 0.3408 0.1298 0.1034 \n## \n## Eigenvalues for unconstrained axes:\n##   MDS1   MDS2   MDS3   MDS4   MDS5   MDS6   MDS7   MDS8 \n## 1.3418 0.7788 0.3983 0.2858 0.1825 0.1617 0.1186 0.0931 \n## (Showing 8 of 19 unconstrained eigenvalues)\n# Perform a permutation test on the dbRDA object\n# - obj_db: the dbRDA (dbrda) object containing species distances and predictors\n# - by = \"margin\": tests the **marginal effect** of each predictor, i.e., the unique contribution\n#   of a variable after accounting for all other predictors in the model\n# - permutations = 999: randomly permutes the data 999 times to generate a null distribution\n#   for the F-statistic, allowing assessment of statistical significance\nanova.cca(obj_db,\n          by = \"margin\",\n          permutations = 999)## Permutation test for dbrda under reduced model\n## Marginal effects of terms\n## Permutation: free\n## Number of permutations: 999\n## \n## Model: dbrda(formula = m_y ~ n + p + k + ca, data = df_env, distance = \"bray\")\n##          Df SumOfSqs      F Pr(>F)\n## n         1   0.2879 1.6177  0.136\n## p         1   0.1529 0.8593  0.507\n## k         1   0.1450 0.8150  0.581\n## ca        1   0.2733 1.5357  0.162\n## Residual 19   3.3809\n# Extract site scores from the dbRDA object\n# - display = \"sites\": retrieves the coordinates of each site in the constrained ordination space\n# - scaling = 2: correlation (or \"standardized\") scaling, which emphasizes relationships between sites\n#   and environmental predictors rather than absolute distances\n# - as_tibble(): converts the site scores to a tibble for easier manipulation with dplyr\n# - bind_cols(df_env): adds the original environmental variables for plotting, coloring, or annotation\n# - janitor::clean_names(): ensures consistent, safe column names for programming\n\ndf_db <- scores(obj_db, \n                display = \"sites\",\n                scaling = 2) %>% \n  as_tibble() %>%              \n  bind_cols(df_env) %>%        \n  janitor::clean_names()       \n\n# Create a basic ggplot ordination plot\n# - Each point represents a site in the dbRDA ordination space\n# - Points can later be colored, shaped, or labeled by environmental variables for interpretation\ndf_db %>% \n  ggplot(aes(x = db_rda1,\n             y = db_rda2)) +\n  geom_point() +               # plot sites as points\n  labs(x = \"dbRDA1\",\n       y = \"dbRDA2\")"},{"path":"constrained-ordination.html","id":"other-options-1","chapter":"20 Constrained Ordination","heading":"20.1.3 Other Options","text":"Like unconstrained ordination, various constrained ordination methods, different assumptions levels flexibility. Common approaches include RDA dbRDA, can seen two extremes: RDA assumes linear relationships normally distributed response variables, dbRDA allows non-Euclidean distances non-linear relationships.appropriate method depends data characteristics analytical goals. Methods stricter assumptions, like RDA, limited applicability, can provide quantitative precision assumptions met. flexible methods, dbRDA distance-based approaches, widely applicable, may sacrifice quantitative detail exchange robustness non-linear non-normal data.","code":""},{"path":"structural-equation-modeling.html","id":"structural-equation-modeling","chapter":"21 Structural Equation Modeling","heading":"21 Structural Equation Modeling","text":"path analysis structural equation modeling (SEM), interested understanding relationships among multiple variables simultaneously. Unlike simple regression, models single response function one predictors, SEM allows researchers specify networks mediating pathways, providing framework quantify direct indirect contributions.Learning Objectives:Understand conceptual difference univariate regression, path analysis, SEM.Understand conceptual difference univariate regression, path analysis, SEM.Specify fit path/SEM models using biological data, identifying direct indirect effects.Specify fit path/SEM models using biological data, identifying direct indirect effects.Interpret model outputs, including path coefficients, residual variances, fit statistics.Interpret model outputs, including path coefficients, residual variances, fit statistics.","code":"\npacman::p_load(tidyverse,\n               GGally,\n               vegan,\n               lavaan,\n               lavaanPlot)"},{"path":"structural-equation-modeling.html","id":"path-analysis","chapter":"21 Structural Equation Modeling","heading":"21.1 Path analysis","text":"Path analysis special case SEM. designed cases variables interest directly measured, interested direct indirect effects observed variables.Let’s use following data set, involves plant, herbivore, predator variables measured 100 plots.dataset contains measurements plot, including plant biomass (mass_plant), variation plant height expressed coefficient variation (cv_h_plant), herbivore biomass (mass_herbiv), predator biomass (mass_pred), along plot identifier (plot_id).","code":"\n# Specify the URL of the raw CSV file on GitHub\nurl <- \"https://raw.githubusercontent.com/aterui/biostats/master/data_raw/data_foodweb.csv\"\n\n# Read the CSV file into a tibble\n(df_fw <- read_csv(url))## # A tibble: 100 × 5\n##    plot_id mass_plant cv_h_plant mass_herbiv mass_pred\n##      <dbl>      <dbl>      <dbl>       <dbl>     <dbl>\n##  1       1       99.3       4.98        50.3      24.6\n##  2       2       99.8       4.97        49.9      24.7\n##  3       3      102.        4.99        50.5      25.1\n##  4       4      100.        4.96        50.2      24.9\n##  5       5      100.        4.99        50.4      25.2\n##  6       6      102.        5.05        50.4      25.1\n##  7       7      100.        4.92        50.4      25.3\n##  8       8       98.6       4.99        50.4      25.3\n##  9       9       99.3       5.06        50.1      24.7\n## 10      10       99.7       5.07        49.5      24.9\n## # ℹ 90 more rows"},{"path":"structural-equation-modeling.html","id":"directed-acyclic-graph","chapter":"21 Structural Equation Modeling","heading":"21.1.1 Directed acyclic graph","text":"Given bottom-influences system, variables may sequentially connected, plant biomass affecting herbivore biomass, turn affects predator biomass. time, variation plant height (cv_h_plant) may reflect shelter availability herbivores, suggesting variable also influence higher trophic levels.system, can start full network represented directed acyclic graph (DAG), encodes hypothesized causal relationships among variables.\nFigure 21.1: Directed acyclic graph (DAG) representing hypothesized causal structure among plant, herbivore, predator variables. Nodes denote measured variables (plant mass, plant height, herbivore mass, predator mass), directed edges indicate assumed directional effects. Plant mass plant height modeled exogenous variables influencing herbivore mass, turn influences predator mass; plant variables may also direct effects higher trophic levels.\nnetworks, discerning two types variables particularly important: endogenous exogenous variables.Endogenous variables whose values influenced variables system; one incoming arrows DAG. example, herbivore predator biomass endogenous depend plant biomass factors.Exogenous variables determined outside system influenced variables network; outgoing arrows. instance, plant biomass variation plant height can treated exogenous, drive changes higher trophic levels without influenced .makes network DAG absence loops: arrows flow lower higher trophic levels, feedback lower levels. excludes potential influences higher trophic levels lower ones (.e., feedback loops). However, cyclic relationships incorporated within standard SEM framework, one major limitations.","code":""},{"path":"structural-equation-modeling.html","id":"develop-a-hypothesis","chapter":"21 Structural Equation Modeling","heading":"21.1.2 Develop a hypothesis","text":"example represents full DAG, serves reasonable starting point.However, links may biologically meaningful—instance, reasonable assume herbivore biomass influenced plant attributes, variables likely less important predators.Visualizing network helps identify links potentially relevant among variables.Plant variables moderately correlated herbivore biomass, predator biomass. raw observations can guide hypothesis development provide rationale omitting direct plant--predator paths DAG.\nFigure 21.2: Simplified directed acyclic graph (DAG) representing hypothesized causal relationships among plant, herbivore, predator traits used focal analysis. Compared full DAG, hypothesis-driven DAG retains direct pathways primary interest, omitting direct links plant variables predator mass.\n","code":"\ndf_fw %>% \n  select(-plot_id) %>%        # remove plot_id column for plotting\n  ggpairs(                    # create pairwise scatterplots and correlations\n    progress = FALSE          # suppress progress messages\n  ) +\n  theme_bw()"},{"path":"structural-equation-modeling.html","id":"implementation","chapter":"21 Structural Equation Modeling","heading":"21.1.3 Implementation","text":"variety R packages can used implement SEM, focus lavaan, one widely used packages purpose. full description functionality available lavaan website, cover basics .key syntax ~, used specify regression-like relationship variables. Let’s start simple example. first define model corresponds hypothetical DAG.model description m1 passed sem() function lavaan package, performs analysis using data provide.important output sem() function Model Test User Model section, reports chi-square test statistic, degrees freedom, p-value.test statistic (1.466) measures discrepancy SEM model-implied covariance matrix observed covariance matrix, degrees freedom provide sense simple model relative full DAG—, many potential links full DAG omitted. words, test essentially asks: “model adequately reproduce observed relationships data?”Unlike ordinary hypothesis tests, p-value tests null hypothesis model fits data perfectly. high p-value (typically > 0.05) indicates evidence misfit. case, p-value 0.480 suggests model provides good fit data.can obtain estimates path model, quantify strength relationships among variables:Regressions: section informative. Estimate column reports raw effect sizes, values depend units variability predictor variables therefore directly comparable across predictors.setting standardize = TRUE, however, lavaan also reports Std., gives standardized coefficients (.e., effects common scale). standardized values allow direct comparison relative strength different paths.example, strongest path mass_herbiv mass_pred (0.482), followed mass_plant mass_herbiv (0.327), cv_h_plant mass_herbiv (0.208).results can visualized lavaanPlot() function lavaanPlot package.","code":"\n# Specify the SEM model using lavaan syntax\n# Note: the variable names must exactly match the column names in the dataframe (df_fw)\nm1 <- '\n  # regression of herbivore biomass on plant variables\n  mass_herbiv ~ mass_plant + cv_h_plant\n  # regression of predator biomass on herbivore biomass\n  mass_pred ~ mass_herbiv\n'\n(fit1 <- sem(model = m1,\n             data = df_fw))## lavaan 0.6-12 ended normally after 1 iterations\n## \n##   Estimator                                         ML\n##   Optimization method                           NLMINB\n##   Number of model parameters                         5\n## \n##   Number of observations                           100\n## \n## Model Test User Model:\n##                                                       \n##   Test statistic                                 1.466\n##   Degrees of freedom                                 2\n##   P-value (Chi-square)                           0.480\nsummary(fit1, standardize = TRUE)## lavaan 0.6-12 ended normally after 1 iterations\n## \n##   Estimator                                         ML\n##   Optimization method                           NLMINB\n##   Number of model parameters                         5\n## \n##   Number of observations                           100\n## \n## Model Test User Model:\n##                                                       \n##   Test statistic                                 1.466\n##   Degrees of freedom                                 2\n##   P-value (Chi-square)                           0.480\n## \n## Parameter Estimates:\n## \n##   Standard errors                             Standard\n##   Information                                 Expected\n##   Information saturated (h1) model          Structured\n## \n## Regressions:\n##                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n##   mass_herbiv ~                                                         \n##     mass_plant        0.173    0.050    3.449    0.001    0.173    0.327\n##     cv_h_plant        2.052    0.935    2.195    0.028    2.052    0.208\n##   mass_pred ~                                                           \n##     mass_herbiv       0.244    0.044    5.497    0.000    0.244    0.482\n## \n## Variances:\n##                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n##    .mass_herbiv       0.185    0.026    7.071    0.000    0.185    0.805\n##    .mass_pred         0.045    0.006    7.071    0.000    0.045    0.768\nlavaanPlot(model = fit1, coefs = TRUE, stand = TRUE)"},{"path":"structural-equation-modeling.html","id":"model-comparison-1","chapter":"21 Structural Equation Modeling","heading":"21.1.4 Model comparison","text":"model omits direct plant--predator pathways. However, variation plant height may still influence predators indirectly altering physical habitat structure. possibility represented following DAGs.\nFigure 21.3: Comparison two alternative directed acyclic graphs (DAGs) representing competing causal hypotheses among plant, herbivore, predator variables. Panel shows hypothesized DAG, predator mass influenced indirectly herbivore mass. Panel B shows alternative DAG includes additional direct pathway plant height predator mass, representing plausible untested mechanism (e.g., habitat structure plant-mediated effects).\ntest alternative hypothesis, determine whether new path worth including?can address comparing model performance using likelihood ratio test (LRT), uses difference chi-square values two models test statistic. rationale simpler model special case complex model (.e., models nested), improvement fit can attributed additional path.adding new path substantially reduces chi-square value relative increase model complexity (degrees freedom), suggests path explains meaningful structure data worth including.perform test, first need specify alternative model represents updated DAG additional pathway:can use anova() function comparison.output shows chi-square difference test comparing two nested SEMs. , fit2 complex model one fewer degree freedom (Df = 1), fit1 simpler model (Df = 2). Adding extra path reduces chi-square value 1.4662 0.3983, giving chi-square difference 1.0678 1 degree freedom.associated p-value (0.3014) significant, indicating complex model provide meaningful improvement fit. Consistent result, information criteria (AIC BIC) slightly lower fit1, supporting conclusion simpler, parsimonious model preferred.","code":"\nm2 <- '\n  mass_herbiv ~ mass_plant + cv_h_plant\n  mass_pred ~ mass_herbiv + cv_h_plant\n'\n\n(fit2 <- sem(m2,\n             data = df_fw))## lavaan 0.6-12 ended normally after 1 iterations\n## \n##   Estimator                                         ML\n##   Optimization method                           NLMINB\n##   Number of model parameters                         6\n## \n##   Number of observations                           100\n## \n## Model Test User Model:\n##                                                       \n##   Test statistic                                 0.398\n##   Degrees of freedom                                 1\n##   P-value (Chi-square)                           0.528\nanova(fit1, fit2)## Chi-Squared Difference Test\n## \n##      Df    AIC    BIC  Chisq Chisq diff Df diff Pr(>Chisq)\n## fit2  1 99.826 115.46 0.3983                              \n## fit1  2 98.894 111.92 1.4662     1.0678       1     0.3014"},{"path":"structural-equation-modeling.html","id":"structural-equation-modeling-1","chapter":"21 Structural Equation Modeling","heading":"21.2 Structural Equation Modeling","text":"section, extend path analysis structural equation modeling (SEM). distinguishes SEM simple path model ability incorporate latent variables DAG.Latent variables “unobserved” variables exist within statistical model. typically composites multiple observed variables, often strongly correlated , represent underlying constructs measured directly.","code":""},{"path":"structural-equation-modeling.html","id":"sem-vs.-path-analysis","chapter":"21 Structural Equation Modeling","heading":"21.2.1 SEM vs. Path analysis","text":"explore , use following dataset.Measurements taken individual plants grown different levels soil fertility, recorded column soil_n (inorganic nitrogen). measurements associated plant’s herbivory rate (herbivory), specific leaf area (sla), carbon--nitrogen ratio (cn_ratio), percent lignin (per_lignin). last three measurements – specific leaf area (sla), carbon--nitrogen ratio (cn_ratio), percent lignin (per_lignin) – plant traits may affect herbivory rate.first glance, might propose following DAG, models plant trait herbivory rate separate, individual elements.However, examining three traits, see highly correlated appear strongly influenced soil inorganic nitrogen:plant traits may collectively represent plant tissue “palatability,” influenced soil fertility , turn, affects herbivory rate.context, can define latent variable, palatability, function three plant traits. latent variable serves mediator linking soil fertility herbivory rate. addition, corresponding path diagram modified reflect inclusion latent construct.\nFigure 21.4: DAG structural equation model (SEM) linking soil fertility, palatability, herbivory. Observed variables (SLA, CN Ratio, % Lignin, Soil N, Herbivory) shown circles, latent variable Palatability shown square. Unlike simple path analysis, SEM allows inclusion latent constructs inferred multiple observed indicators, enabling decomposition complex traits like palatability underlying components.\n","code":"\nurl <- \"https://raw.githubusercontent.com/aterui/biostats/master/data_raw/data_herbivory.csv\"\n\n(df_herbv <- read_csv(url))## # A tibble: 100 × 5\n##    soil_n herbivory   sla cn_ratio per_lignin\n##     <dbl>     <dbl> <dbl>    <dbl>      <dbl>\n##  1    7.9      94.3 101.       9.4        6.9\n##  2   10.5      97.7 100.       9.1        6.9\n##  3    9.4     116.   99.9      9.7        8.1\n##  4    9.1     101.   99.9      9.8        8  \n##  5    7.7     101.   99.4     10.6        8.1\n##  6   10       117.  100.       9.4        7.8\n##  7    8.2     105.   98.8     10.3        8.3\n##  8    5.9      87.4  98.9     11.1        8.7\n##  9    8.8      93.1 101        9.4        7.4\n## 10   12.3      95.5 101.      10          7.8\n## # ℹ 90 more rows\ndf_herbv %>% \n  ggpairs(\n    progress = FALSE,\n    columns = c(\"soil_n\",\n                \"sla\",\n                \"cn_ratio\",\n                \"per_lignin\")\n  ) +\n  theme_bw()"},{"path":"structural-equation-modeling.html","id":"implementation-1","chapter":"21 Structural Equation Modeling","heading":"21.2.2 Implementation","text":"implementing SEM, use special syntax =~ specify latent variables model. example, may use following syntax: palatability =~ sla + cn_ratio + perlignin construct latent variable composed three plant traits. including latent constructs, SEM extends traditional path analysis, offering flexible framework model complex relationships.primary output interpretation similar - case, p-value > 0.05, suggesting model specification reasonably good. can exploit information calling summary():major difference simple path analysis inclusion Latent Variables:, decomposes construct palatability—variable exists statistical model, directly observed. Estimate column shows factor loadings, indicate strongly observed variable contributes latent variable palatability.example, palatability positively associated sla negatively associated cn_ratio per_lignin. means higher palatability corresponds leaves higher specific leaf area (SLA), lower carbon--nitrogen ratio (C:N), lower lignin content (% lignin).Regressions: section shows pathways soil fertility palatability palatability herbivory rate significant, strongest effect observed soil fertility palatability.","code":"\nm_sem <- '\n# latent variable\n  palatability =~ sla + cn_ratio + per_lignin\n  \n# regression\n  palatability ~ soil_n\n  herbivory ~ palatability\n'\n\n(fit_sem <- sem(m_sem,\n                data = df_herbv))## lavaan 0.6-12 ended normally after 29 iterations\n## \n##   Estimator                                         ML\n##   Optimization method                           NLMINB\n##   Number of model parameters                         9\n## \n##   Number of observations                           100\n## \n## Model Test User Model:\n##                                                       \n##   Test statistic                                 8.970\n##   Degrees of freedom                                 5\n##   P-value (Chi-square)                           0.110\nsummary(fit_sem, standardize = TRUE)## lavaan 0.6-12 ended normally after 29 iterations\n## \n##   Estimator                                         ML\n##   Optimization method                           NLMINB\n##   Number of model parameters                         9\n## \n##   Number of observations                           100\n## \n## Model Test User Model:\n##                                                       \n##   Test statistic                                 8.970\n##   Degrees of freedom                                 5\n##   P-value (Chi-square)                           0.110\n## \n## Parameter Estimates:\n## \n##   Standard errors                             Standard\n##   Information                                 Expected\n##   Information saturated (h1) model          Structured\n## \n## Latent Variables:\n##                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n##   palatability =~                                                       \n##     sla               1.000                               0.616    0.884\n##     cn_ratio         -0.837    0.067  -12.584    0.000   -0.516   -0.882\n##     per_lignin       -1.107    0.079  -14.002    0.000   -0.682   -0.942\n## \n## Regressions:\n##                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n##   palatability ~                                                        \n##     soil_n            0.108    0.025    4.365    0.000    0.176    0.422\n##   herbivory ~                                                           \n##     palatability      4.190    1.480    2.832    0.005    2.581    0.284\n## \n## Variances:\n##                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n##    .sla               0.106    0.021    5.117    0.000    0.106    0.218\n##    .cn_ratio          0.076    0.015    5.162    0.000    0.076    0.221\n##    .per_lignin        0.059    0.019    3.080    0.002    0.059    0.112\n##    .herbivory        75.957   10.804    7.030    0.000   75.957    0.919\n##    .palatability      0.312    0.057    5.494    0.000    0.822    0.822"},{"path":"structural-equation-modeling.html","id":"limitation","chapter":"21 Structural Equation Modeling","heading":"21.3 Limitation","text":"examples rely one major assumption: lavaan package, variables model assumed follow normal distribution. can restrictive assumption, especially given variety data types often encountered biological research.Importantly, limitation comes package, underlying statistics. Mathematically, possible construct path analysis SEM non-normal variables even include random effects.However, models challenging estimate using standard methods likelihood function becomes highly complex. cases, piecewise SEM (Lefcheck 2015) Bayesian methods provide flexible framework (examples: Takagi et al. 2014; Terui et al 2018). currently packages allow fully automated implementation, coding models directly BUGS Stan makes possible fit complex SEMs (steep learning curve).","code":""},{"path":"piecewise-sem.html","id":"piecewise-sem","chapter":"22 Piecewise SEM","heading":"22 Piecewise SEM","text":"Path analysis Structural Equation Modeling (SEM) provide powerful framework addressing mediating interactions among two variables. Traditional SEM implemented using covariance-based approaches assumes multivariate normality large sample sizes. contrast, piecewise SEM extends framework fitting individual component models (e.g., linear models, generalized linear models, mixed-effects models) evaluating overall network structure using tests conditional independence. makes piecewise SEM especially well suited biological data, often violate assumptions normality, independence, simple experimental design.chapter, focus piecewise SEM can used link biological processes, quantify direct indirect effects, evaluate competing hypotheses using real biological data.Learning Objectives:Understand conceptual differences traditional piecewise SEM.Understand conceptual differences traditional piecewise SEM.Specify fit piecewise SEM models using biological data.Specify fit piecewise SEM models using biological data.Interpret model outputs, including path coefficients, residual variances, overall model fit statistics.Interpret model outputs, including path coefficients, residual variances, overall model fit statistics.","code":"\npacman::p_load(tidyverse,\n               GGally,\n               piecewiseSEM)"},{"path":"piecewise-sem.html","id":"non-normal-distributions","chapter":"22 Piecewise SEM","heading":"22.1 Non-normal Distributions","text":"","code":""},{"path":"piecewise-sem.html","id":"post-fire-plant-diversity-the-original-model","chapter":"22 Piecewise SEM","heading":"22.1.1 Post-fire plant diversity: the original model","text":"One major limitations lavaan package reliance multivariate normality; easily accommodate non-normal distributions response (endogeneous) variables. biological datasets, variables often deviate normality, treating normally distributed can increase risk inappropriate statistical inference.Piecewise SEM addresses limitation allowing component models follow variety distributions, Poisson, binomial, negative binomial, making much flexible real-world biological data. important note, however, piecewise SEM full SEM traditional sense; support latent variables within directed acyclic graph (DAG). practice, piecewise SEM understood extension path analysis combines generalized /mixed-effect models retaining network structure relationships among variables.illustrate utility piecewise SEM, use dataset Grace Keeley (2006). study, authors examined relationships among landscape position, abiotic conditions, fire severity, plant diversity. dataset conveniently included piecewiseSEM package, allowing us replicate explore ecological relationships using modern piecewise SEM methods.Keeley dataset contains nine ecological variables measured across 90 post-fire shrubland sites California: distance fire source meters (distance), elevation sea level meters (elev), abiotic conditions index represents composite score multiple non-biological environmental factors soil properties (abiotic), within-plot heterogeneity measuring spatial variability species composition (hetero), stand age years since previous fire event (age), fire severity (firesev), fractional vegetation cover (cover), species richness plants (rich).proposed DAG Grace Keeley (2006) follows – landscape position (represented distance coast) affects abiotic conditions habitat heterogeneity, topography, soil, microclimate vary across landscape. Stand age influences fire severity, since older vegetation tends accumulate fuel burn differently. Fire severity, turn, affects vegetation cover, altering plant species richness via direct effects cover, abiotic conditions, habitat heterogeneity, integrating cumulative influence upstream environmental disturbance factors.\nFigure 22.1: Conceptual path diagram illustrating hypothesized relationships among landscape position, environmental conditions, disturbance, plant diversity based Grace Keeley (2006). Arrows indicate directional (causal) paths specified piecewise structural equation model. Distance represents landscape position, influences abiotic conditions, heterogeneity, stand age. Fire severity vegetation cover act intermediate variables affecting plant species richness. diagram represents directed acyclic graph (DAG) used define component models piecewise SEM analysis.\npsem() function takes multiple “piecewise” model objects input combines single causal network. component models, constructs path diagram evaluates implied conditional independence claims define structural equation model.conditional independence claims correspond missing paths proposed DAG. implied independence relationships statistically violated —- meaning missing path fact significant – suggests proposed model structure incomplete may omitting important biological relationships.example, begin fitting piecewise SEM response variables assumed follow normal distributions. reflects original approach used Grace Keeley (2006) provides useful baseline understanding method operates extending complex, non-normal models.Tests directed separation: report missing paths DAG statistically significant. independence claims rejected, suggests proposed model structure may incomplete. example, relationships rich ~ distance + ... cover ~ hetero + ... identified significant, indicating pathways may need explicitly included final model.Global goodness--fit: reports chi-squared statistic Fisher’s C. Unlike covariance-based SEM (e.g., lavaan), chi-squared statistic piecewise SEM derived variance–covariance matrix. Instead, computed comparing summed log-likelihoods component models saturated unsaturated model structures, allowing framework accommodate non-normal response distributions.Fisher’s C provides alternative measure overall model fit calculated \\(C = -2 \\sum_i \\ln(p_i)\\), \\(\\) indexes conditional independence claim \\(p_i\\) corresponding p-value. Large values Fisher’s C (.e., small p-values) indicate one independence assumptions violated, suggesting proposed model may missing important pathways.Coefficients: summarizes estimated effects component model. lavaan framework, raw parameter estimates (Estimate) standardized coefficients (Std.Estimate) reported. raw estimates depend scale predictors therefore directly comparable, whereas standardized estimates allow comparison effect sizes across variables. table clearly identifies response predictor variables path, provides concise interpretable summary overall model output.","code":"\ndata(\"keeley\")\n\n(df_keeley <- keeley %>% \n  as_tibble())## # A tibble: 90 × 8\n##    distance  elev abiotic   age hetero firesev cover  rich\n##       <dbl> <int>   <dbl> <int>  <dbl>   <dbl> <dbl> <int>\n##  1     53.4  1225    60.7    40  0.757    3.5  1.04     51\n##  2     37.0    60    40.9    25  0.491    4.05 0.478    31\n##  3     53.7   200    51.0    15  0.844    2.6  0.949    71\n##  4     53.7   200    61.2    15  0.691    2.9  1.19     64\n##  5     52.0   970    46.7    23  0.546    4.3  1.30     68\n##  6     52.0   970    39.8    24  0.653    4    1.17     34\n##  7     52.0   950    56.4    35  0.742    4.8  0.862    39\n##  8     51.4   740    47.0    14  0.822    4.8  0.419    66\n##  9     37.0   170    42.1    45  0.603    7.25 0.129    25\n## 10     37.9   190    32.6    35  0.805    6.2  0.306    31\n## # ℹ 80 more rows\n# Define individual models\nm1 <- lm(abiotic ~ distance, data = df_keeley)\nm2 <- lm(hetero ~ distance, data = df_keeley)\nm3 <- lm(firesev ~ age, data = df_keeley)\nm4 <- lm(cover ~ firesev, data = df_keeley)\nm5 <- lm(rich ~ cover + abiotic + hetero, data = df_keeley)\n\n# Combine into piecewise SEM\nsem_model <- psem(m1, m2, m3, m4, m5)\n\n# Evaluate\nsummary(sem_model, .progressBar = FALSE)## \n## Structural Equation Model of sem_model \n## \n## Call:\n##   abiotic ~ distance\n##   hetero ~ distance\n##   firesev ~ age\n##   cover ~ firesev\n##   rich ~ cover + abiotic + hetero\n## \n##     AIC\n##  1523.060\n## \n## ---\n## Tests of directed separation:\n## \n##             Independ.Claim Test.Type DF Crit.Value P.Value   \n##   firesev ~ distance + ...      coef 87    -1.6793  0.0967   \n##     cover ~ distance + ...      coef 87     1.3302  0.1869   \n##      rich ~ distance + ...      coef 85     3.1710  0.0021 **\n##        abiotic ~ age + ...      coef 87    -0.0652  0.9481   \n##         hetero ~ age + ...      coef 87     0.0043  0.9966   \n##          cover ~ age + ...      coef 87    -1.8018  0.0750   \n##           rich ~ age + ...      coef 85    -1.2670  0.2086   \n##     hetero ~ abiotic + ...      coef 87     1.3296  0.1871   \n##    firesev ~ abiotic + ...      coef 86    -0.9713  0.3341   \n##      cover ~ abiotic + ...      coef 86    -0.2678  0.7895   \n##     firesev ~ hetero + ...      coef 86     0.4923  0.6237   \n##       cover ~ hetero + ...      coef 86    -2.7229  0.0078 **\n##       rich ~ firesev + ...      coef 84    -1.3692  0.1746   \n## \n## --\n## Global goodness-of-fit:\n## \n## Chi-Squared = 30.929 with P-value = 0.003 and on 13 degrees of freedom\n## Fisher's C = 48.924 with P-value = 0.004 and on 26 degrees of freedom\n## \n## ---\n## Coefficients:\n## \n##   Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate    \n##    abiotic  distance   0.3998    0.0823 88     4.8562   0e+00       0.4597 ***\n##     hetero  distance   0.0045    0.0013 88     3.4593   8e-04       0.3460 ***\n##    firesev       age   0.0597    0.0125 88     4.7781   0e+00       0.4539 ***\n##      cover   firesev  -0.0839    0.0184 88    -4.5594   0e+00      -0.4371 ***\n##       rich     cover  17.0138    3.7923 86     4.4864   0e+00       0.3573 ***\n##       rich   abiotic   0.6847    0.1607 86     4.2609   1e-04       0.3481 ***\n##       rich    hetero  55.3686   10.8212 86     5.1167   0e+00       0.4209 ***\n## \n##   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05\n## \n## ---\n## Individual R-squared:\n## \n##   Response method R.squared\n##    abiotic   none      0.21\n##     hetero   none      0.12\n##    firesev   none      0.21\n##      cover   none      0.19\n##       rich   none      0.49"},{"path":"piecewise-sem.html","id":"post-fire-plant-diversity-the-revised-model","chapter":"22 Piecewise SEM","heading":"22.1.2 Post-fire plant diversity: the revised model","text":"response variables may better described non-normal distributions, can extend piecewise SEM framework fitting generalized linear models. particular, species richness count variable appropriately modeled using negative binomial distribution.addition, analysis highlighted potentially important pathways included original DAG – specifically, direct effects landscape position (distance) richness (rich) heterogeneity (hetero) vegetation cover (cover). paths represent direct influences factor may fully captured mediating variables original model., revise previous model fitting species richness negative binomial GLM adding missing paths.including previously missing paths, Fisher’s C statistic became non-significant, indicating revised model fits data reasonably well. results can visualized using plot() function, solid arrows indicate statistically significant pathways dashed arrows indicate non-significant pathways.\nFigure 22.2: Visualization piecewise structural equation model Keeley dataset. Solid arrows indicate statistically significant pathways, dashed arrows represent non-significant pathways. Node labels correspond variables DAG: distance (distance coast), abiotic (abiotic conditions), hetero (habitat heterogeneity), age (stand age), firesev (fire severity), cover (vegetation cover), rich (plant species richness).\n","code":"\n# Define individual models\nm1 <- lm(abiotic ~ distance, data = df_keeley)      # unchanged\nm2 <- lm(hetero ~ distance, data = df_keeley)       # unchanged\nm3 <- lm(firesev ~ age, data = df_keeley)           # unchanged\n\n# m4 now includes a direct effect of hetero on cover (added path)\nm4 <- lm(cover ~ firesev + hetero, data = df_keeley)  \n\n# m5 now models richness as negative binomial (MASS::glm.nb) \n# and includes direct effect of distance on richness (added path)\nm5 <- MASS::glm.nb(rich ~ cover + abiotic + hetero + distance, \n                   data = df_keeley)  \n\n# Combine into piecewise SEM\nsem_model <- psem(m1, m2, m3, m4, m5)\n\n# Evaluate model\nsummary(sem_model, .progressBar = FALSE)## \n## Structural Equation Model of sem_model \n## \n## Call:\n##   abiotic ~ distance\n##   hetero ~ distance\n##   firesev ~ age\n##   cover ~ firesev + hetero\n##   rich ~ cover + abiotic + hetero + distance\n## \n##     AIC\n##  1514.731\n## \n## ---\n## Tests of directed separation:\n## \n##             Independ.Claim Test.Type DF Crit.Value P.Value  \n##   firesev ~ distance + ...      coef 87    -1.6793  0.0967  \n##     cover ~ distance + ...      coef 86     2.2342  0.0281 *\n##        abiotic ~ age + ...      coef 87    -0.0652  0.9481  \n##         hetero ~ age + ...      coef 87     0.0043  0.9966  \n##           rich ~ age + ...      coef 84    -0.8843  0.3765  \n##          cover ~ age + ...      coef 86    -2.0164  0.0469 *\n##     hetero ~ abiotic + ...      coef 87     1.3296  0.1871  \n##    firesev ~ abiotic + ...      coef 86    -0.9713  0.3341  \n##      cover ~ abiotic + ...      coef 85     0.1213  0.9038  \n##     firesev ~ hetero + ...      coef 86     0.4923  0.6237  \n##       rich ~ firesev + ...      coef 83    -1.5271  0.1267  \n## \n## --\n## Global goodness-of-fit:\n## \n## Chi-Squared = 17.339 with P-value = 0.098 and on 11 degrees of freedom\n## Fisher's C = 30.829 with P-value = 0.1 and on 22 degrees of freedom\n## \n## ---\n## Coefficients:\n## \n##   Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate    \n##    abiotic  distance   0.3998    0.0823 88     4.8562  0.0000       0.4597 ***\n##     hetero  distance   0.0045    0.0013 88     3.4593  0.0008       0.3460 ***\n##    firesev       age   0.0597    0.0125 88     4.7781  0.0000       0.4539 ***\n##      cover   firesev  -0.0859    0.0181 87    -4.7399  0.0000      -0.4472 ***\n##      cover    hetero  -0.5299    0.2607 87    -2.0329  0.0451      -0.1918   *\n##       rich     cover   0.3277    0.0777 85     4.2200  0.0000       0.3102 ***\n##       rich   abiotic   0.0096    0.0033 85     2.8888  0.0039       0.2206  **\n##       rich    hetero   0.9762    0.2285 85     4.2718  0.0000       0.3344 ***\n##       rich  distance   0.0103    0.0032 85     3.2787  0.0010       0.2725  **\n## \n##   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05\n## \n## ---\n## Individual R-squared:\n## \n##   Response     method R.squared\n##    abiotic       none      0.21\n##     hetero       none      0.12\n##    firesev       none      0.21\n##      cover       none      0.23\n##       rich nagelkerke      0.78\nplot(sem_model)"},{"path":"piecewise-sem.html","id":"random-effects","chapter":"22 Piecewise SEM","heading":"22.2 Random Effects","text":"","code":""},{"path":"piecewise-sem.html","id":"tree-growth-data-structure","chapter":"22 Piecewise SEM","heading":"22.2.1 Tree growth: data structure","text":"Another simplifying assumption traditional SEMs data points independent. assumption often violated, particularly repeated measurements individuals time sampling spatially clustered within confined locality. hierarchical structures data-generating process properly accounted . Piecewise SEM can accommodate incorporating random effects, allowing model account non-independence among observations. use shipley data Shipley 2009 illustrate case.dataset contains information tree growth (Growth) survival (Live) given observation year, along environmental variables may influence demographic responses. include cumulative degree days first bud burst (DD) date first bud burst (Date).proceeding analysis, first perform data cleaning ensure accuracy consistency.Importantly, dataset contains temporally spatially repeated measurements – individuals tracked growth time die (tree indexes individuals), five individuals sampled site (site indexes locations). Clearly, observations independent, hierarchical structure must accounted analysis.begin developing hypotheses, first explore pairwise relationships among variables.relationships appear clear. Increasing cumulative degree days (dd) promotes earlier bud burst trees (date; julian date year), turn may enhance growth (growth) survival (live).\nFigure 22.3: Directed acyclic graph (DAG) illustrating hypothesized relationships among key variables shipley dataset. Arrows indicate direction hypothesized influence: cumulative degree days (DD) affects date first bud burst (Date first bud), influences Growth, turn affects Survival.\n","code":"\ndata(\"shipley\")\n\ndf_shipley <- shipley %>% \n  as_tibble() %>% \n  janitor::clean_names() %>% \n  drop_na(growth)\ndf_shipley %>% \n  group_by(site) %>% \n  summarize(n_tree = n_distinct(tree))## # A tibble: 20 × 2\n##     site n_tree\n##    <int>  <int>\n##  1     1      5\n##  2     2      5\n##  3     3      5\n##  4     4      5\n##  5     5      5\n##  6     6      5\n##  7     7      5\n##  8     8      5\n##  9     9      5\n## 10    10      5\n## 11    11      5\n## 12    12      5\n## 13    13      5\n## 14    14      5\n## 15    15      5\n## 16    16      5\n## 17    17      5\n## 18    18      5\n## 19    19      5\n## 20    20      5\n# Pairwise plots of key variables\ndf_shipley %>% \n  ggpairs(\n    columns = c(\"dd\", \n                \"date\",\n                \"growth\",\n                \"live\"),  # select variables to plot\n    progress = FALSE      # disable progress bar\n  ) +\n  theme_bw()              # clean theme for the plot"},{"path":"piecewise-sem.html","id":"tree-growth-implementation","chapter":"22 Piecewise SEM","heading":"22.2.2 Tree growth: implementation","text":"psem() function accepts GLMMs class glmmTMB lmerMod (lme4 package), making straightforward include random effects piecewise SEM. example, include site tree random effects. Since independent, unnested random effects, specifying separate terms model formula appropriate.model specification interpretation largely . case, chi-squared test Fisher’s C non-significant, indicating model fits data well.One key difference Individual R-squared: section, now reports Marginal Conditional values. represent model’s ability explain variation using fixed effects (marginal) using fixed random effects (conditional). random effects often explain substantial portion variation—particularly growth model—conditional R-squared values typically much higher. Note R-squared values defined following Nakagawa Schielzeth (2012) differ traditional R-squared.","code":"\n# Model 1: date depends on dd, with random intercepts for site and tree\nm1 <- glmmTMB(date ~ dd + (1 | site) + (1 | tree), \n              data = df_shipley,\n              family = \"gaussian\")\n\n# Model 2: growth depends on date, same random effects\nm2 <- glmmTMB(growth ~ date + (1 | site) + (1 | tree), \n              data = df_shipley,\n              family = \"gaussian\")\n\n# Model 3: live (binary) depends on growth, logistic mixed model\nm3 <- glmmTMB(live ~ growth + (1 | site) + (1 | tree), \n              data = df_shipley, \n              family = \"binomial\")\n\n# Combine models into a piecewise SEM\nsem_glmm <- psem(m1, m2, m3)\n\n# Summarize SEM (paths, significance, and Shipley's test)\nsummary(sem_glmm, .progressBar = FALSE)## \n## Structural Equation Model of sem_glmm \n## \n## Call:\n##   date ~ dd\n##   growth ~ date\n##   live ~ growth\n## \n##     AIC\n##  12567.398\n## \n## ---\n## Tests of directed separation:\n## \n##      Independ.Claim Test.Type   DF Crit.Value P.Value \n##   growth ~ dd + ...      coef 1431    -0.2368  0.8128 \n##     live ~ dd + ...      coef 1431    -0.2234  0.8232 \n##   live ~ date + ...      coef 1431    -1.3764  0.1687 \n## \n## --\n## Global goodness-of-fit:\n## \n## Chi-Squared = 2.14 with P-value = 0.544 and on 3 degrees of freedom\n## Fisher's C = 4.363 with P-value = 0.628 and on 6 degrees of freedom\n## \n## ---\n## Coefficients:\n## \n##   Response Predictor Estimate Std.Error   DF Crit.Value P.Value Std.Estimate    \n##       date        dd  -0.4976    0.0049 1431  -100.9187       0      -0.6281 ***\n##     growth      date   0.3005    0.0268 1431    11.2202       0       0.3822 ***\n##       live    growth   0.3479    0.0584 1431     5.9598       0       0.7866 ***\n## \n##   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05\n## \n## ---\n## Individual R-squared:\n## \n##   Response method Marginal Conditional\n##       date   none     0.42        0.98\n##     growth   none     0.11        0.83\n##       live   none     0.56        0.63"},{"path":"piecewise-sem.html","id":"limitations","chapter":"22 Piecewise SEM","heading":"22.3 Limitations","text":"piecewise SEM greatly expands scope data analysis, notable limitation inability incorporate latent variables—feature implemented current package. However, inclusion latent variables mathematical limitation, limitation implementation. Incorporating latent variables possible within generalized SEM frameworks, although fitting process becomes challenging. interested approach, Bayesian modeling offers flexible alternative, emerging packages (e.g., brms package) increasingly allow analyses without directly writing Stan BUGS code. Nevertheless, extensions require solid understanding underlying statistical principles.","code":""},{"path":"time-series-analysis.html","id":"time-series-analysis","chapter":"23 Time-Series Analysis","heading":"23 Time-Series Analysis","text":"Time-Series Analysis one challenging topics statistics violates fundamental assumption: independence data points. Nevertheless, type data common biology often analyzed incorrectly. section, explore common pitfalls time-series analysis review statistical models can account temporal dependence.Learning Objectives:Understand challenges inherent time-series analysis.Understand challenges inherent time-series analysis.Learn basic modeling approaches time-series data.Learn basic modeling approaches time-series data.Learn incorporate external predictors time-series models.Learn incorporate external predictors time-series models.","code":"\npacman::p_load(tidyverse,\n               forecast,\n               lterdatasampler,\n               daymetr,\n               glarma)"},{"path":"time-series-analysis.html","id":"pitfall-of-time-series","chapter":"23 Time-Series Analysis","heading":"23.1 Pitfall of Time-Series","text":"","code":""},{"path":"time-series-analysis.html","id":"time-series-anormalies","chapter":"23 Time-Series Analysis","heading":"23.1.1 Time-series anormalies","text":"Let’s explore simple time series dataset, showing air-quality anomalies measured 100 years.plot dataset shows air-quality anomalies tend increase time.\nFigure 23.1: Time-series annual anomalies. Points represent observed values year, lines connect consecutive observations highlight temporal trends.\nProviding statistical evidence appears straightforward — can simply regress anomalies time.strong, positive effect observation year, slope estimate 0.04, indicating anomalies increase 0.04 per year average.\nFigure 23.2: Time-series anomalies years. Dotted lines connect consecutive observations, semi-transparent points show individual measurements, solid line represents fitted linear trend linear regression model.\nregression line may appear fit data well, obvious issues. However, think , already fallen common pitfall time-series analysis — type analysis many potential problems.","code":"\nurl <- \"https://raw.githubusercontent.com/aterui/biostats/master/data_raw/data_ts_anormaly.csv\"\n(df_ts <- read_csv(url))## # A tibble: 100 × 2\n##    anormaly  year\n##       <dbl> <dbl>\n##  1    0      1926\n##  2   -0.560  1927\n##  3   -0.791  1928\n##  4    0.768  1929\n##  5    0.839  1930\n##  6    0.968  1931\n##  7    2.68   1932\n##  8    3.14   1933\n##  9    1.88   1934\n## 10    1.19   1935\n## # ℹ 90 more rows\n# Plot time-series anomalies\ndf_ts %>% \n  ggplot(aes(x = year,          # Map 'year' to the x-axis\n             y = anormaly)) +  # Map 'anormaly' to the y-axis\n  geom_line() +                  # Add a line connecting the points\n  geom_point() +                 # Add points at each observation\n  theme_bw() +                   # Use a clean black-and-white theme\n  labs(\n    x = \"Year\",                     # Label x-axis\n    y = \"Anomaly\"                   # Label y-axis\n  )\n# Fit a simple linear model: anomaly as a function of year\nm_lm <- lm(anormaly ~ year, data = df_ts)\n\n# Show the model summary\nsummary(m_lm)## \n## Call:\n## lm(formula = anormaly ~ year, data = df_ts)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -4.0830 -1.1333 -0.1559  1.2911  5.6330 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -90.770684  12.715802  -7.138 1.66e-10 ***\n## year          0.047154   0.006436   7.327 6.73e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.858 on 98 degrees of freedom\n## Multiple R-squared:  0.3539, Adjusted R-squared:  0.3473 \n## F-statistic: 53.68 on 1 and 98 DF,  p-value: 6.729e-11\ndf_ts %>% \n  ggplot(aes(x = year, \n             y = anormaly)) +  # Set up the plot: x = year, y = anomaly\n  geom_line(linetype = \"dotted\") +     # Draw a dotted line connecting the points\n  geom_point(alpha = 0.25) +           # Add points with transparency (alpha = 0.25)\n  geom_abline(intercept = coef(m_lm)[1],  # Add regression line from linear model\n              slope = coef(m_lm)[2]) +\n  theme_bw()                             # Use a clean black-and-white theme"},{"path":"time-series-analysis.html","id":"temporal-autocorrelation","chapter":"23 Time-Series Analysis","heading":"23.1.2 Temporal autocorrelation","text":"data appear show increasing trend, dataset generated simulation assumes true temporal increase — produced process known random walk.\\[\ny_t = y_{t-1} + \\varepsilon_{t}\\\\\n\\varepsilon_t \\sim \\text{Normal}(0, \\sigma^2)\n\\]system unique observation depends immediate past: previous value (\\(y_{t-1}\\)) plus noise (\\(\\varepsilon_t\\)) produces current value (\\(y_t\\)).behavior random walk highly stochastic, producing different patterns simulation run. can observe running following code, generated dataset shown .expected measuring object time, observations independent. leads temporal autocorrelation, feature violates fundamental assumption many statistical methods — independence data points. Temporal autocorrelation can assessed using acf() function R:\nFigure 23.3: Autocorrelation function plot. x-axis represents lag, time difference observations (e.g., lag 1 correlation consecutive observations, lag 2 two steps apart, etc.), y-axis shows autocorrelation coefficient, ranging -1 1, indicating strongly observations lag correlated.\nACF (autocorrelation function) plot, x-axis represents lag, time difference observations (e.g., lag 1 correlation consecutive observations, lag 2 two steps apart, etc.), y-axis shows autocorrelation coefficient, ranging -1 1, indicating strongly observations lag correlated.Positive values mean observations tend move direction previous ones, negative values indicate opposite movement, values near zero suggest little correlation. dashed lines represent approximate confidence bounds, spikes outside lines indicate statistically significant correlations lag.Treating temporally correlated measurements independent can seriously compromise statistical inference lead unsupported conclusions., explore accommodate challenging nature time-series data using basic models, models can extended assess influence external factors also change time biological data analysis.","code":"\n# Initialize a vector to store the time series (100 time steps)\ny <- rep(NA, 100)\n\n# Generate random noise for each time step\neps <- rnorm(n = length(y))\n\n# Set the initial condition of the time series\ny[1] <- 0\n\n# Generate a random walk:\n# each value equals the previous value plus random noise\nfor (t in 1:(length(y) - 1)) {\n  y[t + 1] <- y[t] + eps[t]\n}\n\n# Combine the time series with a corresponding year variable\ndf_y <- tibble(\n  anormaly = y,\n  year = 1925 + seq_len(length(y))\n)\n\n# Plot time-series anomalies (not shown)\ndf_y %>% \n  ggplot(aes(x = year,\n             y = anormaly)) +\n  geom_line() +              \n  geom_point() +             \n  theme_bw() +               \n  labs(\n    x = \"Year\",              \n    y = \"Anomaly\"            \n  )\n## Data must be ordered from the oldest observation to the most recent\ndf_ts <- arrange(df_ts, year)\nacf(df_ts$anormaly)"},{"path":"time-series-analysis.html","id":"basic-models","chapter":"23 Time-Series Analysis","heading":"23.2 Basic Models","text":"starting point, focus AR, MA, ARMA models, assume stationary process. extend frameworks ARIMA model, incorporates integration step handle non-stationary time series.Stationarity fundamental concept time-series analysis: stationary process constant mean, variance, autocorrelation structure time. Non-stationarity means one properties change time—example, series may exhibit trends, shifts variability, evolving temporal dependence—making direct modeling inference challenging without first transforming data.use built-LakeHuron dataset learn implementation modeling approaches. models discussed can implemented using Arima() function forecast package, since AR, MA, ARMA models special cases general ARIMA framework.\nFigure 23.4: Time series Lake Huron water levels 1875 1972. Points represent annual water levels, dotted line connects consecutive observations. solid black line shows linear trend reference , providing visual guide overall direction change facilitating comparison predictions time-series models.\n","code":"\n# Create a tibble (modern data frame) from the LakeHuron time series\ndf_huron <- tibble(\n  year = time(LakeHuron),                # Extracts the time component (years) from the LakeHuron ts object\n  water_level = as.numeric(LakeHuron)    # Converts LakeHuron values to numeric (from ts class)\n) %>% \n  arrange(year)                           # Ensures the data is ordered by year\n\n# Plot Lake Huron time series with a linear trend\ndf_huron %>% \n  ggplot(aes(x = year, y = water_level)) +\n  geom_point(alpha = 0.25) +       # Semi-transparent points\n  geom_line(linetype = \"dotted\") + # Dotted line connecting points\n  geom_smooth(method = \"lm\",       # Linear trend line\n              color = \"black\",\n              linewidth = 0.5) +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Water Level\")"},{"path":"time-series-analysis.html","id":"ar-model","chapter":"23 Time-Series Analysis","heading":"23.2.1 AR Model","text":"autoregressive (AR) model describes time series current value depends one past values plus random noise.\\[\ny_t = \\mu + \\sum_{=1}^p \\phi_i y_{t - } + \\varepsilon_t\n\\]\\(\\mu\\) constant term (intercept), \\(\\phi_i\\) autoregressive parameter, \\(\\varepsilon_t\\) (white) noise.AR(\\(p\\)) model, present observation linear combination previous \\(p\\) observations. \\(p\\) referred order model. AR models stationary sum autoregressive parameters \\(\\sum_i \\phi_i\\) less one – analogous population models carrying capacity ecology.practice, supply vector time-series data arima() function. begin simplest autoregressive model order one, specified setting first element order argument 1 (.e., order = c(1, 0, 0)).setup corresponds setting order (\\(p = 1\\)) AR model, meaning process “remembers” previous observation. remaining elements order argument define components model, relevant additional models introduced .can extract -sample fitted values AR(1) model using fitted() function. , compare model predictions simple linear trend illustrate time-series model captures temporal dependence beyond linear model provides.\nFigure 23.5: Observed Lake Huron water levels (points) -sample fitted values AR(1) model (steelblue line). AR(1) model captures temporal dependence series, providing better representation year--year variation simple linear trend.\nAR models assume stationarity, meaning expected values variance remain constant time. projecting time-series data future, AR models can produce forecasts account temporal dependence, often giving different realistic picture simple linear trend.predict() function preforms projection n.ahead argument.\nFigure 23.6: Observed Lake Huron water levels (points) AR(1) model fitted values 50-year forecasts (steelblue line). shaded grey ribbon represents 95% confidence intervals around predictions, illustrating uncertainty forecast. visualization highlights AR(1) model captures temporal dependence projects future values differently simple linear trend.\n","code":"\n(m_ar1 <- Arima(\n  df_huron$water_level,       # The time series data we want to model\n  order = c(1, 0, 0)          # ARIMA model orders: c(p, d, q)\n))## Series: df_huron$water_level \n## ARIMA(1,0,0) with non-zero mean \n## \n## Coefficients:\n##          ar1      mean\n##       0.8375  579.1153\n## s.e.  0.0538    0.4240\n## \n## sigma^2 = 0.5199:  log likelihood = -106.6\n## AIC=219.2   AICc=219.45   BIC=226.95\n# Add fitted values from AR(1) model to the dataset\ndf_huron_ar1 <- df_huron %>% \n  mutate(fit = fitted(m_ar1) %>%   # Extract fitted (in-sample predicted) values from the AR(1) model\n           as.numeric())           # Convert to numeric if necessary\n\n# Plot observed and fitted values\ndf_huron_ar1 %>% \n  ggplot() +\n  geom_point(aes(x = year, \n                 y = water_level),\n             alpha = 0.25) +        # Plot observed water levels\n  geom_line(aes(x = year, \n                y = fit),           # Plot AR(1) fitted values\n            color = \"steelblue\") +\n  theme_bw()                        # Clean black-and-white theme\n# Add in-sample fitted values\ndf_huron_ar1 <- df_huron %>% \n  mutate(fit = fitted(m_ar1) %>% as.numeric())\n\n# Forecast the next 50 time points using the AR(1) model\ndf_pred_ar1 <- forecast::forecast(m_ar1, h = 50) %>% \n  as_tibble(rownames = \"year\") %>% \n  janitor::clean_names() %>% \n  mutate(year = as.numeric(year) + min(df_huron$year)) %>% \n  rename(fit = point_forecast)\n\n# Combine historical AR(1) fitted values and future forecasts, then plot\ndf_huron_ar1 %>% \n  bind_rows(df_pred_ar1) %>% \n  ggplot() +\n  geom_point(aes(x = year, y = water_level),  # Observed water levels (semi-transparent)\n             alpha = 0.25) +\n  geom_ribbon(aes(x = year,                   # Visualize forecast uncertainty\n                  ymin = lo_95,\n                  ymax = hi_95),\n              fill = \"grey\",\n              color = NA,\n              alpha = 0.6) +\n  geom_line(aes(x = year, y = fit),           # Fitted values for historical data + forecasts\n            color = \"steelblue\") +\n  theme_bw() +\n  labs(x = \"Year\",\n       y = \"Water Level\")"},{"path":"time-series-analysis.html","id":"ma-model","chapter":"23 Time-Series Analysis","heading":"23.2.2 MA Model","text":"moving average (MA) model represents time series function past random noises rather past observations. MA(q) model, current value depends current previous q error terms.\\[\ny_t = \\mu + \\sum_{=1}^q \\theta_i \\varepsilon_{t-} + \\varepsilon_t\n\\]\\(\\mu\\) constant term (intercept), \\(\\theta_i\\) moving-average parameter, \\(\\varepsilon_t\\) (white) noise. Arima() function, specifying order = c(0, 0, 1) corresponds moving-average model order 1 (MA(1)), current value depends recent random noise.can perform similar fitting, forecasting, visualization MA models, highlights behavior differs AR models. particular, average projected values quickly converges average observed values.\nFigure 23.7: Observed Lake Huron water levels (points) MA(1) model fitted values 50-year forecasts (steelblue line). shaded grey ribbon represents 95% confidence intervals around predictions, illustrating uncertainty forecast.\nconvergence occurs MA models capture autocorrelation error terms, current value directly depend past observations, unlike AR models.","code":"\n(m_ma1 <- Arima(\n  df_huron$water_level,       # The time series data we want to model\n  order = c(0, 0, 1)          # ARIMA model orders: c(p, d, q)\n))## Series: df_huron$water_level \n## ARIMA(0,0,1) with non-zero mean \n## \n## Coefficients:\n##          ma1      mean\n##       0.8302  578.9982\n## s.e.  0.0633    0.1580\n## \n## sigma^2 = 0.7517:  log likelihood = -124.65\n## AIC=255.3   AICc=255.55   BIC=263.05"},{"path":"time-series-analysis.html","id":"arma-model","chapter":"23 Time-Series Analysis","heading":"23.2.3 ARMA Model","text":"ARMA model combines autoregressive moving-average components, allowing current value depend past observations past random noises.\\[\ny_t = \\mu + \\sum_{=1}^p\\phi_i y_{t-} + \\sum_{=1}^q \\theta_i\\varepsilon_{t-} + \\varepsilon_t\n\\]parameters defined . Since ARMA model combines AR MA components, specify order = c(1, 0, 1) include first-order AR MA terms, allowing current value depend previous observation recent random noise.can visualize ARMA(1,1) model way AR(1) MA(1) models combining historical fitted values --sample forecasts displaying uncertainty:\nFigure 23.8: Observed Lake Huron water levels (points) ARMA(1,1) model fitted values 50-year forecasts (steelblue line). shaded grey ribbon represents 95% confindence intervals around predictions, illustrating uncertainty forecast.\n","code":"\n(m_arma <- Arima(\n  df_huron$water_level,       # The time series data we want to model\n  order = c(1, 0, 1)          # ARIMA model orders: c(p, d, q)\n))## Series: df_huron$water_level \n## ARIMA(1,0,1) with non-zero mean \n## \n## Coefficients:\n##          ar1     ma1      mean\n##       0.7449  0.3206  579.0555\n## s.e.  0.0777  0.1135    0.3501\n## \n## sigma^2 = 0.4899:  log likelihood = -103.25\n## AIC=214.49   AICc=214.92   BIC=224.83"},{"path":"time-series-analysis.html","id":"arima-model","chapter":"23 Time-Series Analysis","heading":"23.2.4 ARIMA model","text":"ARIMA model can thought ARMA model applied differenced series. illustrate, consider ARIMA model first-order differencing. define new variable: \\(\\delta_t = y_t - y_{t-1}\\).ARIMA fits ARMA(p, q) model \\(\\delta_t\\), capturing autoregressive moving-average structure differenced (stationary) series:\\[\n\\delta_t = \\sum_{=1}^p \\phi_i \\delta_{t-} + \\sum_{=1}^q \\theta_i \\varepsilon_{t-} +\\varepsilon_t\n\\]special case general d-order ARIMA(p,d,q) model, specifically ARIMA(p,1,q) \\(d=1\\). detailed explanations examples, can refer well-developed documentation, WikipediaThis extension allows ARIMA models handle non-stationary time series, mean trend changes time. , assume \\(p=1\\) \\(d=1\\) illustrate gain insights ARIMA model operates.constant term estimated series first-order differenced. visualization highlights shifting mean influences model’s projections time – uncertainty greater stationary models.\nFigure 23.9: Observed Lake Huron water levels (points) ARIMA(1,1,0) model fitted values 50-year forecasts (steelblue line). shaded grey ribbon represents 95% confindence intervals around predictions, illustrating forecast uncertainty.\n","code":"\n(m_arima <- Arima(df_huron$water_level,\n                  order = c(1, 1, 0)))## Series: df_huron$water_level \n## ARIMA(1,1,0) \n## \n## Coefficients:\n##          ar1\n##       0.1362\n## s.e.  0.1021\n## \n## sigma^2 = 0.5544:  log likelihood = -108.23\n## AIC=220.45   AICc=220.58   BIC=225.6"},{"path":"time-series-analysis.html","id":"model-selection","chapter":"23 Time-Series Analysis","heading":"23.2.5 Model selection","text":"Selecting appropriate model orders can challenging, multiple model types (AR, MA, ARMA, ARIMA) many possible combinations parameters (p, d, q). One common approach guide model selection use information criteria, Akaike Information Criterion (AIC), balance model fit complexity.approach conveniently implemented forecast package. particular, function auto.arima() automatically searches range ARIMA models selects combination parameters yields parsimonious model data, simplifying model selection process.case, ARIMA(2,1,1) selected, suggests non-stationary series shifting mean (due differencing) autocorrelations changes captured AR MA terms.","code":"\n# Possible extensions / additional arguments:\n# max.p, max.q       : maximum AR and MA orders to consider\n# d, D               : specify non-seasonal or seasonal differencing (or let auto.arima decide)\n# seasonal           : include seasonal ARIMA components (TRUE/FALSE)\n# max.P, max.Q       : maximum seasonal AR and MA orders\n# approximation      : use approximation for faster computation on long series\n# lambda             : Box-Cox transformation for stabilizing variance\n\nauto.arima(\n  df_huron$water_level, # data\n  stepwise = FALSE,  # stepwise = FALSE: search all possible models rather than using stepwise approximation\n  ic = \"aic\" # model selection based on AIC\n)## Series: df_huron$water_level \n## ARIMA(2,1,1) \n## \n## Coefficients:\n##          ar1      ar2      ma1\n##       0.9712  -0.2924  -0.9108\n## s.e.  0.1137   0.1030   0.0712\n## \n## sigma^2 = 0.5003:  log likelihood = -102.54\n## AIC=213.07   AICc=213.51   BIC=223.37"},{"path":"time-series-analysis.html","id":"applied-models","chapter":"23 Time-Series Analysis","heading":"23.3 Applied Models","text":"time-series models introduced allow us account temporal autocorrelation, primary interest often external variables influence response. , , can incorporate predictors time-series model?Recall linear model following structure predictors \\(x\\) included:\\[\ny_i = \\alpha + \\sum_k \\beta_k x_{}^{(k)} + \\varepsilon_i\n\\]\\(\\alpha\\) intercept, \\(\\beta\\) slope. framework, assume error term \\(\\varepsilon_i\\) independent across observations \\(\\varepsilon_i \\sim \\text{Normal}(0, \\sigma^2)\\).However, assume independence error term, problems arise data exhibit temporal autocorrelation. model relationship \\(x\\) \\(y\\) observations independent, effect \\(x\\) can estimated artificially significant, increasing risk Type error; , may conclude \\(x\\) significant effect even .can include ARIMA components error term account temporal autocorrelation evaluating effects external predictors. example, error term follows ARMA process, model can written :\\[\ny_t = \\alpha + \\sum_k \\beta_k x_{t}^{(k)} + u_t\\\\\nu_t = \\underbrace{\\sum_{=1}^p \\phi_i y_{t - }}_{\\text{AR}} + \\underbrace{\\sum_{= 1}^q \\theta_i \\varepsilon_{t - }}_{\\text{MA}} + \\underbrace{\\varepsilon_t}_{\\text{Noise}}\n\\]R offers variety functions performing type modeling. Let’s explore works practice.","code":""},{"path":"time-series-analysis.html","id":"arimax","chapter":"23 Time-Series Analysis","heading":"23.3.1 ARIMAX","text":"ARIMAX ARIMA model includes external predictors. illustrate use, use ice-cover data Lake Mendota, Wisconsin, available lmerdatasampler R package. focus period 1980 2014 (35 years).\nFigure 23.10: Ice-cover data Lake Mendota (1980–2014) visualized dotted line semi-transparent points. plot shows temporal trajectory ice duration 35 years, highlighting interannual variability.\nexplore ice-cover duration varies air temperature. Daily temperature data can obtained DAYMET, daymetr R package allows download data directly R.original data format difficult work , transform structure compatible tidyverse framework. following code calculates average daily-minimum temperature.can now join temperature data ice-duration data, enabling analysis air temperature influences ice-cover duration.Arima() auto.arima() functions include xreg argument, allows us supply potential predictor variables. xreg can vector matrix, must number rows response time series (ice duration case).ARIMAX output now includes estimated coefficient external predictor (xreg) along selected model structure (AR(1)). coefficient represents impact temperature ice-cover duration appears highly significant, estimate -8.06 standard error 2.67.can also compute confidence intervals obtain detailed information uncertainty around estimate. 95% confidence interval (CI) coefficient include zero, estimate considered statistically significant highly reliable.case, 95% confidence interval xreg coefficient (temperature effect) entirely negative overlap zero, indicating ice duration decreases significantly winter temperature increases.Including predictor variables makes forecast meaningful. Although model fitted data 1980–2014, temperature observations 2015–2024. use observed temperatures exercise, predictor values also come model outputs, climate model projections, allowing us explore potential impacts climate change.include temperature predictor, forecast longer constant varies according future temperature values.\nFigure 23.11: Historical ice-cover duration (points) ARIMAX model predictions (blue line) Lake Mendota, including forecasts 2015–2024. Shaded ribbons show 80% 95% confidence intervals forecasts. temperature included predictor, forecast varies future temperature values rather remaining constant.\n","code":"\n# Load and inspect the dataset\ndata(\"ntl_icecover\")  # Built-in dataset with ice cover information for multiple lakes\n\n# Convert to tibble and filter for a specific lake and years >= 1980\ndf_ice <- ntl_icecover %>% \n  as_tibble() %>% # Convert to tibble for tidyverse-friendly operations\n  filter(between(year, 1980, 2014), # Keep only years from 1980 onward\n         lakeid == \"Lake Mendota\") # Keep only data for Lake Mendota\n\n# Create a line + point plot of ice duration over time\ndf_ice %>% \n  ggplot(aes(x = year, # Year on x-axis\n             y = ice_duration)) + # Ice duration (days) on y-axis\n  geom_line(linetype = \"dotted\") + # Add a dotted line connecting points\n  geom_point(alpha = 0.25) + # Add semi-transparent points\n  theme_bw() + # Use a clean black-and-white theme\n  labs(y = \"Ice duration\",\n       x = \"Year\")\n# Download daily climate data from Daymet for Lake Mendota\nlist_mendota <- download_daymet(\n  site = \"Lake_Mendota\",   # Arbitrary name you assign to this site\n  lat = 43.1,              # Latitude of the lake\n  lon = -89.4,             # Longitude of the lake\n  start = 1980,            # Start year\n  end = 2024,              # End year\n  internal = TRUE          # Return the data as an R object rather than saving to disk\n)\ndf_temp <- list_mendota$data %>% \n  as_tibble() %>%                  # Convert the data to a tibble for tidyverse-friendly operations\n  janitor::clean_names() %>%       # Clean column names (lowercase, underscores)\n  mutate(\n    # Create a proper Date object from year and day-of-year\n    date = as.Date(paste(year, yday, sep = \"-\"), format = \"%Y-%j\"),\n    # Extract the month from the date\n    month = month(date)\n  ) %>% \n  arrange(year, yday) %>% \n  group_by(year) %>% # Group by year\n  summarize(temp_min = round(mean(tmin_deg_c), 2)) # Compute average daily minimum temperature\n# Join the temperature data to the ice cover data by year\ndf_ice <- df_ice %>% \n  left_join(df_temp, by = \"year\")\n(obj_arima <- auto.arima(\n  df_ice$ice_duration, # Response variable: ice duration time series\n  xreg = df_ice$temp_min, # External predictor: average minimum temperature\n  stepwise = FALSE # Use exhaustive search instead of stepwise approximation for model selection\n))## Series: df_ice$ice_duration \n## Regression with ARIMA(1,0,0) errors \n## \n## Coefficients:\n##           ar1  intercept     xreg\n##       -0.4650   107.3676  -8.0622\n## s.e.   0.1484     6.6398   2.6708\n## \n## sigma^2 = 364.5:  log likelihood = -151.44\n## AIC=310.89   AICc=312.22   BIC=317.11\nconfint(obj_arima, level = 0.95)##                 2.5 %      97.5 %\n## ar1        -0.7559461  -0.1741116\n## intercept  94.3538337 120.3814411\n## xreg      -13.2967927  -2.8276105\n# 1. Add fitted values from the ARIMAX model to the original ice-cover dataset\ndf_ice <- df_ice %>% \n  mutate(fit = fitted(obj_arima)) # 'fitted()' gives the model's in-sample predicted values\n\n# 2. Prepare new temperature data for forecasting\nv_temp <- df_temp %>% \n  filter(year > 2014) %>% # Select years beyond the original modeling period\n  pull(temp_min) # Extract the temperature values as a numeric vector\n\n# 3. Forecast ice duration for 2015–2024 using ARIMAX model with new temperature values\ndf_fore <- forecast::forecast(obj_arima, xreg = v_temp) %>% # Forecast with external predictor\n  as_tibble() %>% # Convert forecast object to tibble\n  janitor::clean_names() %>% # Clean column names\n  rename(fit = point_forecast) %>% # Rename point forecast column to 'fit'\n  mutate(year = 2015:2024) # Add year column for alignment\n\n# 4. Combine historical data with forecasted data\ndf_ice <- df_ice %>% \n  bind_rows(df_fore)  # Append forecasted years to the original dataset\n# Combine historical fitted values and forecasts, then plot\ndf_ice %>% \n  ggplot() +\n  # 1. Observed ice duration points\n  geom_point(aes(x = year, y = ice_duration), alpha = 0.25) + \n  # 2. 95% confidence interval ribbon for forecasted values\n  geom_ribbon(aes(x = year, ymin = lo_95, ymax = hi_95),\n              fill = \"grey\", alpha = 0.6) + \n  # 3. 80% confidence interval ribbon for forecasted values\n  geom_ribbon(aes(x = year, ymin = lo_80, ymax = hi_80),\n              fill = \"grey\", alpha = 0.6) + \n  # 4. Line showing fitted values (historical + forecast)\n  geom_line(aes(x = year, y = fit), color = \"steelblue\") +  \n  # 5. Clean theme\n  theme_bw() +\n  # 6. Axis labels\n  labs(x = \"Year\", y = \"Ice duration\")"},{"path":"time-series-analysis.html","id":"glarma","chapter":"23 Time-Series Analysis","heading":"23.3.2 GLARMA","text":"One major limitation ARIMAX models assumption normally distributed errors, makes unsuitable non-normal data. overcome , Generalized Linear ARMA (GLARMA) model developed, analogous extension linear models generalized linear models.next example, analyze fish count data Cutthroat Trout Mack Creek, Oregon. count data, modeling temporal process using normal distribution inappropriate, making GLARMA suitable choice.Cutthroat Trout cold-water species, summer maximum temperature may important predictor abundance. investigate , obtained climate data Mack Creek DAYMET.ARIMAX example, can summarize summer temperature data using date reference.data format glarma() somewhat unique. response variable must provided vector, predictors (summer temperature case) must supplied matrix. also essential include column intercept (column ones); without , interpretation model coefficients can difficult.type argument specifies error distribution response variable. Since modeling count data, first choice Poisson distribution. (type = \"Poi\").phiLags argument defines order(s) AR process. Unlike Arima() function, can specify multiple AR orders separately—example, 1 3, 1 5. example, start first-order AR term baseline.Linear Model Coefficients section can interpreted much like output standard GLM. temperature effect estimated insignificant - compare result glm() function Poisson errors, produce significant effect summer temperature (.e., likely Type error due temporal autocorrelation)!addition, GLARMA provides extra sections – notably, GLARMA Coefficients – summarize estimated autoregressive (AR) parameters. example, first-order AR parameter, phi_1, estimated approximately 0.16. parameter estimated link scale; Poisson distribution, corresponds log scale.Unfortunately, glarma() function include automated model selection. Therefore, must choose compare model orders manually. , explore different AR orders, 1 3. following code fits three models reports AIC values comparison.AR(1) model lowest AIC, suggesting initial choice first-order autoregressive term reasonable. Arima() function, GLARMA model allows us obtain fitted values using fitted() function.\nFigure 23.12: Observed annual cutthroat trout counts Mack Creek, Oregon, colored maximum summer temperature, fitted values first-order GLARMA model overlaid line.\n","code":"\n# Load HJ Andrews LTER vertebrate survey data\ndata(\"and_vertebrates\")\n\ndf_ct <- and_vertebrates  %>% \n  filter(species == \"Cutthroat trout\",\n         length_1_mm > 160) %>%  # Keep only large cutthroat trout (hypothetical matured adults)\n  group_by(year) %>% # Group records by year\n  summarize(n = n(), # Count number of captures per year\n            .groups = \"drop\")\n\ndf_ct %>% \n  ggplot(aes(x = year,\n             y = n)) +\n  geom_line(linetype = \"dotted\") +\n  geom_point() +\n  labs(x = \"Year\",\n       y = \"Count\") +\n  theme_bw()\n# Download daily climate data from Daymet for Lake Mendota\nlist_mack <- download_daymet(\n  site = \"Mack_Creek\",   # Arbitrary name you assign to this site\n  lat = 44.2276,         # Latitude of the lake\n  lon = -122.1673,       # Longitude of the lake\n  start = 1987,          # Start year\n  end = 2019,            # End year\n  internal = TRUE        # Return the data as an R object rather than saving to disk\n)\ndf_stemp <- list_mack$data %>% \n  janitor::clean_names() %>% \n    mutate(\n    date = as.Date(paste(year, yday, sep = \"-\"), format = \"%Y-%j\"),\n    month = month(date)\n  ) %>% \n  filter(month %in% c(6:8)) %>% # Select only summer months\n  group_by(year) %>% \n  summarize(temp_max = mean(tmax_deg_c)) # Averaging daily max temperature\n\n# Add annual precipitation data by matching on year\ndf_ct <- df_ct %>% \n  left_join(df_stemp,\n            by = \"year\")\n# Response variable: annual cutthroat trout counts\ny <- df_ct$n \n\n# Design matrix for mean structure\nX <- model.matrix(~ temp_max, # Intercept + summer temperature predictor\n                  data = df_ct)\n\nobj_glarma <- glarma(\n  y = y,                               # Count time series\n  X = X,                               # GLM design matrix\n  phiLags = 1,                         # AR structure in residuals (lag 1)\n  type = \"Poi\"                         # Poisson to allow count data\n)\n\nsummary(obj_glarma)                    # Model coefficients and diagnostics## \n## Call: glarma(y = y, X = X, type = \"Poi\", phiLags = 1)\n## \n## Pearson Residuals:\n##     Min       1Q   Median       3Q      Max  \n## -2.0852  -0.8918  -0.0879   1.0448   3.2881  \n## \n## GLARMA Coefficients:\n##       Estimate Std.Error z-ratio Pr(>|z|)    \n## phi_1   0.1644    0.0415   3.962 7.42e-05 ***\n## \n## Linear Model Coefficients:\n##             Estimate Std.Error z-ratio Pr(>|z|)    \n## (Intercept)  3.50886   0.91518   3.834 0.000126 ***\n## temp_max    -0.05208   0.03614  -1.441 0.149537    \n## \n##     Null deviance: 76.138  on 32  degrees of freedom\n## Residual deviance: 53.679  on 30  degrees of freedom\n## AIC: 191.0176 \n## \n## Number of Fisher Scoring iterations: 30\n## \n## LRT and Wald Test:\n## Alternative hypothesis: model is a GLARMA process\n## Null hypothesis: model is a GLM with the same regression structure\n##           Statistic  p-value    \n## LR Test       10.45  0.00123 ** \n## Wald Test     15.70 7.42e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nlist_glarma <- lapply(1:3,\n                      function(o) {\n                        obj_glarma <- glarma(y = y, \n                                             X = X, \n                                             phiLags = c(1:o),\n                                             type = \"Poi\")\n                      })\n\nlapply(list_glarma, function(x) x$aic)## [[1]]\n## [1] 191.0176\n## \n## [[2]]\n## [1] 192.2589\n## \n## [[3]]\n## [1] 192.8682\ndf_ct %>% \n  mutate(fit = as.numeric(fitted(obj_glarma))) %>%  # Add GLARMA fitted values as a new column\n  ggplot() +\n  geom_point(aes(\n    x = year,\n    y = n,\n    color = temp_max\n  )) +\n  geom_line(aes(\n    x = year, # X-axis: year\n    y = fit   # Y-axis: GLARMA fitted values\n  )) +\n  scale_color_viridis_c() + # Continuous color scale for temperature\n  labs(\n    y = \"Population Count\",\n    x = \"Year\",            \n    color = \"Temperature\" # Legend title\n  ) +\n  theme_bw()"},{"path":"basic-terminology.html","id":"basic-terminology","chapter":"Basic Terminology","heading":"Basic Terminology","text":"Statistics uses specialized terminology, interpretation often intuitive. , list key terms may require clarification, along helpful links explanation.Parametric vs. Non-parametricConfidence intervalCorrelationCovarianceCovariance vs. CorrelationCovariance Matrix","code":""},{"path":"git-github.html","id":"git-github","chapter":"Git & GitHub","heading":"Git & GitHub","text":"section, explain integrate Git GitHub R Studio. R Studio already excellent tool, becomes even powerful combined Git GitHub. Git free open-source distributed version control system tracks changes code work project, ensuring aware modifications made script () files. Tracking changes crucial avoid unintended errors code helps prevent creation redundant files. Although Git primarily local system, online counterpart called GitHub.set system, ’ll need follow steps. first step install Git computer:Windows: Install Git . installation process, ’ll prompted “Adjusting PATH environment.” Choose “Git command line also 3rd-party software” ’s already selected.Windows: Install Git . installation process, ’ll prompted “Adjusting PATH environment.” Choose “Git command line also 3rd-party software” ’s already selected.Mac: Follow instructions provided .Mac: Follow instructions provided .Git installed, open R Studio navigate Create Project > New Directory > New Project. see checkbox labeled “Create git repository,” make sure select creating new project (refer Figure 23.13). action display Git pane upper right panel R Studio, indicating Git integration enabled.\nFigure 23.13: installing Git, see Create git repository.\nunable find options mentioned , follow steps:Click Tools menu bar R Studio.Select Terminal choose New Terminal.terminal window, type git press Enter. command display location Git executable computer.Next, go back Tools menu bar select Global Options.options window, navigate Git/SVN.Look field labeled Git executable specify location Git executable obtained terminal.setting Git, can proceed create account GitHub. ’s free platform, choosing username, consider using lowercase letters including name make easier others find .R Studio seamlessly integrates Git GitHub, using Git client can provide additional visual aids. various options Git clients (see choices ), exercise, use GitHub Desktop. Install GitHub Desktop computer .","code":""},{"path":"git-github.html","id":"commit-push","chapter":"Git & GitHub","heading":"Commit & Push","text":"","code":""},{"path":"git-github.html","id":"register-your-git-repo","chapter":"Git & GitHub","heading":"Register Your Git repo","text":"open R Project ’ve just created git repository, follow steps:Open R Studio.Click File menu bar.Select Open Project.Browse directory created R Project select corresponding .Rproj file.R Studio open project, see project name top-right corner window.create sample .R file named sample.R within project, can use shortcut Ctrl + Shift + N follow steps:Click File menu bar.Select New File.Choose R Script.new script editor open, can write R code.Write code editor save sample.R clicking File selecting Save using shortcut Ctrl + S.Remember save file writing code.open GitHub Desktop app, locate application computer launch . opened, see GUI similar one depicted Figure 23.14.\nFigure 23.14: GUI GitHub Desktop\nClick “Current Repository” button located top left corner GitHub Desktop app interface. , select “Add” followed “Add existing repository” drop menu, shown Figure 23.15.\nFigure 23.15: Add dropdown top left\n","code":"\n## produce 100 random numbers that follows a normal distribution\nx <- rnorm(100, mean = 0, sd = 1)\n\n## estimate mean\nmean(x)\n\n## estimate SD\nsd(x)"},{"path":"git-github.html","id":"commit","chapter":"Git & GitHub","heading":"Commit","text":"GitHub Desktop prompt enter local path Git repository. Browse select directory R Project located. selected directory, local Git repository appear list repositories left side bar GitHub Desktop, shown Figure 23.15.Now repository added GitHub Desktop, ready start committing files Git. Committing process recording changes made files Git. proceed committing, click Git repository GitHub Desktop. see following options information displayed:list changed files: section shows files modified since last commit.summary changes: section provides overview changes made selected files.Commit message: can enter descriptive message explaining changes made commit.providing meaningful commit message, can keep track changes made files easily understand purpose commit.\nFigure 23.16: see Create sample.R summary (required) bottom left\nstage, file (.R file) saved local directory recorded Git change history. commit changes, follow steps:First, select file(s) want commit. can checking checkboxes next files listed “Changes” section GitHub Desktop app.First, select file(s) want commit. can checking checkboxes next files listed “Changes” section GitHub Desktop app.selected files, go bottom left GitHub Desktop app window. see small box says summary (required) Create sample.R.selected files, go bottom left GitHub Desktop app window. see small box says summary (required) Create sample.R.box, enter descriptive title explains commit. important provide meaningful summary, proceed commit unless enter information. example, exercise, can enter Initial commit commit title. subsequent commits, recommended provide informative commit messages accurately describe changes made. can refer online resources recommendations write effective commit titles descriptions.box, enter descriptive title explains commit. important provide meaningful summary, proceed commit unless enter information. example, exercise, can enter Initial commit commit title. subsequent commits, recommended provide informative commit messages accurately describe changes made. can refer online resources recommendations write effective commit titles descriptions.entering commit title, click button says Commit master. action record changes made selected files Git.entering commit title, click button says Commit master. action record changes made selected files Git.Now, changes selected files successfully recorded Git commit. ’s important commit regularly provide meaningful commit messages can track changes understand evolution project needed.","code":""},{"path":"git-github.html","id":"push","chapter":"Git & GitHub","heading":"Push","text":"’s important note changes currently recorded local computer’s Git repository published online GitHub repository. send local changes online GitHub repository, need use “Push” operation GitHub Desktop.make commit GitHub Desktop, prompted dialog box asking want push commit online repository, shown Figure 23.17. first push, won’t corresponding repository GitHub linked local repository. case, GitHub Desktop ask want publish GitHub. Please note “publishing” means making repository available GitHub, remain private unless explicitly choose make public.comfortable changes made want send online repository, click “Push” button GitHub Desktop. action push commits remote repository GitHub, effectively synchronizing local repository online repository.\nFigure 23.17: Push code, hit highlighted menu button\n","code":""},{"path":"git-github.html","id":"edit","chapter":"Git & GitHub","heading":"Edit","text":"created file named sample.R per previous instructions. Now, let’s make minor change content sample.R file. Open sample.R file preferred text editor R Studio, make modification code. example, can update code ::Save changes sample.R file. Now, let’s go back GitHub Desktop see can handle change.making change sample.R file, open GitHub Desktop . notice GitHub Desktop automatically detects difference new old versions file. highlights specific parts script edited, can extremely helpful reviewing comparing changes code.feature provided GitHub Desktop makes easier track modifications understand specific updates made files. helps streamline coding process providing clear visual representation differences different versions code.\nFigure 23.18: Git detects edits codes\nsample repository can found .","code":"\n## produce 100 random numbers that follows a normal distribution\nx <- rnorm(100, mean = 0, sd = 1)\n\n## estimate mean\nmean(x)\n\n## estimate SD\nsd(x)\n## produce 100 random numbers that follows a normal distribution\nx <- rnorm(100, mean = 0, sd = 1)\n\n## estimate mean\nmedian(x)\n\n## estimate SD\nvar(x)"},{"path":"base-plot.html","id":"base-plot","chapter":"Base Plot","heading":"Base Plot","text":"","code":""},{"path":"base-plot.html","id":"overview-1","chapter":"Base Plot","heading":"Overview","text":"R offers variety functions aid visualizing data. graphics package R provides set functions basic graphics (list functions). demonstrate functionality graphics functions, utilize built-iris dataset R.","code":"\nhead(iris)## # A tibble: 6 × 5\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n## 1          5.1         3.5          1.4         0.2 setosa \n## 2          4.9         3            1.4         0.2 setosa \n## 3          4.7         3.2          1.3         0.2 setosa \n## 4          4.6         3.1          1.5         0.2 setosa \n## 5          5           3.6          1.4         0.2 setosa \n## 6          5.4         3.9          1.7         0.4 setosa"},{"path":"base-plot.html","id":"plot","chapter":"Base Plot","heading":"Plot","text":"creating plot, typically need specify formula define relationship variables. instance, wish visualize association x y (y vertical axis x horizontal axis), formula y ~ x (left side formula represents vertical axis). iris dataset, access following columns: Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, Species. subsequent example, plot relationship Sepal.Length Sepal.Width:data = argument informs function dataset variables (Sepal.Length Sepal.Width) extracted.","code":"\nplot(Sepal.Length ~ Sepal.Width, data = iris)"},{"path":"base-plot.html","id":"symbol","chapter":"Base Plot","heading":"Symbol","text":"pch argument. Choose 1 25 (google r plot pch details)","code":"\nplot(Sepal.Length ~ Sepal.Width, data = iris,\n     pch = 19)"},{"path":"base-plot.html","id":"symbol-size","chapter":"Base Plot","heading":"Symbol size","text":"cex argument. cex = 1 default value. cex = 2 twice large default value.","code":"\nplot(Sepal.Length ~ Sepal.Width, data = iris,\n     pch = 19, cex = 2)"},{"path":"base-plot.html","id":"symbol-color-border","chapter":"Base Plot","heading":"Symbol color (border)","text":"col argument (quote \"color name\" specifying). Google r color name color options.","code":"\nplot(Sepal.Length ~ Sepal.Width, data = iris,\n     pch = 21, cex = 2, col = \"gray\")"},{"path":"base-plot.html","id":"symbol-color-fill","chapter":"Base Plot","heading":"Symbol color (fill)","text":"bg argument (quote \"color name\" specifying). Available subset symbol options (symbols pre-defined filled color).","code":"\nplot(Sepal.Length ~ Sepal.Width, data = iris,\n     pch = 21, cex = 2, bg = \"lightgray\")"},{"path":"base-plot.html","id":"label","chapter":"Base Plot","heading":"Label","text":"ylab xlab arguments. Provide \"quoted text\".","code":"\nplot(Sepal.Length ~ Sepal.Width, data = iris,\n     pch = 21, cex = 2, bg = \"lightgray\",\n     xlab = \"Sepal width (cm)\", ylab = \"Sepal length (cm)\")"},{"path":"base-plot.html","id":"axis","chapter":"Base Plot","heading":"Axis","text":"Delete axes axes = F re-draw box() axis() functions.","code":"\nplot(Sepal.Length ~ Sepal.Width, data = iris,\n     pch = 21, cex = 2, bg = \"lightgray\",\n     xlab = \"Sepal width (cm)\", ylab = \"Sepal length (cm)\",\n     axes = F)\nbox(bty = \"l\") # L-shaped border lines\naxis(1) # 1: draw x-axis\naxis(2, las = 2) # 2: draw y-axis, las = 2: make axis lables horizontal"},{"path":"base-plot.html","id":"boxplot-1","chapter":"Base Plot","heading":"Boxplot","text":"boxplot() used x-axis factor-type data (default, plot() produce boxplot x-axis factor variable). iris dataset, column Species factor variable. Compare Sepal.Length among species using boxplot().can customize plot(), slighlty different.","code":"\nboxplot(Sepal.Length ~ Species, data = iris)"},{"path":"base-plot.html","id":"box-color","chapter":"Base Plot","heading":"Box color","text":"col argument.","code":"\nboxplot(Sepal.Length ~ Species, data = iris,\n        col = \"lightgray\")"},{"path":"base-plot.html","id":"border-color","chapter":"Base Plot","heading":"Border color","text":"border argument.","code":"\nboxplot(Sepal.Length ~ Species, data = iris,\n        col = \"lightgray\", border = \"grey48\")"},{"path":"base-plot.html","id":"box-width","chapter":"Base Plot","heading":"Box width","text":"boxwex argument.","code":"\nboxplot(Sepal.Length ~ Species, data = iris,\n        col = \"lightgray\", border = \"grey48\",\n        boxwex = 0.4 )"},{"path":"base-plot.html","id":"axis-1","chapter":"Base Plot","heading":"Axis","text":"Delete axes axes = F re-draw box() axis() functions.","code":"\nboxplot(Sepal.Length ~ Species, data = iris,\n        col = \"lightgray\", border = \"grey48\",\n        boxwex = 0.4, ylab = \"Sepal length (cm)\",\n        axes = F)\nbox(bty = \"l\")\naxis(1, at = c(1, 2, 3), labels = c(\"Setosa\", \"Versicolor\", \"Virginica\") )\naxis(2, las = 2)"}]
