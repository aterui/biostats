<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 9 Likelihood | BIOSTATS</title>
<meta name="author" content="Akira Terui">
<meta name="description" content="In Chapter 8, we introduced non-normal distributions to represent count and proportional data. The glm() function was used to obtain parameter estimates, but how does this function achieve that?...">
<meta name="generator" content="bookdown 0.29 with bs4_book()">
<meta property="og:title" content="Chapter 9 Likelihood | BIOSTATS">
<meta property="og:type" content="book">
<meta property="og:url" content="https://bookdown.org/janedoe/bookdown-demo/likelihood.html">
<meta property="og:description" content="In Chapter 8, we introduced non-normal distributions to represent count and proportional data. The glm() function was used to obtain parameter estimates, but how does this function achieve that?...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 9 Likelihood | BIOSTATS">
<meta name="twitter:description" content="In Chapter 8, we introduced non-normal distributions to represent count and proportional data. The glm() function was used to obtain parameter estimates, but how does this function achieve that?...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">BIOSTATS</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Introduction</a></li>
<li class="book-part">Basics</li>
<li><a class="" href="descriptive-statistics.html"><span class="header-section-number">1</span> Descriptive Statistics</a></li>
<li><a class="" href="sampling.html"><span class="header-section-number">2</span> Sampling</a></li>
<li><a class="" href="probabilistic-view.html"><span class="header-section-number">3</span> Probabilistic View</a></li>
<li><a class="" href="two-group-comparison.html"><span class="header-section-number">4</span> Two-Group Comparison</a></li>
<li><a class="" href="multiple-group-comparison.html"><span class="header-section-number">5</span> Multiple-Group Comparison</a></li>
<li><a class="" href="regression.html"><span class="header-section-number">6</span> Regression</a></li>
<li><a class="" href="linear-model.html"><span class="header-section-number">7</span> Linear Model</a></li>
<li><a class="" href="generalized-linear-model.html"><span class="header-section-number">8</span> Generalized Linear Model</a></li>
<li><a class="active" href="likelihood.html"><span class="header-section-number">9</span> Likelihood</a></li>
<li><a class="" href="model-comparison.html"><span class="header-section-number">10</span> Model Comparison</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="project.html">Project</a></li>
<li><a class="" href="git-github.html">Git &amp; GitHub</a></li>
<li><a class="" href="data-structure.html">Data Structure</a></li>
<li><a class="" href="tidyverse.html">Tidyverse</a></li>
<li><a class="" href="base-plot.html">Base Plot</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/aterui/biostats">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="likelihood" class="section level1" number="9">
<h1>
<span class="header-section-number">9</span> Likelihood<a class="anchor" aria-label="anchor" href="#likelihood"><i class="fas fa-link"></i></a>
</h1>
<p>In Chapter <a href="generalized-linear-model.html#generalized-linear-model">8</a>, we introduced non-normal distributions to represent count and proportional data. The <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> function was used to obtain parameter estimates, but how does this function achieve that? <strong>In reality, the least squares approach is only applicable to normal distributions.</strong> Therefore, we require an alternative approach to estimate parameters within the GLM framework. In this section, I will introduce the concept of likelihood as a fundamental principle for parameter estimation in the GLM framework.</p>
<div id="finding-the-more-likely" class="section level2" number="9.1">
<h2>
<span class="header-section-number">9.1</span> Finding the “More Likely”<a class="anchor" aria-label="anchor" href="#finding-the-more-likely"><i class="fas fa-link"></i></a>
</h2>
<p>In the least squares method, our goal is to minimize the sum of squared errors, ensuring that the fitted model has the smallest deviation from the observed data points. However, this approach is not applicable to non-normal variables. In such cases, we need to rely on the “probability” of observing a data point or numerical value.</p>
<p>Let’s consider an example where we have count data, denoted as <span class="math inline">\(\pmb{y} = \{y_1, y_2, ..., y_N\}\)</span>. To simplify the matter, let’s focus on a specific data point, say <span class="math inline">\(y_1 = 3\)</span>. Assuming a Poisson distribution, we can calculate the probability of observing this particular value by using an arbitrary parameter value for the mean, denoted as <span class="math inline">\(\lambda\)</span>. To refresh our memory, we can refer back to the Poisson distribution discussed in Chapter <a href="probabilistic-view.html#probabilistic-view">3</a>.</p>
<p><span class="math display">\[
\Pr(y = k) = \frac{\lambda^{k}\exp(-\lambda)}{k!}
\]</span></p>
<p>This equation represents the probability of the variable <span class="math inline">\(y\)</span> taking a specific value <span class="math inline">\(k\)</span>. In our current example, we are interested in the case where <span class="math inline">\(k = 3\)</span>. By specifying a value for the mean parameter <span class="math inline">\(\lambda\)</span>, we can compute the probability associated with this value. Let’s try using <span class="math inline">\(\lambda = 3.5\)</span> for this calculation:</p>
<p><span class="math display">\[
\Pr(y_1 = 3) = \frac{3.5^3 \exp(-3.5)}{3!} \approx 0.215
\]</span></p>
<p>We can calculate this with <code><a href="https://rdrr.io/r/stats/Poisson.html">dpois()</a></code> in R:</p>
<div class="sourceCode" id="cb231"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># dpois()</span></span>
<span><span class="co"># the first argument is "k"</span></span>
<span><span class="co"># the second argument is "lambda"</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html">dpois</a></span><span class="op">(</span><span class="fl">3</span>, lambda <span class="op">=</span> <span class="fl">3.5</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0.2157855</code></pre>
<div class="sourceCode" id="cb233"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># write the formula to confirm</span></span>
<span><span class="op">(</span><span class="va">p</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="fl">3.5</span><span class="op">^</span><span class="fl">3</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="fl">3.5</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/Special.html">factorial</a></span><span class="op">(</span><span class="fl">3</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0.2157855</code></pre>
<p>If we assume a mean of <span class="math inline">\(3.5\)</span> in the Poisson distribution, the probability of observing a value of <span class="math inline">\(3\)</span> is approximately <span class="math inline">\(0.215\)</span>. We can experiment with different values of <span class="math inline">\(\lambda\)</span> to see if there is a potentially “better” value. By exploring different values of <span class="math inline">\(\lambda\)</span>, we can observe how the probability of observing a specific value changes. This allows us to assess which value of <span class="math inline">\(\lambda\)</span> provides a better fit or captures the data more accurately. The following example explores <span class="math inline">\(\lambda = 0 - 10\)</span> with a <span class="math inline">\(0.1\)</span> interval.</p>
<div class="sourceCode" id="cb235"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># change lambda from 0 to 10 by 0.1</span></span>
<span><span class="va">lambda</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">10</span>, by <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># probability</span></span>
<span><span class="va">pr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html">dpois</a></span><span class="op">(</span><span class="fl">3</span>, lambda <span class="op">=</span> <span class="va">lambda</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># create a data frame</span></span>
<span><span class="va">df_pois</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span>y <span class="op">=</span> <span class="fl">3</span>,</span>
<span>                  lambda <span class="op">=</span> <span class="va">lambda</span>,</span>
<span>                  pr <span class="op">=</span> <span class="va">pr</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">df_pois</span><span class="op">)</span></span></code></pre></div>
<pre><code>## # A tibble: 101 × 3
##        y lambda       pr
##    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;
##  1     3    0   0       
##  2     3    0.1 0.000151
##  3     3    0.2 0.00109 
##  4     3    0.3 0.00333 
##  5     3    0.4 0.00715 
##  6     3    0.5 0.0126  
##  7     3    0.6 0.0198  
##  8     3    0.7 0.0284  
##  9     3    0.8 0.0383  
## 10     3    0.9 0.0494  
## # ℹ 91 more rows</code></pre>
<p>Make a plot:</p>
<div class="sourceCode" id="cb237"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">df_pois</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">lambda</span>,</span>
<span>             y <span class="op">=</span> <span class="va">pr</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x <span class="op">=</span> <span class="st">"lambda"</span>,</span>
<span>       y <span class="op">=</span> <span class="st">"Pr(k = 3)"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:dpois-plot"></span>
<img src="biostats_files/figure-html/dpois-plot-1.png" alt="Relationship between the parameter $\lambda$ and the probability of observing a value of three." width="672"><p class="caption">
Figure 9.1: Relationship between the parameter <span class="math inline">\(\lambda\)</span> and the probability of observing a value of three.
</p>
</div>
<p>By examining the figure or data, it appears that the probability is highest around <span class="math inline">\(\lambda = 3\)</span>. To confirm this observation, let’s arrange the data frame <code>df_pois</code>.</p>
<div class="sourceCode" id="cb238"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># arrange() re-orders the dataframe based on the speficied column in an ascending order</span></span>
<span><span class="co"># desc() flips the order (descending)</span></span>
<span></span>
<span><span class="va">df_pois</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/arrange.html">arrange</a></span><span class="op">(</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/desc.html">desc</a></span><span class="op">(</span><span class="va">pr</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code>## # A tibble: 101 × 3
##        y lambda    pr
##    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
##  1     3    3   0.224
##  2     3    3.1 0.224
##  3     3    2.9 0.224
##  4     3    3.2 0.223
##  5     3    2.8 0.222
##  6     3    3.3 0.221
##  7     3    2.7 0.220
##  8     3    3.4 0.219
##  9     3    2.6 0.218
## 10     3    3.5 0.216
## # ℹ 91 more rows</code></pre>
<p>That reasoning is sound because the probability of <span class="math inline">\(y_1\)</span> being equal to <span class="math inline">\(3\)</span> is maximized when <span class="math inline">\(\lambda = 3\)</span>. This represents the simplest form of a <strong>likelihood function</strong> denoted as <span class="math inline">\(L(\lambda~|~y_1)\)</span>, where <span class="math inline">\((\cdot~|~y_1)\)</span> indicates that <span class="math inline">\(y_1\)</span> is fixed or given. To evaluate the probability of observing the fixed value of <span class="math inline">\(y_1\)</span>, we vary the value of <span class="math inline">\(\lambda\)</span>.</p>
<p>However, what if we have multiple values of <span class="math inline">\(y_i\)</span>? Let’s consider the scenario where <span class="math inline">\(\pmb{y} = \{3, 2, 5\}\)</span>. In such cases, we must account for the probability of simultaneously observing these values. When the events are independent, the probability of observing multiple events can be expressed as the product of their individual probabilities. Consequently, the likelihood function takes the following form:</p>
<p><span class="math display">\[
\begin{aligned}
L(\lambda~|~\pmb{y}) &amp;= \Pr(y_1 = 3) \times \Pr(y_2 = 2) \times \Pr(y_3 = 5)\\
&amp;= \frac{\lambda^{3}\exp(-\lambda)}{3!} \times \frac{\lambda^{2}\exp(-\lambda)}{2!} \times \frac{\lambda^{5}\exp(-\lambda)}{5!}\\
&amp;= \prod_i^3 \frac{\lambda^{y_i} \exp(-\lambda)}{y_i!}
\end{aligned}
\]</span></p>
<p>Implement this in R:</p>
<div class="sourceCode" id="cb240"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># try lambda = 3 for y = 3, 2, 5</span></span>
<span><span class="va">pr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html">dpois</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">2</span>, <span class="fl">5</span><span class="op">)</span>, lambda <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">pr</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0.2240418 0.2240418 0.1008188</code></pre>
<div class="sourceCode" id="cb242"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># probability of observing 3, 2, 5 simultaneously</span></span>
<span><span class="co"># with lambda = 3</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/prod.html">prod</a></span><span class="op">(</span><span class="va">pr</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0.005060573</code></pre>
<p>Similar to the previous example, let’s search for a more suitable value of <span class="math inline">\(\lambda\)</span> that better captures the observed data.</p>
<div class="sourceCode" id="cb244"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># lambda = 0 - 10 by 0.01</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">2</span>, <span class="fl">5</span><span class="op">)</span></span>
<span><span class="va">lambda</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">10</span>, by <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># sapply repeats the task in FUN</span></span>
<span><span class="co"># each element in "X" will be sequencially substituted in "z"</span></span>
<span><span class="va">pr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span>X <span class="op">=</span> <span class="va">lambda</span>,</span>
<span>             FUN <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">z</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/prod.html">prod</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html">dpois</a></span><span class="op">(</span><span class="va">y</span>, lambda <span class="op">=</span> <span class="va">z</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># make a data frame and arrange by pr (likelihood)</span></span>
<span><span class="va">df_pois</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span>lambda <span class="op">=</span> <span class="va">lambda</span>,</span>
<span>                  pr <span class="op">=</span> <span class="va">pr</span><span class="op">)</span></span>
<span></span>
<span><span class="va">df_pois</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/arrange.html">arrange</a></span><span class="op">(</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/desc.html">desc</a></span><span class="op">(</span><span class="va">pr</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code>## # A tibble: 1,001 × 2
##    lambda      pr
##     &lt;dbl&gt;   &lt;dbl&gt;
##  1   3.33 0.00534
##  2   3.34 0.00534
##  3   3.32 0.00534
##  4   3.35 0.00534
##  5   3.31 0.00534
##  6   3.36 0.00534
##  7   3.3  0.00534
##  8   3.37 0.00534
##  9   3.29 0.00533
## 10   3.38 0.00533
## # ℹ 991 more rows</code></pre>
<div class="sourceCode" id="cb246"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># visualize</span></span>
<span><span class="va">df_pois</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">lambda</span>,</span>
<span>             y <span class="op">=</span> <span class="va">pr</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>y <span class="op">=</span> <span class="st">"Likelihood"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:dpois-likelihood"></span>
<img src="biostats_files/figure-html/dpois-likelihood-1.png" alt="Likelihood of observing three count data points $\pmb{y}$ simultaneously." width="672"><p class="caption">
Figure 9.2: Likelihood of observing three count data points <span class="math inline">\(\pmb{y}\)</span> simultaneously.
</p>
</div>
<p>In this exercise with an interval of <span class="math inline">\(0.01\)</span>, <span class="math inline">\(\lambda \approx 3.33\)</span> is identified as the most suitable value. Interestingly, the sample mean matches this value:</p>
<div class="sourceCode" id="cb247"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">2</span>, <span class="fl">5</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 3.333333</code></pre>
<p>Why? See below.</p>
</div>
<div id="maximum-likelihood-method" class="section level2" number="9.2">
<h2>
<span class="header-section-number">9.2</span> Maximum Likelihood Method<a class="anchor" aria-label="anchor" href="#maximum-likelihood-method"><i class="fas fa-link"></i></a>
</h2>
<div id="simple-case" class="section level3" number="9.2.1">
<h3>
<span class="header-section-number">9.2.1</span> Simple case<a class="anchor" aria-label="anchor" href="#simple-case"><i class="fas fa-link"></i></a>
</h3>
<p>Finding the parameter value that maximizes the probability of observing a set of observed values is known as the <strong>Maximum Likelihood Estimate (MLE)</strong>. This estimation method is commonly employed in the GLM framework and various other statistical inference techniques. The MLE approach is applicable as long as the likelihood can be defined, making it a versatile method for many statistical analyses.</p>
<p>However, the procedure I employed in the previous section (Section <a href="likelihood.html#finding-the-more-likely">9.1</a>) is not mathematically rigorous because the optimal value of <span class="math inline">\(\lambda\)</span> is dependent on the resolution of the interval. As the interval becomes smaller, it is possible to discover an even better value of <span class="math inline">\(\lambda\)</span> ad infinitum due to its continuous nature.</p>
<p>A more reliable approach to identifying the “peak” is by utilizing the first-order derivative. The first-order derivative represents the slope of the likelihood function at a given <span class="math inline">\(\lambda\)</span> value. If the derivative equals zero, it indicates that we have reached the peak of the likelihood function. We can leverage this mathematical principle to determine the maximum likelihood estimate of <span class="math inline">\(\lambda\)</span>. Since computing the derivative of a product is mathematically challenging, we can apply a logarithm transformation to the likelihood function. Recall that <span class="math inline">\(\log ab = \log a + \log b\)</span>. By taking the logarithm, we can convert the product operation <span class="math inline">\(\prod\)</span> into a summation operation <span class="math inline">\(\sum\)</span>. Specifically:</p>
<p><span class="math display">\[
\begin{aligned}
\log L(\lambda | \pmb{y}) &amp;= \log \prod_i^N \frac{\lambda^{y_i} \exp(-\lambda)}{y_i!}\\
&amp;= \log \frac{\lambda^{y_1} \exp(-\lambda)}{y_1!} + \log \frac{\lambda^{y_2} \exp(-\lambda)}{y_2!} + ... + \log \frac{\lambda^{y_N} \exp(-\lambda)}{y_N!}\\
&amp;= \sum_i^N \log \frac{\lambda^{y_i} \exp(-\lambda)}{y_i!}\\
&amp;= \sum_i^N (y_i \log \lambda -\lambda -\log y_i!)
\end{aligned}
\]</span></p>
<p>Taking the first derivative of <span class="math inline">\(\log L(\lambda | \pmb{y})\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial\log L(\lambda|\pmb{y})}{\partial \lambda} &amp;= \sum_i^N (\frac{y_i}{\lambda} -1)\\
&amp;= \frac{\sum_i^N y_i}{\lambda} - \sum_i^N 1\\
&amp;= \frac{\sum_i^N y_i}{\lambda} - N\\
\end{aligned}
\]</span> To obtain the value of <span class="math inline">\(\lambda\)</span> that maximizes the likelihood, we set the derivative equal to zero:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial\log L(\lambda|\pmb{y})}{\partial \lambda} &amp;= \frac{\sum_i^N y_i}{\lambda} - N = 0\\
\lambda &amp;= \frac{\sum_i^N y_i}{N}
\end{aligned}
\]</span></p>
<p>Interesting – the maximum likelihood estimate of <span class="math inline">\(\lambda\)</span> is equal to the sample mean. This is THE reason why we could use the sample mean as the estimate of <span class="math inline">\(\lambda\)</span> in Chapter <a href="probabilistic-view.html#probabilistic-view">3</a>.</p>
</div>
<div id="general-case" class="section level3" number="9.2.2">
<h3>
<span class="header-section-number">9.2.2</span> General case<a class="anchor" aria-label="anchor" href="#general-case"><i class="fas fa-link"></i></a>
</h3>
<p>In practice, obtaining the MLE analytically can be highly complex and often infeasible. Consequently, the GLM framework relies on numerical search algorithms to approximate the MLE estimates (see <a href="https://math.mcmaster.ca/~bolker/emdbook/chap7A.pdf">Ben Bolker’s book chapter</a> for full details; in particular, quasi-Newton methods). Within GLMs, it is common to establish relationships between the mean <span class="math inline">\(\lambda\)</span> and explanatory variables in order to investigate their influences. For instance:</p>
<p><span class="math display">\[
\begin{aligned}
y_i &amp;\sim \mbox{Poisson}(\lambda_i)\\
\log(\lambda_i) &amp;= \alpha + \sum_k \beta_k x_k
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(x_k\)</span> represents <span class="math inline">\(k\)</span>th explanatory variable. This translates into:</p>
<p><span class="math display">\[
\log L(\alpha, \beta_k|\pmb{y}) = \sum_i \left[y_i (\alpha + \sum_k\beta_k x_k) - \exp(\alpha + \sum_k \beta_k x_k) - \log y_i! \right]
\]</span></p>
<p>and solving the <span class="math inline">\(k+1\)</span> partial derivatives:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial \log L(\alpha, \beta_k|\pmb{y})}{\partial \alpha} &amp;= 0\\
\frac{\partial \log L(\alpha, \beta_k|\pmb{y})}{\partial \beta_1} &amp;= 0\\
&amp;...\\
\frac{\partial \log L(\alpha, \beta_k|\pmb{y})}{\partial \beta_k} &amp;= 0\\
\end{aligned}
\]</span>Thus, the function <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> does a lot for us! The log likelihood of the fitted model can be extracted with <code><a href="https://rdrr.io/r/stats/logLik.html">logLik()</a></code>:</p>
<div class="sourceCode" id="cb249"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># load garden plant data</span></span>
<span><span class="co"># df_count &lt;- read_csv("data_raw/data_garden_count.csv")</span></span>
<span></span>
<span><span class="va">m_pois</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">count</span> <span class="op">~</span> <span class="va">nitrate</span>,</span>
<span>              data <span class="op">=</span> <span class="va">df_count</span>,</span>
<span>              family <span class="op">=</span> <span class="st">"poisson"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/logLik.html">logLik</a></span><span class="op">(</span><span class="va">m_pois</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 'log Lik.' -44.92912 (df=2)</code></pre>
</div>
</div>
<div id="laboratory-8" class="section level2" number="9.3">
<h2>
<span class="header-section-number">9.3</span> Laboratory<a class="anchor" aria-label="anchor" href="#laboratory-8"><i class="fas fa-link"></i></a>
</h2>
<div id="binomial-distribution" class="section level3" number="9.3.1">
<h3>
<span class="header-section-number">9.3.1</span> Binomial Distribution<a class="anchor" aria-label="anchor" href="#binomial-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>The binomial distribution is a probability distribution that describes the number of successes in a fixed number of independent Bernoulli trials, where each trial has the same probability of success, denoted by <span class="math inline">\(p\)</span>. The probability mass function (PMF) of a binomial distribution is given by:</p>
<p><span class="math display">\[
\Pr(y = k) = \begin{pmatrix} N\\ k \end{pmatrix} p^k(1 - p)^{N-k}
\]</span></p>
<p>where <span class="math inline">\(\begin{pmatrix} N\\ k \end{pmatrix}\)</span> is the binomial coefficient, representing the number of ways to choose <span class="math inline">\(k\)</span> successes out of <span class="math inline">\(N\)</span> trials.</p>
<ol style="list-style-type: decimal">
<li><p>The function <code><a href="https://rdrr.io/r/stats/Binomial.html">dbinom()</a></code> calculates <span class="math inline">\(\Pr(y = k)\)</span> for a binomial distribution. Using this function, calculate the likelihood of observing the vector <span class="math inline">\(\pmb{y} = \{2, 2, 0, 0, 3, 1, 3, 3, 4, 3\}\)</span> for the following values of <span class="math inline">\(p\)</span>: <span class="math inline">\(p \in \{0, 0.01, 0.02, ..., 1.0\}\)</span>, where <span class="math inline">\(N = 10\)</span>.</p></li>
<li><p>From the set of <span class="math inline">\(p\)</span> examined, find the parameter value that maximizes the likelihood.</p></li>
<li><p>Using the <span class="math inline">\(p\)</span> that maximizes the likelihood, calculate <span class="math inline">\(N \times p\)</span>. Compare this value with the sample mean of the vector <span class="math inline">\(\pmb{y}\)</span>.</p></li>
</ol>
</div>
<div id="normal-distribution-1" class="section level3" number="9.3.2">
<h3>
<span class="header-section-number">9.3.2</span> Normal Distribution<a class="anchor" aria-label="anchor" href="#normal-distribution-1"><i class="fas fa-link"></i></a>
</h3>
<p>The normal distribution is a continuous probability distribution that describes the distribution of a continuous random variable. It is characterized by its mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. The probability density function (PDF) of a normal distribution is given by:</p>
<p><span class="math display">\[
f(y) = \frac{1}{\sqrt{2 \pi} \sigma} \exp\left(-\frac{(y - \mu)^2}{2 \sigma^2}\right)
\]</span></p>
<p>As the normal distribution deals with continuous variables, we need a technique to derive the Maximum Likelihood Estimation (MLE). For this purpose, we will consider <span class="math inline">\(f(y)\)</span> as the height of a rectangle to simplify the problem, and use <span class="math inline">\(\Delta y\)</span> as the base length. The probability of observing a value <span class="math inline">\(y = k\)</span>, denoted as <span class="math inline">\(\Pr(y = k)\)</span>, can then be approximated as the area of this rectangle:</p>
<p><span class="math display">\[
\Pr(y = k) = \text{rectangle area} = f(y) \times \Delta y
\]</span></p>
<p>Assuming we have <span class="math inline">\(N\)</span> observations, the likelihood of observing the entire vector <span class="math inline">\(\pmb{y}\)</span> is given by:</p>
<p><span class="math display">\[
L(\mu, \sigma | \pmb{y}) = \prod_{i}^N f(y_i) \Delta y
\]</span></p>
<ol style="list-style-type: decimal">
<li><p>Yield the logarithm of <span class="math inline">\(L(\mu, \sigma | \pmb{y})\)</span>.</p></li>
<li><p>Derive the first-order derivative of the log-likelihood function with respect to <span class="math inline">\(\mu\)</span> and solve for <span class="math inline">\(\mu\)</span> by setting <span class="math inline">\(\frac{\partial \log L(\mu, \sigma)}{\partial \mu} = 0\)</span>. Assume <span class="math inline">\(\sigma\)</span> is a constant.</p></li>
</ol>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="generalized-linear-model.html"><span class="header-section-number">8</span> Generalized Linear Model</a></div>
<div class="next"><a href="model-comparison.html"><span class="header-section-number">10</span> Model Comparison</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#likelihood"><span class="header-section-number">9</span> Likelihood</a></li>
<li><a class="nav-link" href="#finding-the-more-likely"><span class="header-section-number">9.1</span> Finding the “More Likely”</a></li>
<li>
<a class="nav-link" href="#maximum-likelihood-method"><span class="header-section-number">9.2</span> Maximum Likelihood Method</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#simple-case"><span class="header-section-number">9.2.1</span> Simple case</a></li>
<li><a class="nav-link" href="#general-case"><span class="header-section-number">9.2.2</span> General case</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#laboratory-8"><span class="header-section-number">9.3</span> Laboratory</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#binomial-distribution"><span class="header-section-number">9.3.1</span> Binomial Distribution</a></li>
<li><a class="nav-link" href="#normal-distribution-1"><span class="header-section-number">9.3.2</span> Normal Distribution</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/aterui/biostats/blob/master/chapters/09-likelihood.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/aterui/biostats/edit/master/chapters/09-likelihood.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>BIOSTATS</strong>" was written by Akira Terui. It was last built on 2023-09-28.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
