# Linear model

Many of you might hate equation -- I hated too because no one told me how to read it. However, you will like it once you understand how to interpret; it is actually the clearest explanation. In this section, I will continue to use the example of fish counts in a lake to explain the use of linear models.

## Mean estimate

### Normal distribution

In the example of fish counts, we estimated mean and variance to infer how fish counts distribute over a range of possible values (probability distribution). Implicitly, I assumed that the "true" mean -- i.e., the expected number of fish counts in a plot -- is uniform within the lake. Let's express this assumption as a "model." Let $y_i$ be fish count in plot $i$:

$$
y_i = \mu + \varepsilon_i
$$

The above equation reads fish count $y_i$ can be expressed as the sum of mean density $\mu$ and residual $\varepsilon_i$ (raw deviation from the mean; $\varepsilon_i = y_i - \mu$). Notice that the parameter $\mu$ does not have subscript $i$ (i.e., $\mu$ is constant across plots) since we assume that the mean fish count is uniform across plots.

We can build this model pretty easily in R using function `lm()` (Linear Model, LM) with caution that this function assumes a Normal distribution, which we identified as an inappropriate distribution for our fish data. However, let me use this function for the sake of simplicity. In the function `lm()`, we write a **response** variable (i.e., the variable we want to explain = fish count) on the left side of tilde `~`, and **explanatory** variables (or predictors) on the right. In our simplest model, we do not have explanatory variables, so put `1` on the right.

```{r lm, echo = T}

fit <- lm(count ~ 1, data = df_count)
summary(fit)
```

This is perhaps the simplest model we can make -- the variation in fish count $y_i$ can be expressed as the mean $\mu$ plus random variation $\varepsilon_i$. However, where is the mean in the output? In the model output, the estimated mean is shown in the column `Estimate` under the area `Coefficients:`. We can prove it by comparing the estimate and the mean calculated with function `mean()`.

> **Exercise:** calculate mean fish count with `mean()`

Now we see how this model works. however, where is $\varepsilon_i$ then? You can get values of $\varepsilon_i$ using function `resid()`.

```{r resid, echo=TRUE}

eps <- resid(fit)
print(eps)
```

It is unclear how this is derived, so let's compare it with $y_i - \mu$. Note that SD of the residual ($\text{SD} = \sqrt{\text{Variance}}$) is the degree of variation in the data, and the value `Residual standard error:` should match `eps`'s SD.

> **Exercise:** calculate $y_i - \mu$ and compare it with `eps`. Also, calculate the SD of `eps` and compare it with the reported value `Residual standard error:` in the `lm()` output.

The model can be written in many ways. It varies by person, but I strongly encourage you to know the following expression of a statistical model:

$$
y_i \sim \text{Normal}(\mu, \sigma^2)
$$

The interpretation of this equation is that $y_i$ follows a normal distribution with a mean $\mu$ and variance $\sigma^2$. When the left and right hands of the equation are related with tilde, it indicates the relationship is stochastic -- the outcome $y_i$ always deviates from the expectation $\mu$, unlike equal sign $y_i = \mu$. This expression is identical to the following:

$$
y_i = \mu + \varepsilon_i\\
\varepsilon_i \sim \text{Normal}(0, \sigma^2)
$$

This expression clarifies that the deviation $\varepsilon_i$ is stochastic, but the expectation $\mu$ is deterministic. This is the basic structure of statistical model -- **observation is a mixture of deterministic and stochastic components**.

### Poisson distribution

In the previous section, we have identified that fish count is better described by a Poisson distribution. How can fit a model with a Poisson distribution? R has functionality to perform this type of analysis, and Generalized Linear Model (GLM) (`glm()`) is the natural extension of `lm()` function for non-normal distributions. It's pretty easy to use:

```{r glm, echo = TRUE}

fit_pois <- glm(count ~ 1, data = df_count, family = "poisson")
```

The only difference is that we used `glm()` and specified a Poisson distribution as `family = "poisson"`. Let's see what's in there.

```{r glm-summary, echo = TRUE}
summary(fit_pois)
```

Wait...it says `Estimate` `r round(fit_pois$coefficients, 2)`, which is quite different from what we see in our fish count data (mean = `r round(mean(df_count$count), 2)`). What's going on? This is because there is internal variable transformation when using a Poisson distribution. Since a Poisson distribution deals with positive integer (including zero), its mean cannot be negative. To avoid this potential problem in some data sets, the function `glm()` estimates the mean after log-transformation, and then back-transform to an ordinary scale. The reported value is in a natural log-scale -- we can retrieve the mean estimate in an ordinary scale by transforming the estimate by exponent `exp()` .

```{r exp-poisson, echo = T}

# get intercept value in a log-scale
fit_pois$coefficients

# transform to an ordinary scale
exp(fit_pois$coefficients)
```

## Effect estimate

### 
