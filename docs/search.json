[{"path":"index.html","id":"introduction","chapter":"Introduction","heading":"Introduction","text":"textbook designed provide comprehensive introduction fundamental statistical techniques practical applications context biological data analysis. sets book apart “flipped-order” approach teaching. Unlike traditional statistics courses often begin theoretical concepts, book takes different approach initially presenting real-world data. grounding material practical scenarios, aims make learning process accessible, particularly new statistics.“flipped-order” introduction, readers able engage concrete examples statistical methods use delving underlying theories concepts support . pedagogical approach aims provide solid foundation readers comprehend apply statistical techniques effectively biological sciences.","code":""},{"path":"descriptive-statistics.html","id":"descriptive-statistics","chapter":"1 Descriptive Statistics","heading":"1 Descriptive Statistics","text":"Descriptive statistics set summary measures provide concise overview dataset. help us understand characteristics properties data without delving complex statistical analyses. commonly used descriptive statistics include mean, standard deviation, median.illustrate concept, let’s consider fish length measurement \\(x\\). fish identified subscript, lengths denoted follows:\\[\nx_1 = 15.9\\\\\nx_2 = 15.1\\\\\nx_3 = 21.9\\\\\nx_4 = 13.3\\\\\nx_5 = 24.4\\\\\n\\] Often times, use subscript \\(\\) (character like) instead actual number indicate given data point. example, write fish length \\(x_i\\) individual \\(\\). Alternatively, instead writing individual data point, can represent vector using boldface, denoted \\(\\pmb{x}\\). case, vector \\(\\pmb{x}\\) can expressed :\\[\n\\pmb{x} = \\{15.9, 15.1, 21.9, 13.3, 24.4\\}\n\\]Now let’s see represent summary statistics vector \\(\\pmb{x}\\).","code":""},{"path":"descriptive-statistics.html","id":"central-tendency","chapter":"1 Descriptive Statistics","heading":"1.1 Central Tendency","text":"","code":""},{"path":"descriptive-statistics.html","id":"central-tendency-measures","chapter":"1 Descriptive Statistics","heading":"1.1.1 Central Tendency Measures","text":"Central tendency measures (Table 1.1) provide insights typical central value dataset. three commonly used measures:Arithmetic Mean. commonly used measure central tendency. represents additive average data. calculate arithmetic mean, sum values divide total number data points. can heavily influenced extreme values, known outliers.Arithmetic Mean. commonly used measure central tendency. represents additive average data. calculate arithmetic mean, sum values divide total number data points. can heavily influenced extreme values, known outliers.Geometric Mean. geometric mean multiplicative average. always smaller arithmetic mean less sensitive unusually large values. However, applicable data contain negative values.Geometric Mean. geometric mean multiplicative average. always smaller arithmetic mean less sensitive unusually large values. However, applicable data contain negative values.Median. median value separates higher half lower half dataset. represents 50th percentile data point. median less affected outliers compared arithmetic mean. calculate median, arrange data ascending order select middle value dataset odd number values. dataset even number values, take average two middle values.Median. median value separates higher half lower half dataset. represents 50th percentile data point. median less affected outliers compared arithmetic mean. calculate median, arrange data ascending order select middle value dataset odd number values. dataset even number values, take average two middle values.\nTable 1.1: Common measures central tendency. \\(N\\) refers number data points.\n\\(x_{(\\frac{N + 1}{2}^\\text{th})}~\\text{N odd}\\)","code":""},{"path":"descriptive-statistics.html","id":"r-exercise","chapter":"1 Descriptive Statistics","heading":"1.1.2 R Exercise","text":"learn measures, let’s create vectors \\(\\pmb{x} = \\{15.9, 15.1, 21.9, 13.3, 24.4\\}\\) \\(\\pmb{y} = \\{15.9, 15.1, 21.9, 53.3, 24.4\\}\\) – \\(\\pmb{y}\\) identical \\(\\pmb{x}\\) contains one outlier value. make difference? construct vectors R, use c(), function stands “construct.” script:Confirm constructed correctly:Cool! Now can calculate summary statistics x y.Arithmetic MeanWhile R function arithmetic mean(), let’s calculate value scratch:Compare outputs mean() :Geometric MeanUnfortunately, build-function geometric mean \\(\\mu_{ge}\\) R (far know; packages though). , can calculate value scratch :MedianLastly, let’s median:Compare outputs median()","code":"\n# construct vectors x and y\nx <- c(15.9, 15.1, 21.9, 13.3, 24.4)\ny <- c(15.9, 15.1, 21.9, 53.3, 24.4)\nx## [1] 15.9 15.1 21.9 13.3 24.4\ny## [1] 15.9 15.1 21.9 53.3 24.4\n# for vector x\nn_x <- length(x) # the number of elements in x = the number of data points\nsum_x <- sum(x) # summation for x\nmu_x <- sum_x / n_x # arithmetic mean\nprint(mu_x) # print calculated value## [1] 18.12\n# for vector y; we can calculate directly too\nmu_y <- sum(y) / length(y)\nprint(mu_y) # print calculated value## [1] 26.12\nprint(mean(x))## [1] 18.12\nprint(mean(y))## [1] 26.12\n# for vector x\nprod_x <- prod(x) # product of vector x; x1 * x2 * x3...\nn_x <- length(x)\nmug_x <- prod_x^(1 / n_x) # ^ means power\nprint(mug_x)## [1] 17.63648\n# for vector y\nmug_y <- prod(y)^(1 / length(y))\nprint(mug_y)## [1] 23.28022\n# for vector x\nx <- sort(x) # sort x from small to large\nindex <- (length(x) + 1) / 2 # (N + 1)/2 th index as length(x) is an odd number\nmed_x <- x[index]\nprint(med_x)## [1] 15.9\n# for vector y\ny <- sort(y) # sort y from small to large\nmed_y <- y[(length(y) + 1) / 2]\nprint(med_y)## [1] 21.9\nprint(median(x))## [1] 15.9\nprint(median(y))## [1] 21.9"},{"path":"descriptive-statistics.html","id":"variation","chapter":"1 Descriptive Statistics","heading":"1.2 Variation","text":"","code":""},{"path":"descriptive-statistics.html","id":"variation-measures","chapter":"1 Descriptive Statistics","heading":"1.2.1 Variation Measures","text":"Variation measures (Table 1.2) provide information spread data points.Variance. Variance statistical measure quantifies spread dispersion dataset. provides numerical value indicates far individual data points dataset deviate mean average value. words, variance measures average squared difference data point mean. Standard deviation (SD) square root variance.Variance. Variance statistical measure quantifies spread dispersion dataset. provides numerical value indicates far individual data points dataset deviate mean average value. words, variance measures average squared difference data point mean. Standard deviation (SD) square root variance.Inter-Quantile Range. interquartile range (IQR) statistical measure provides information spread dispersion dataset, specifically focusing middle 50% data. robust measure variability less affected outliers compared variance.Inter-Quantile Range. interquartile range (IQR) statistical measure provides information spread dispersion dataset, specifically focusing middle 50% data. robust measure variability less affected outliers compared variance.Median Absolute Deviation. Median Absolute Deviation (MAD) similar variance, provides robust estimation variability less affected outliers compared variance. MAD defined median absolute deviations data’s median.Median Absolute Deviation. Median Absolute Deviation (MAD) similar variance, provides robust estimation variability less affected outliers compared variance. MAD defined median absolute deviations data’s median.Coefficient Variation. coefficient variation (CV) statistical measure expresses relative variability dataset relation mean. particularly useful comparing variability different datasets different scales units measurement.Coefficient Variation. coefficient variation (CV) statistical measure expresses relative variability dataset relation mean. particularly useful comparing variability different datasets different scales units measurement.MAD/Median. MAD/Median statistical measure used assess relative variability dataset without assuming specific distribution parametric model. CV sensitive outliers reliance arithmetic mean. However, MAD/Median robust issue.MAD/Median. MAD/Median statistical measure used assess relative variability dataset without assuming specific distribution parametric model. CV sensitive outliers reliance arithmetic mean. However, MAD/Median robust issue.\nTable 1.2: Common measures variation. \\(N\\) refers number data points. \\(x_l\\) \\(x_h\\) \\(l^{th}\\) \\(h^{th}\\) percentiles.\n","code":""},{"path":"descriptive-statistics.html","id":"r-exercise-1","chapter":"1 Descriptive Statistics","heading":"1.2.2 R Exercise","text":"VarianceStandard Deviation (SD)Coefficient Variation (CV)IQR; , let use 25 75 percentiles \\(x_l\\) \\(x_h\\).MADMAD / Median","code":"\n# for x\nsqd_x <- (x - mean(x))^2 # sqared deviance\nsum_sqd_x <- sum(sqd_x)\nvar_x <- sum_sqd_x / length(x)\nprint(var_x)## [1] 18.2016\n# for y\nvar_y <- sum((y - mean(y))^2) / length(y)\nprint(var_y)## [1] 197.0816\n# for x\nsd_x <- sqrt(var_x) # sqrt(): square root\nprint(sd_x)## [1] 4.266333\n# for y\nsd_y <- sqrt(var_y)\nprint(sd_y)## [1] 14.03858\n# for x\ncv_x <- sd_x / mean(x)\nprint(cv_x)## [1] 0.2354489\n# for y\ncv_y <- sd_y / mean(y)\nprint(cv_y)## [1] 0.5374646\n# for x\nx_l <- quantile(x, 0.25) # quantile(): return quantile values, 25 percentile\nx_h <- quantile(x, 0.75) # quantile(): return quantile values, 75 percentile\niqr_x <- abs(x_l - x_h) # abs(): absolute value\nprint(iqr_x)## 25% \n## 6.8\n# for y\ny_q <- quantile(y, c(0.25, 0.75)) # return as a vector\niqr_y <- abs(y_q[1] - y_q[2]) # y_q[1] = 25 percentile; y_q[2] = 75 percentile\nprint(iqr_y)## 25% \n## 8.5\n# for x\nad_x <- abs(x - mean(x))\nmad_x <- median(ad_x)\nprint(mad_x)## [1] 3.78\n# for y\nmad_y <- median(abs(y - mean(y)))\nprint(mad_y)## [1] 10.22\n# for x\nmm_x <- mad_x / median(x)\nprint(mm_x)## [1] 0.2377358\n# for y\nmm_y <- mad_y / median(y)\nprint(mm_y)## [1] 0.4666667"},{"path":"descriptive-statistics.html","id":"laboratory","chapter":"1 Descriptive Statistics","heading":"1.3 Laboratory","text":"","code":""},{"path":"descriptive-statistics.html","id":"comparing-central-tendency-measures","chapter":"1 Descriptive Statistics","heading":"1.3.1 Comparing Central Tendency Measures","text":"differences three measures central tendency? investigate , let’s perform following exercise.Create new vector z length \\(1000\\) exp(rnorm(n = 1000, mean = 0, sd = 0.1)), calculate arithmetic mean, geometric mean, median.Create new vector z length \\(1000\\) exp(rnorm(n = 1000, mean = 0, sd = 0.1)), calculate arithmetic mean, geometric mean, median.Draw histogram z using functions tibble(), ggplot(), geom_histogram().Draw histogram z using functions tibble(), ggplot(), geom_histogram().Draw vertical lines arithmetic mean, geometric mean, median histogram different colors using function geom_vline() .Draw vertical lines arithmetic mean, geometric mean, median histogram different colors using function geom_vline() .Compare values central tendency measures.Compare values central tendency measures.Create new vector z_rev -z + max(z) + 0.1, repeat step 1 – 4.Create new vector z_rev -z + max(z) + 0.1, repeat step 1 – 4.","code":""},{"path":"descriptive-statistics.html","id":"comparing-variation-measures","chapter":"1 Descriptive Statistics","heading":"1.3.2 Comparing Variation Measures","text":"absolute (variance, SD, MAD, IQR) relative measures (CV, MAD/Median) variation? understand , suppose 100 measurements fish weight unit “gram.” (w following script)Using data, perform following exercise:Convert unit w “milligram” create new vector m.Convert unit w “milligram” create new vector m.Calculate SD MAD w m.Calculate SD MAD w m.Calculate CV MAD/Median w m.Calculate CV MAD/Median w m.","code":"\nw <- rnorm(100, mean = 10, sd = 1)\nhead(w) # show first 10 elements in w## [1]  9.237455  9.171337 10.834474  9.032348  9.971185 10.232525"},{"path":"sampling.html","id":"sampling","chapter":"2 Sampling","heading":"2 Sampling","text":"“need statistics first place?” initial question arose entered field ecology. Initially, assumed straightforward query immediate response. However, soon realized profound question complex answer. short, “need statistics often possess partial information seek understand.” Now, let’s explore elaborate explanation .Key words: parameter, sample mean, sample variance, biased/unbiased","code":""},{"path":"sampling.html","id":"the-unknown-garden-plant-example","chapter":"2 Sampling","heading":"2.1 The Unknown: Garden Plant Example","text":"Consider scenario conducting study plant height garden. garden, exists thousand individual plants, making impractical single researcher measure . Instead, due resource limitations, sample \\(10\\) plants selected calculate average height extent variation among plant individuals:Cool. Let’s use data set learn pitfall behind . Create vector plant height h put tibble() analyze :format (tibble()) better raw vector height allows flexible analysis. Let’s add columns mu_height var_height using mutate(), function adds new column(s) existing tibble() (data.frame()):Awesome, able get average height variance! – however, confident ? obtained plant height 10…1000. different measure another set 10 plant individuals? Let’s see:Create another tibble() :Wow, ’s totally different.","code":"## # A tibble: 10 × 4\n##     ...1 plant_id height unit \n##    <dbl>    <dbl>  <dbl> <chr>\n##  1     1        1   16.9 cm   \n##  2     2        2   20.9 cm   \n##  3     3        3   15.8 cm   \n##  4     4        4   28   cm   \n##  5     5        5   21.6 cm   \n##  6     6        6   15.9 cm   \n##  7     7        7   22.4 cm   \n##  8     8        8   23.7 cm   \n##  9     9        9   22.9 cm   \n## 10    10       10   18.5 cm\nh <- c(16.9, 20.9, 15.8, 28, 21.6, 15.9, 22.4, 23.7, 22.9, 18.5)\n\ndf_h1 <- tibble(plant_id = 1:10, # a vector from 1 to 10 by 1\n                height = h, # height\n                unit = \"cm\") # unit\n# nrow() returns the number of rows\n# while piping, \".\" refers to the dataframe inherited \n# i.e., nrow(.) counts the number of rows in df_h1\ndf_h1 <- df_h1 %>% \n  mutate(mu_height = mean(height),\n         var_height = sum((height - mu_height)^2) / nrow(.))\n\nprint(df_h1)## # A tibble: 10 × 5\n##    plant_id height unit  mu_height var_height\n##       <int>  <dbl> <chr>     <dbl>      <dbl>\n##  1        1   16.9 cm         20.7       13.7\n##  2        2   20.9 cm         20.7       13.7\n##  3        3   15.8 cm         20.7       13.7\n##  4        4   28   cm         20.7       13.7\n##  5        5   21.6 cm         20.7       13.7\n##  6        6   15.9 cm         20.7       13.7\n##  7        7   22.4 cm         20.7       13.7\n##  8        8   23.7 cm         20.7       13.7\n##  9        9   22.9 cm         20.7       13.7\n## 10       10   18.5 cm         20.7       13.7## # A tibble: 10 × 4\n##     ...1 plant_id height unit \n##    <dbl>    <dbl>  <dbl> <chr>\n##  1    11       11   27.6 cm   \n##  2    12       12   21.9 cm   \n##  3    13       13   16.9 cm   \n##  4    14       14    8.9 cm   \n##  5    15       15   25.6 cm   \n##  6    16       16   19.8 cm   \n##  7    17       17   19.9 cm   \n##  8    18       18   24.7 cm   \n##  9    19       19   24.1 cm   \n## 10    20       20   23   cm\nh <- c(27.6, 21.9, 16.9, 8.9, 25.6, 19.8, 19.9, 24.7, 24.1, 23)\n\ndf_h2 <- tibble(plant_id = 11:20, # a vector from 11 to 20 by 1\n                height = h,\n                unit = \"cm\") %>% \n  mutate(mu_height = mean(height),\n         var_height = sum((height - mu_height)^2) / nrow(.))\n\nprint(df_h2)## # A tibble: 10 × 5\n##    plant_id height unit  mu_height var_height\n##       <int>  <dbl> <chr>     <dbl>      <dbl>\n##  1       11   27.6 cm         21.2       25.8\n##  2       12   21.9 cm         21.2       25.8\n##  3       13   16.9 cm         21.2       25.8\n##  4       14    8.9 cm         21.2       25.8\n##  5       15   25.6 cm         21.2       25.8\n##  6       16   19.8 cm         21.2       25.8\n##  7       17   19.9 cm         21.2       25.8\n##  8       18   24.7 cm         21.2       25.8\n##  9       19   24.1 cm         21.2       25.8\n## 10       20   23   cm         21.2       25.8"},{"path":"sampling.html","id":"linking-part-to-the-whole","chapter":"2 Sampling","heading":"2.2 Linking Part to the Whole","text":"exercise highlights important takeaway: can determine data average variance sample, may perfectly represent characteristics entire garden.field biological research, often impractical impossible sample entire population, must rely estimating unknowns (case, mean variance) available samples. statistics comes play, offering tool infer information entire population based partial information obtained samples.unknowns interested , population mean variance example, referred “parameters.” parameters directly measured can estimated samples statistical inference.Provided certain assumptions met, sample mean unbiased point estimate population mean. “unbiased” means sample means – repeat sampling process – centered around population mean. meantime, sample variance – use formula Chapter 1 – “biased.” tends smaller population variance.Let’s explore concept simple simulations. Suppose data thousand plant individuals, although scenario may unrealistic practice. However, conducting simulations, can examine different sample means variances can deviate true values.Download data containing height measurements thousand individuals, place file data_raw/ project directory. can load csv file R follows:Using synthetic dataset (generated random value generator), can calculate true mean variance (reference values). important note case, use term “calculate” mean variance represent parameters entire population, known us scenario.can simulate sampling 10 plant individuals randomly selecting 10 rows df_h0:Since sample_n() selects rows randomly, (likely) get different set 10 individuals/rows every single time. another set 10 rows (notice df_i overwritten new data set):Let’s obtain 100 sets 10 plant individuals (randomly selected) estimate mean variance . can perform random sampling one one, cumbersome – least, want . Instead, can leverage technique loop:Take look mu_i var_i :element mu_i var_i, saved estimated mean (\\(\\hat{\\mu}\\); reads mu hat) variance (\\(\\hat{\\sigma}^2\\)) 10 plant height measures dataset . drawing histogram values, can examine distributions mean variance estimates. use R package patchwork make better figure:\nFigure 2.1: Sample mean biased variance.\nsample means indeed symmetrically distributed around true mean, sample variances tend biased skewed right, often underestimating true variance.bias estimating variance arises due inferring parameter small number samples. However, good news: unbiased estimator variance exists. formula unbiased estimator variance follows:\\[\n\\frac{\\sum_i^N (x_i - \\mu)^2}{N-1}\n\\]correction denominator (\\(N\\) replaced \\(N-1\\)) compensates bias, providing estimate true variance without systematic underestimation (although seems simple correction, deep math underlies derivation \\(N-1\\)). default formula var() R, function used estimate unbiased variance (unbiased SD sd()). Comparison reveals works:Add histogram unbiased variance:\nFigure 2.2: Comparison biased unbiased variances.\nsummary, samples can provide information part whole population. complete picture entire population often unknown, rely estimating key parameters available samples. concept applies wide range parametric analyses statistics, use sample data make inferences population parameters.Recognizing limitations uncertainties associated working samples essential proper statistical analysis interpretation results various fields study.","code":"\n# load csv data on R\ndf_h0 <- read_csv(\"data_raw/data_plant_height.csv\")\n\n# show the first 10 rows\nprint(df_h0)## # A tibble: 1,000 × 4\n##     ...1 plant_id height unit \n##    <dbl>    <dbl>  <dbl> <chr>\n##  1     1        1   16.9 cm   \n##  2     2        2   20.9 cm   \n##  3     3        3   15.8 cm   \n##  4     4        4   28   cm   \n##  5     5        5   21.6 cm   \n##  6     6        6   15.9 cm   \n##  7     7        7   22.4 cm   \n##  8     8        8   23.7 cm   \n##  9     9        9   22.9 cm   \n## 10    10       10   18.5 cm   \n## # ℹ 990 more rows\nmu <- mean(df_h0$height)\nsigma2 <- sum((df_h0$height - mu)^2) / nrow(df_h0)\n\nprint(mu)## [1] 19.9426\nprint(sigma2)## [1] 26.74083\ndf_i <- df_h0 %>% \n  sample_n(size = 10) # size specifies the number of rows to be selected randomly\n\nprint(df_i)## # A tibble: 10 × 4\n##     ...1 plant_id height unit \n##    <dbl>    <dbl>  <dbl> <chr>\n##  1   308      308   22.8 cm   \n##  2   461      461   17.7 cm   \n##  3   602      602   27.5 cm   \n##  4   181      181   13.8 cm   \n##  5   488      488   14.8 cm   \n##  6   565      565   17   cm   \n##  7   302      302   14.8 cm   \n##  8    96       96   22.8 cm   \n##  9   853      853   14.8 cm   \n## 10   987      987   15.9 cm\ndf_i <- df_h0 %>% \n  sample_n(size = 10)\n\nprint(df_i)## # A tibble: 10 × 4\n##     ...1 plant_id height unit \n##    <dbl>    <dbl>  <dbl> <chr>\n##  1   962      962   19.9 cm   \n##  2   618      618   11   cm   \n##  3   614      614   13   cm   \n##  4   735      735   22.1 cm   \n##  5   215      215   24.9 cm   \n##  6    32       32   19.5 cm   \n##  7   230      230   21.3 cm   \n##  8   367      367   23.4 cm   \n##  9   270      270   15.3 cm   \n## 10   524      524   21   cm\n# for reproducibility\nset.seed(3)\n\nmu_i <- var_i <- NULL # create empty objects\n\n# repeat the work in {} from i = 1 to i = 100\nfor (i in 1:100) {\n  \n  df_i <- df_h0 %>% \n    sample_n(size = 10) # random samples of 10 individuals\n  \n  # save mean for sample set i\n  mu_i[i] <- mean(df_i$height)\n  \n  # save variance for sample set i\n  var_i[i] <- sum((df_i$height - mean(df_i$height))^2) / nrow(df_i) \n  \n}\nprint(mu_i)##   [1] 19.97 17.86 22.55 22.03 17.00 23.28 21.33 21.23 18.55 19.29 22.14 19.84\n##  [13] 22.43 22.13 21.13 19.40 19.39 20.43 19.12 20.66 21.01 19.48 21.73 19.63\n##  [25] 21.03 20.60 21.11 20.42 18.76 23.70 20.31 22.22 21.34 20.70 20.96 20.03\n##  [37] 21.77 19.19 19.87 21.38 19.64 23.31 19.89 19.21 19.68 19.54 17.54 19.05\n##  [49] 18.91 20.57 18.33 18.07 19.48 17.70 20.24 17.74 20.45 16.48 18.93 17.60\n##  [61] 17.23 20.75 18.06 20.06 20.80 21.72 19.02 25.08 18.90 20.69 23.28 20.87\n##  [73] 18.65 19.74 21.47 17.95 16.98 18.30 19.77 17.25 19.60 21.27 19.28 20.42\n##  [85] 19.60 18.41 20.15 21.24 19.70 21.56 20.75 19.54 17.54 18.52 19.85 18.40\n##  [97] 20.39 17.07 17.84 20.66\nprint(var_i)##   [1] 14.1961 21.6884 28.2685 21.6341  6.5700 24.5996 23.6941 24.8681 21.6485\n##  [10] 36.3689 17.8844 31.5784 21.0521 34.2341 20.6281 10.4420 20.0469 31.2781\n##  [19] 32.0316 23.0964  9.8789 31.8916 28.3061 19.6861  8.2641 23.3640 26.8769\n##  [28] 48.9416 27.7644 42.4220 38.4989 12.7076 15.9004  8.6740 11.7284 26.2061\n##  [37] 21.7461 25.9269 28.0421  6.2616 16.5584 27.7489 17.3609 23.1349 38.6936\n##  [46] 25.0984 13.5864 25.2625  7.7149 17.5341 11.4681 38.1561 12.6376 42.5480\n##  [55] 31.9224 13.9824 14.9725 25.8296 33.6781  8.2440 31.4741 29.7805 26.7324\n##  [64] 47.3704 26.8660 24.0536 29.5076 23.7376 14.8000 51.8449 13.3536 18.5581\n##  [73] 18.9185 12.4544 36.2201 16.3225 17.9236 42.8760 18.5421 22.0205 34.1680\n##  [82] 28.8841 26.6976 22.7356 35.2900 19.2229 34.3905 17.3784 23.1880 42.4264\n##  [91] 18.5365 18.5424 21.5204 13.1276 17.7185 21.2940 27.1649 35.3141 43.5624\n## [100] 55.1484\n#install.packages(\"patchwork\") # install only once\nlibrary(patchwork)\n\ndf_sample <- tibble(mu_hat = mu_i, var_hat = var_i)\n\n# histogram for mean\ng_mu <- df_sample %>% \n  ggplot(aes(x = mu_hat)) +\n  geom_histogram() +\n  geom_vline(xintercept = mu)\n\n# histogram for variance\ng_var <- df_sample %>% \n  ggplot(aes(x = var_hat)) +\n  geom_histogram() +\n  geom_vline(xintercept = sigma2)\n\n# layout vertically\n# possible only if \"patchwork\" is loaded\ng_mu / g_var\n# for reproducibility\nset.seed(3)\n\n# redo simulations ----\nmu_i <- var_i <- var_ub_i <- NULL # create empty objects\n\n# repeat the work in {} from i = 1 to i = 100\nfor (i in 1:100) {\n  \n  df_i <- df_h0 %>% \n    sample_n(size = 10) # random samples of 10 individuals\n  \n  # save mean for sample set i\n  mu_i[i] <- mean(df_i$height)\n  \n  # save variance for sample set i\n  var_i[i] <- sum((df_i$height - mean(df_i$height))^2) / nrow(df_i) \n  \n  var_ub_i[i] <- var(df_i$height)\n}\n# draw histograms ----\ndf_sample <- tibble(mu_hat = mu_i,\n                    var_hat = var_i,\n                    var_ub_hat = var_ub_i)\n\n# histogram for mu\ng_mu <- df_sample %>% \n  ggplot(aes(x = mu_hat)) +\n  geom_histogram() +\n  geom_vline(xintercept = mu)\n\n# histogram for variance\n# scale_x_continuous() adjusts scale in x-axis\ng_var <- df_sample %>% \n  ggplot(aes(x = var_hat)) +\n  geom_histogram() +\n  geom_vline(xintercept = sigma2) +\n  scale_x_continuous(limits= c(min(c(var_i, var_ub_i)),\n                               max(c(var_i, var_ub_i))))\n\n# histogram for unbiased variance\ng_var_ub <- df_sample %>% \n  ggplot(aes(x = var_ub_hat)) +\n  geom_histogram() +\n  geom_vline(xintercept = sigma2) +\n  scale_x_continuous(limits= c(min(c(var_i, var_ub_i)),\n                               max(c(var_i, var_ub_i))))\n\ng_mu / g_var / g_var_ub"},{"path":"sampling.html","id":"laboratory-1","chapter":"2 Sampling","heading":"2.3 Laboratory","text":"used 10 plants estimate sample means variances. Obtain 100 sub-datasets 50 100 measures , draw histograms sample means unbiased variances (use var()).used 10 plants estimate sample means variances. Obtain 100 sub-datasets 50 100 measures , draw histograms sample means unbiased variances (use var()).Sample means unbiased variances unbiased samples randomly selected. happens samples non-random? Suppose investigator unable find plants less 10 cm height – following code excludes less 10 cm height:\n\ndf_h10 <- df_h0 %>% \n  filter(height >= 10)\nRepeat step 1 df_h10 instead df_h0 compare results.Sample means unbiased variances unbiased samples randomly selected. happens samples non-random? Suppose investigator unable find plants less 10 cm height – following code excludes less 10 cm height:Repeat step 1 df_h10 instead df_h0 compare results.","code":"\ndf_h10 <- df_h0 %>% \n  filter(height >= 10)"},{"path":"probabilistic-view.html","id":"probabilistic-view","chapter":"3 Probabilistic View","heading":"3 Probabilistic View","text":"Chapter 2 emphasized concept sampling introduced crucial aspect statistics: randomness. Although mean represents central tendency data, encompass data points. Inevitable deviations occur, need way express “randomness.” concept probability distributions aids fully understanding stochastic nature observed data.Key words: probability density function (PDF), probability mass function (PMF), expected value","code":""},{"path":"probabilistic-view.html","id":"continuous-variable","chapter":"3 Probabilistic View","heading":"3.1 Continuous Variable","text":"","code":""},{"path":"probabilistic-view.html","id":"probability-density-function","chapter":"3 Probabilistic View","heading":"3.1.1 Probability Density Function","text":"Let use plant height example Chapter 2:Chapter 2 primarily focused exploring mean variance, let’s broaden perspective gain comprehensive understanding entire distribution height.\nFigure 3.1: Distribution plant height\ndistribution comprises thousand height measurements. However, certain patterns consider. data centered around mean exhibits symmetrical distribution. characteristic implies distribution can approximated simple formula relies key parameters.statistics, symmetrical bell-shaped form commonly approximated Normal distribution, often denoted “variable \\(x\\) assumed follow Normal distribution.” express using following mathematical expression:\\[\nx \\sim \\mbox{Normal}(\\mu, \\sigma^2)\n\\]Unlike “equation,” use “\\(=\\)” sign variable \\(x\\) equivalent Normal distribution. Rather, represents “stochastic” relationship signifies probability \\(x\\) taking specific range values.Normal distribution characterized two parameters: mean variance. Strictly speaking, terms “mean” “variance” probability distribution differ “sample mean” “sample variance,” although converge variable \\(x\\) assumed follow Normal distribution (refer Chapter 9 details). precisely, mean referred “expected value” denoted \\(\\mbox{E}(x)\\). variance represents expected value \\(x^2\\) denoted \\(\\mbox{E}(x^2)\\). expected value serves central tendency random variable, indicating values likely occur. hand, variance quantifies extent individual data points deviate expected value. Although use terms mean variance simplicity, important recognize distinction.parameters determined, formula defines Normal distribution yield probability density given range values. formula \\(f(x)\\) known probability density function (PDF) displayed :\\[\nf(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n\\]Well, understanding meaning formula can challenging without context. However, visualization can useful tool help comprehend mathematical expressions.","code":"\n# load csv data on R\ndf_h0 <- read_csv(\"data_raw/data_plant_height.csv\")\ndf_h0 %>% \n  ggplot(aes(x = height)) + \n  geom_histogram(binwidth = 1) + # specify binwidth\n  geom_vline(aes(xintercept = mean(height))) # draw vertical line at the mean"},{"path":"probabilistic-view.html","id":"pdf-to-frequency-distribution","chapter":"3 Probabilistic View","heading":"3.1.2 PDF to frequency distribution","text":"R, function dnorm() can used calculate probability density given value. first argument, x, vector values want calculate probability density. second argument, mean, third argument, sd, correspond mean standard deviation distribution, respectively. Note must provide mean sd calculate probability density.encompass entire range observed heights, can use \\(\\text{min}(x)\\) \\(\\text{max}(x)\\) lower upper limits, respectively.\nFigure 3.2: Probability density function Normal distribution\nshape curve appears quite similar observed; however, scale y-axis different. y-axis represents “probability density.” convert actual “probability,” need calculate area curve. R, can utilize pnorm() function purpose. calculates probability variable less specified value, provided first argument q.make estimates comparable frequency data, can calculate probability 1 cm bin. expected frequency can obtained multiplying probability sample size, case 1000. allows estimate number observations expect 1 cm bin based calculated probabilities.Overlay:\nFigure 3.3: Histogram overlaid predicted frequency (red dots line)\nremarkable probability distribution, characterized just two parameters (mean variance), can effectively reproduce overall shape observed original data set 1000 data points. great news means determine key parameters, can infer general pattern data.","code":"\n# vector of x values\n# seq() generate min to max values with specified numbers of elements or interval\n# the following produce 100 elements\nx <- seq(min(df_h0$height), max(df_h0$height), length = 100)\n\n# calculate probability density\nmu <- mean(df_h0$height)\nsigma <- sd(df_h0$height)\npd <- dnorm(x, mean = mu, sd = sigma)\n\n# figure\ntibble(y = pd, x = x) %>% # data frame\n  ggplot(aes(x = x, y = y)) +\n  geom_line() + # draw lines\n  labs(y = \"Probability density\") # re-label\n# probability of x < 10\np10 <- pnorm(q = 10, mean = mu, sd = sigma)\nprint(p10)## [1] 0.02731905\n# probability of x < 20\np20 <- pnorm(q = 20, mean = mu, sd = sigma)\nprint(p20)## [1] 0.504426\n# probability of 10 < x < 20\np20_10 <- p20 - p10\nprint(p20_10)## [1] 0.4771069\nx_min <- floor(min(df_h0$height)) # floor takes the integer part of the value\nx_max <- ceiling(max(df_h0$height)) # ceiling takes the next closest integer\nbin <- seq(x_min, x_max, by = 1) # each bin has 1cm\n\np <- NULL # empty object for probability\nfor (i in 1:(length(bin) - 1)) {\n  p[i] <- pnorm(bin[i+1], mean = mu, sd = sigma) - pnorm(bin[i], mean = mu, sd = sigma)\n}\n\n# data frame for probability\n# bin: last element [-length(bin)] was removed to match length\n# expected frequency in each bin is \"prob times sample size\"\ndf_prob <- tibble(p, bin = bin[-length(bin)]) %>% \n  mutate(freq = p * nrow(df_h0))\ndf_h0 %>% \n  ggplot(aes(x = height)) + \n  geom_histogram(binwidth = 1) +\n  geom_point(data = df_prob,\n             aes(y = freq,\n                 x = bin),\n             color = \"salmon\") +\n  geom_line(data = df_prob,\n            aes(y = freq,\n                x = bin),\n            color = \"salmon\")"},{"path":"probabilistic-view.html","id":"discrete-variable","chapter":"3 Probabilistic View","heading":"3.2 Discrete Variable","text":"","code":""},{"path":"probabilistic-view.html","id":"probability-mass-function","chapter":"3 Probabilistic View","heading":"3.2.1 Probability Mass Function","text":"Let’s shift perspective examine density plants garden. analysis, counted number plant individuals within 30 plots, plot covering area one square meter (refer Figure 3.4).\nFigure 3.4: Garden view sampling plots. White squares represent plots. Red dots indicate plant individuals counted.\nDownload data load onto R:Make histogram:\nFigure 3.5: Histogram plant individuals per plot\nseveral significant differences compared plant height example considering density plants garden:possible values discrete (count data) since counting number plant individuals plot.possible values discrete (count data) since counting number plant individuals plot.possible values always positive, counts negative.possible values always positive, counts negative.distribution appears non-symmetric around sample mean.distribution appears non-symmetric around sample mean.Given characteristics, Normal distribution may appropriate choice representing variable. Instead, suitable use Poisson distribution characterize observed distribution discrete variable.\\[\nx \\sim \\mbox{Poisson}(\\lambda)\n\\]Poisson distribution, mean parameter (\\(\\lambda\\)) serves sole parameter1. Probability distributions describe discrete variables expressed using probability mass function (PMF):\\[\ng(x) = \\frac{\\lambda^x \\exp(-\\lambda)}{x!}\n\\]Unlike probability density function (PDF), probability mass function (PMF) represents probability discrete variable \\(x\\) taking specific value. instance, probability \\(x=2\\), denoted \\(\\Pr(x=2)\\), can calculated follows:\\[\n\\Pr(x=2) = g(2) = \\frac{\\lambda^2 \\exp(-\\lambda)}{2!}\n\\]generalized form, write:\\[\n\\Pr(x=k) = g(k) = \\frac{\\lambda^k \\exp(-\\lambda)}{k!}\n\\]","code":"\ndf_count <- read_csv(\"data_raw/data_garden_count.csv\")\nprint(df_count)## # A tibble: 30 × 3\n##     plot count nitrate\n##    <dbl> <dbl>   <dbl>\n##  1     1     1    14.6\n##  2     2     4    32.6\n##  3     3     6    29.3\n##  4     4     4    30.4\n##  5     5     7    34.1\n##  6     6     2    26.6\n##  7     7     2    30.2\n##  8     8     2    28.7\n##  9     9     3    32.5\n## 10    10     2    26  \n## # ℹ 20 more rows\ndf_count %>% \n  ggplot(aes(x = count)) +\n  geom_histogram(binwidth = 0.5, # define binwidth\n                 center = 0) # relative position of each bin"},{"path":"probabilistic-view.html","id":"pmf-to-frequency-distribution","chapter":"3 Probabilistic View","heading":"3.2.2 PMF to frequency distribution","text":"R, can use dpois() function visualize Poisson distribution. function allows plot probability mass function (PMF) Poisson distribution gain insights distribution discrete variable. given example, utilize sample mean estimate \\(\\lambda\\) — somewhat counter-intuitive approach since \\(x\\) conform Normal distribution. However, delve detailed explanation Chapter 9, allowing assume validity time .\nFigure 3.6: Example Poisson distribution\nconvert y-axis probability frequency, multiply probabilities sample size (total number observations) obtain expected frequency. plant height example, can plot expected frequency histogram, providing visual representation distribution discrete variable.\nFigure 3.7: Observed histogram discrete variable overlaid Poisson expectation.\nCool, Poisson distribution excellent job characterizing distribution plant counts.","code":"\n# vector of x values\n# create a vector of 0 to 10 with an interval one\n# must be integer of > 0\nx <- seq(0, 10, by = 1)\n\n# calculate probability mass\nlambda_hat <- mean(df_count$count)\npm <- dpois(x, lambda = lambda_hat)\n\n# figure\ntibble(y = pm, x = x) %>% # data frame\n  ggplot(aes(x = x, y = y)) +\n  geom_line(linetype = \"dashed\") + # draw dashed lines\n  geom_point() + # draw points\n  labs(y = \"Probability\",\n       x = \"Count\") # re-label\ndf_prob <- tibble(x = x, y = pm) %>% \n  mutate(freq = y * nrow(df_count)) # prob x sample size\n\ndf_count %>% \n  ggplot(aes(x = count)) +\n  geom_histogram(binwidth = 0.5,\n                 center = 0) +\n  geom_line(data = df_prob,\n            aes(x = x,\n                y = freq),\n            linetype = \"dashed\") +\n  geom_point(data = df_prob,\n             aes(x = x,\n                y = freq))"},{"path":"probabilistic-view.html","id":"why-probability-distributions","chapter":"3 Probabilistic View","heading":"3.3 Why Probability Distributions?","text":"seen examples, probability distributions provide framework describing likelihood different outcomes events. utilizing probability distributions, can:Make predictions estimate probabilities specific events ranges values.Determine likely values outcomes based available information.Quantify analyze uncertainty associated random variables processes.Probability distributions serve fundamental tools statistics, enabling us model, analyze, make informed decisions face uncertainty.","code":""},{"path":"probabilistic-view.html","id":"laboratory-2","chapter":"3 Probabilistic View","heading":"3.4 Laboratory","text":"","code":""},{"path":"probabilistic-view.html","id":"normal-distribution","chapter":"3 Probabilistic View","heading":"3.4.1 Normal Distribution","text":"function rnorm() produces random variable follows Normal distribution specified mean SD. Using function,Generate variable 50 observations.Create figure similar Figure 3.3","code":""},{"path":"probabilistic-view.html","id":"poisson-distribution","chapter":"3 Probabilistic View","heading":"3.4.2 Poisson Distribution","text":"function rpois() produces random variable follows Poisson distribution specified mean. Using function,Generate variable 1000 observations.Create figure similar Figure 3.7","code":""},{"path":"two-group-comparison.html","id":"two-group-comparison","chapter":"4 Two-Group Comparison","heading":"4 Two-Group Comparison","text":"Data consists “samples” extracted population interest, ’s important acknowledge samples flawless replicas entire population. Given imperfect information, can effectively investigate discern distinction two populations? Chapter, introduce one basic statistical tests – “t-test.” t-test takes following steps:Define test statistic signify difference groups (t-statistic).Define probability distribution t-statistic null hypothesis, .e., scenario assume difference groups.Estimate probability yielding greater less observed test statistic null hypothesis.Key words: t-statistic, null hypothesis, alternative hypothesis, p-value","code":""},{"path":"two-group-comparison.html","id":"explore-data-structure","chapter":"4 Two-Group Comparison","heading":"4.1 Explore Data Structure","text":"Suppose study fish populations species two lakes (Lake b). lakes stark contrast productivity (Lake b looks productive), interest difference mean body size two lakes. obtained \\(50\\) data points fish length lake (download ). Save data_raw/ use read_csv() read data:data frame, fish length data lake b recorded. Confirm unique() distinct() function:Visualization provides powerful tool summarizing data effectively. plotting individual data points overlaid mean values error bars, can observe distribution patterns within data, allowing us identify trends, variations, potential outliers:\nFigure 4.1: Example mean SD plot\nHmm, possible noticeable disparity body size Lake Lake b. However, can ascertain provide evidence claim?","code":"\nlibrary(tidyverse) # call add-in packages everytime you open new R session\ndf_fl <- read_csv(\"data_raw/data_fish_length.csv\")\nprint(df_fl)## # A tibble: 100 × 3\n##    lake  length unit \n##    <chr>  <dbl> <chr>\n##  1 a       10.8 cm   \n##  2 a       13.6 cm   \n##  3 a       10.1 cm   \n##  4 a       18.6 cm   \n##  5 a       14.2 cm   \n##  6 a       10.1 cm   \n##  7 a       14.7 cm   \n##  8 a       15.6 cm   \n##  9 a       15   cm   \n## 10 a       11.9 cm   \n## # ℹ 90 more rows\n# unique returns unique values as a vector\nunique(df_fl$lake)## [1] \"a\" \"b\"\n# distinct returns unique values as a tibble\ndistinct(df_fl, lake)## # A tibble: 2 × 1\n##   lake \n##   <chr>\n## 1 a    \n## 2 b\n# group mean and sd\ndf_fl_mu <- df_fl %>% \n  group_by(lake) %>% # group operation\n  summarize(mu_l = mean(length), # summarize by mean()\n            sd_l = sd(length)) # summarize with sd()\n\n# plot\n# geom_jitter() plot data points with scatter\n# geom_segment() draw lines\n# geom_point() draw points\ndf_fl %>% \n  ggplot(aes(x = lake,\n             y = length)) +\n  geom_jitter(width = 0.1, # scatter width\n              height = 0, # scatter height (no scatter with zero)\n              alpha = 0.25) + # transparency of data points\n  geom_segment(data = df_fl_mu, # switch data frame\n               aes(x = lake,\n                   xend = lake,\n                   y = mu_l - sd_l,\n                   yend = mu_l + sd_l)) +\n  geom_point(data = df_fl_mu, # switch data frame\n             aes(x = lake,\n                 y = mu_l),\n             size = 3) +\n  labs(x = \"Lake\", # x label\n       y = \"Fish body length\") # y label"},{"path":"two-group-comparison.html","id":"test-statistic","chapter":"4 Two-Group Comparison","heading":"4.2 Test Statistic","text":"","code":""},{"path":"two-group-comparison.html","id":"t-statistic","chapter":"4 Two-Group Comparison","heading":"4.2.1 t-statistic","text":"Since focus examining difference means, logical estimate disparity sample means lake. Let denote sample means \\(\\hat{\\mu}_a\\) \\(\\hat{\\mu}_b\\) Lake Lake b, respectively. can estimate difference means using following approach:average fish body size Lake b approximately 2 cm larger Lake . However, crucial recognize still lacking vital information, specifically, uncertainty associated difference. discussed Chapter 2, must acknowledge sample means flawless representations entire population.Fortunately, can utilize sample variances address uncertainties. t-statistic common indicator difference means takes consideration uncertainty associated sample means.\\[\nt = \\frac{\\hat{\\mu_a} - \\hat{\\mu_b}}{\\sqrt{\\hat{\\sigma}^2_p \\left(\\frac{1}{N_a} + \\frac{1}{N_b}\\right)}}\n\\]\\(N_a\\) \\(N_b\\) sample sizes Lake b (.e., \\(N_a = N_b = 50\\) specific example), \\(\\hat{\\sigma}^2_p\\) weighted mean sample variances:\\[\n\\hat{\\sigma}^2_p = \\frac{N_a-1}{N_a + N_b - 2}\\hat{\\sigma}^2_a + \\frac{N_b-1}{N_a + N_b - 2}\\hat{\\sigma}^2_b\n\\]can calculate value R manually:difference sample means -2.06; therefore, t-statistic emphasizes distinction study lakes. occurrence can attributed t-statistic formula.denominator, observe inclusion \\(\\hat{\\sigma}^2_p\\) (estimated pooled variance), well inverses sample sizes \\(N_a\\) \\(N_b\\). Consequently, \\(\\hat{\\sigma}^2_p\\) decreases /sample sizes increase, t-statistic increases. reasonable outcome since decrease variance /increase sample size enhance certainty mean difference.","code":"\n# take another look at df_fl_mu\nprint(df_fl_mu)## # A tibble: 2 × 3\n##   lake   mu_l  sd_l\n##   <chr> <dbl> <dbl>\n## 1 a      13.4  2.92\n## 2 b      15.4  3.39\n# pull mu_l from tibble as vector\nv_mu <- df_fl_mu %>% \n  pull(mu_l)\n\n# lake a\nprint(v_mu[1])## [1] 13.35\n# lake b\nprint(v_mu[2])## [1] 15.406\n# difference\nv_mu[1] - v_mu[2]## [1] -2.056\n# group mean, variance, and sample size\ndf_t <- df_fl %>% \n  group_by(lake) %>% # group operation\n  summarize(mu_l = mean(length), # summarize by mean()\n            var_l = var(length), # summarize with sd()\n            n = n()) # count number of rows per group\n\nprint(df_t)## # A tibble: 2 × 4\n##   lake   mu_l var_l     n\n##   <chr> <dbl> <dbl> <int>\n## 1 a      13.4  8.52    50\n## 2 b      15.4 11.5     50\n# pull values as a vector\nv_mu <- pull(df_t, mu_l)\nv_var <- pull(df_t, var_l)\nv_n <- pull(df_t, n)\n\nvar_p <- ((v_n[1] - 1)/(sum(v_n) - 2)) * v_var[1] +\n  ((v_n[2] - 1)/(sum(v_n) - 2)) * v_var[2]\n\nt_value <- (v_mu[1] - v_mu[2]) / sqrt(var_p * ((1 / v_n[1]) + (1 / v_n[2])))\n\nprint(t_value)## [1] -3.247322"},{"path":"two-group-comparison.html","id":"null-hypothesis","chapter":"4 Two-Group Comparison","heading":"4.2.2 Null Hypothesis","text":"observed t-statistic serves dual purpose accounting disparity means accompanying uncertainty. However, significance t-statistic remains unclear.address , concept Null Hypothesis utilized substantiate observed t-statistic. Null Hypothesis, difference groups \\(\\mu_a = \\mu_b\\), allows us draw probability distribution test-statistic. know probability distribution, can estimate probability observing given t-statistic random chance difference mean body size lakes.Let’s begin assuming difference mean body size fish populations lakes, meaning true difference means zero (\\(\\mu_a - \\mu_b = 0\\); without hats formula!). assumption, can define probability distribution t-statistics, represents t-statistics distributed across range possible values.probability distribution known Student’s t-distribution characterized three parameters: mean, variance, degrees freedom (d.f.). Considering evaluating difference body size test statistic, mean (difference) assumed zero. variance estimated using \\(\\hat{\\sigma}^2_p\\), degrees freedom determined \\(N_a + N_b - 2\\) (d.f. can considered measure related sample size2).R function draw Student’s t-distribution; let’s try :\nFigure 4.2: Distribution t-statistics null hypothesis. probability density distribution determines likelihood observing particular t-statistic. Higher probability density indicates greater likelihood t-statistic occurring.\nprobability density distribution (Figure 4.2) determines likelihood observing particular t-statistic. Higher probability density indicates greater likelihood t-statistic occurring. Compare observed t-statistic null distribution:\nFigure 4.3: Observed t-statistic comparison null distrubution\nprobability distribution, area curve corresponds probability. Notably, area curve falls observed t-statistic (indicated vertical red lines) small (Figure 4.3), meaning observed difference body size unlikely occur null hypothesis (difference true means).function pt() allows us calculate area curve:p-value, .e., probability observing t-statistics less ( pr_below) greater (pr_above) observed t-statistic null hypothesis, can estimated sum pr_below pr_above.","code":"\n# produce 500 values from -5 to 5 with equal interval\nx <- seq(-5, 5, length = 500)\n\n# probability density of t-statistics with df = sum(v_n) - 2\ny <- dt(x, df = sum(v_n) - 2)\n\n# draw figure\ntibble(x, y) %>% \n  ggplot(aes(x = x,\n             y = y)) +\n  geom_line() +\n  labs(y = \"Probability density\",\n       x = \"t-statistic\")\n# draw entire range\ntibble(x, y) %>% \n  ggplot(aes(x = x,\n             y = y)) +\n  geom_line() +\n  geom_vline(xintercept = t_value,\n             color = \"salmon\") + # t_value is the observed t_value\n  geom_vline(xintercept = abs(t_value),\n             color = \"salmon\") + # t_value is the observed t_value\n  labs(y = \"Probability density\",\n       x = \"t-statistic\") \n# calculate area under the curve from -infinity to t_value\npr_below <- pt(q = t_value, df = sum(v_n) - 2)\n\n# calculate area under the curve from abs(t_value) to infinity\npr_above <- 1 - pt(q = abs(t_value), df = sum(v_n) - 2)\n# p_value\np_value <- pr_below + pr_above\nprint(p_value)## [1] 0.001595529"},{"path":"two-group-comparison.html","id":"interpretation","chapter":"4 Two-Group Comparison","heading":"4.2.3 Interpretation","text":"exercise called t-test – perhaps, well-known hypothesis testing. explain interpretation results, let use following notations. \\(t_{obs}\\), observed t-statistic; \\(t_0\\), possible t-statistics null hypothesis; \\(\\Pr(\\cdot)\\), probability “\\(\\cdot\\)” occurs – example, \\(\\Pr(x > 2)\\) means probability \\(x\\) exceeding \\(2\\).first calculated observed t-statistic \\(t_{obs}\\) using data fish body size, -3.25. , calculated p-value, .e., \\(\\Pr(t_0 < t_{obs}) + \\Pr(t_0 > t_{obs})\\) null hypothesis \\(\\mu_a = \\mu_b\\). found p-value low, meaning observed t-statistic unlikely occur \\(\\mu_a = \\mu_b\\) truth. Therefore, logical conclude Alternative Hypothesis \\(\\mu_a \\ne \\mu_b\\) likely.logic used many statistical analyses – define null hypothesis, estimate probability observing given value null hypothesis. tricky, find genius: can substantiate observation(s) objectively otherwise subjective claim!However, crucial recognize , even found observed t-statistic results high p-value (.e., common null hypothesis), finding result support null hypothesis – just can’t reject .","code":""},{"path":"two-group-comparison.html","id":"t-test-in-r","chapter":"4 Two-Group Comparison","heading":"4.3 t-test in R","text":"","code":""},{"path":"two-group-comparison.html","id":"t-test-with-equal-variance","chapter":"4 Two-Group Comparison","heading":"4.3.1 t-test with equal variance","text":"R, t-test can performed t.test(). Let examine function gives us identical results:reported t-statistic t = -3.2473, p-value p-value = 0.001596, degrees freedom agreed (df = 98) estimated manually. Importantly, t-test assumes relative similarity variance groups.","code":"\nx <- df_fl %>%\n  filter(lake == \"a\") %>%  # subset lake a\n  pull(length)\n\ny <- df_fl %>%\n  filter(lake == \"b\") %>% # subset lake b\n  pull(length)\n\nt.test(x, y, var.equal = TRUE)## \n##  Two Sample t-test\n## \n## data:  x and y\n## t = -3.2473, df = 98, p-value = 0.001596\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -3.3124399 -0.7995601\n## sample estimates:\n## mean of x mean of y \n##    13.350    15.406"},{"path":"two-group-comparison.html","id":"t-test-with-unequal-variance","chapter":"4 Two-Group Comparison","heading":"4.3.2 t-test with unequal variance","text":"relative similarity variance groups unrealistic assumption. Luckily, variant t-test “Welch’s t-test,” assume unequal variance groups. implementation easy – set var.equal = FALSE t.test():Welch’s t-test, t-statistics defined differently account unequal variance groups3:\\[\nt = \\frac{\\hat{\\mu_a} - \\hat{\\mu_b}}{\\sqrt{\\left(\\frac{\\hat{\\sigma}^2_a}{N_a} + \\frac{\\hat{\\sigma}^2_b}{N_b}\\right)}}\n\\]t-statistic known follow Student’s t-distribution degrees freedom:\\[\nd.f. = \\frac{\\frac{\\hat{\\sigma}^2_a}{N_a} + \\frac{\\hat{\\sigma}^2_b}{N_b}}{\\frac{(\\hat{\\sigma}^2_a / N_a)^2}{N_a - 1} + \\frac{(\\hat{\\sigma}^2_b/N_b)^2}{N_b-1}}\n\\]Therefore, reported values t-statistic d.f. different estimated. Welch’s t-test covers cases equal variances; therefore, default, use Welch’s test.","code":"\nt.test(x, y, var.equal = FALSE)## \n##  Welch Two Sample t-test\n## \n## data:  x and y\n## t = -3.2473, df = 95.846, p-value = 0.001606\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -3.3127929 -0.7992071\n## sample estimates:\n## mean of x mean of y \n##    13.350    15.406"},{"path":"two-group-comparison.html","id":"laboratory-3","chapter":"4 Two-Group Comparison","heading":"4.4 Laboratory","text":"","code":""},{"path":"two-group-comparison.html","id":"welchs-t-test","chapter":"4 Two-Group Comparison","heading":"4.4.1 Welch’s t-test","text":"Reproduce results t.test(x, y, var.equal = FALSE) without using function (report t-statistic, degrees freedom, p-value).","code":""},{"path":"two-group-comparison.html","id":"difference-and-uncertainty","chapter":"4 Two-Group Comparison","heading":"4.4.2 Difference and Uncertainty","text":"four vectors - a1 a2 & b1 b2.Perform following analysis:Estimate sample means SDs vector. , create tibble() object group (consist characters a1, a2, b1, b2) value columns (consist values ). use group_by() summarize() functions estimate means SDs group.Estimate t-statistics pair vectors (a1 vs. a2 b1 vs. b2).Perform Welch’s t-test pair compare results.","code":"\na1 <- c(13.9, 14.9 ,13.4, 14.3, 11.8, 13.9, 14.5, 15.1, 13.3, 13.9)\na2 <- c(17.4, 17.3, 20.1, 17.2, 18.4, 19.6, 16.8, 18.7, 17.8, 18.9)\n\nb1 <- c(10.9, 20.3, 9.6, 8.3, 14.5, 12.3, 14.5, 16.7, 9.3, 22.0)\nb2 <- c(26.9, 12.9, 11.1, 16.7, 20.0, 20.9, 16.6, 15.4, 16.2, 16.2)"},{"path":"multiple-group-comparison.html","id":"multiple-group-comparison","chapter":"5 Multiple-Group Comparison","heading":"5 Multiple-Group Comparison","text":"t-test used compare two groups, two groups, ANOVA (Analysis Variance) employed. ANOVA allows simultaneous comparison means among multiple groups determine statistically significant differences. ANOVA uses following steps:Partition total variability -group within-group components.Define test statistic ratio -group variability within-group variability (F statistic).Define probability distribution F-statistic null hypothesis.Estimate probability yielding greater observed test statistic.appropriate, post-hoc tests can conducted identify specific differing groups.Key words: F-statistic, Sum Squares","code":""},{"path":"multiple-group-comparison.html","id":"partition-the-variability","chapter":"5 Multiple-Group Comparison","heading":"5.1 Partition the Variability","text":"first step determine aspect examine. case t-test, focus difference sample means groups. One might initially consider conducting t-tests possible combinations. However, approach leads problem known multiple comparisons problem. Hence, viable option. Therefore, need explore data different perspective.facilitate learning, let’s utilize lake fish data, time three lakes analysis:Visualization can helpful understanding data distributed. mean \\(\\pm\\) SD perfectly fine , let use violin plot show different way visualization.\nFigure 5.1: Violin plot fish length three lakes.\nappears Lake b exhibits larger average body size compared lakes. One approach quantifying difference groups examining ratio -group variability within-group variability. observe greater -group variability relative within-group variability, suggests differences among groups substantial. words, much observed variation explained group structure (lake).Let denote -group within-group variability \\(S_b\\) \\(S_w\\), respectively, defined follows:\\[\n\\begin{aligned}\nS_b &= \\sum_g \\sum_i (\\hat{\\mu}_{g()} - \\hat{\\mu})^2\\\\\nS_w &= \\sum_g \\sum_i (x_{} - \\hat{\\mu}_{g()})^2\n\\end{aligned}\n\\]double summation \\(\\sum_g \\sum_i\\) might scare , worries. can decompose equation two steps.","code":"\ndf_anova <- read_csv(\"data_raw/data_fish_length_anova.csv\")\ndistinct(df_anova, lake)## # A tibble: 3 × 1\n##   lake \n##   <chr>\n## 1 a    \n## 2 b    \n## 3 c\n# geom_violin() - function for violin plots\n# geom_jitter() - jittered points\n\ndf_anova %>% \n  ggplot(aes(x = lake,\n             y = length)) +\n  geom_violin(draw_quantiles = 0.5, # draw median horizontal line\n              alpha = 0.2) + # transparency\n  geom_jitter(alpha = 0.2) # transparency"},{"path":"multiple-group-comparison.html","id":"between-group-variability","chapter":"5 Multiple-Group Comparison","heading":"5.1.1 Between-group variability","text":"Let first consider \\(S_b = \\sum_g \\sum_i (\\hat{\\mu}_{g()} - \\hat{\\mu})^2\\). equation, \\(\\hat{\\mu}\\) overall mean fish length \\(\\hat{\\mu}_{g()}\\) group-mean given lake (\\(g \\\\{, b, c\\}\\)). Let’s perform estimation R:column dev_g, estimated \\((\\hat{\\mu}_{g()} - \\hat{\\mu})^2\\). must sum \\(\\) (fish individual) get variability lake (\\(\\sum_i (\\hat{\\mu}_{g()} - \\hat{\\mu})^2\\)). Since \\(\\hat{\\mu}_{g()}\\) constant lake, can simply multiply dev_g sample size n lake:Sum \\(g\\) (lake) get \\(S_b\\).","code":"\n# estimate overall mean\nmu <- mean(df_anova$length)\n\n# estimate group means and sample size each\ndf_g <- df_anova %>% \n  group_by(lake) %>% \n  summarize(mu_g = mean(length), # mean for each group\n            dev_g = (mu_g - mu)^2, # squared deviation for each group\n            n = n()) # sample size for each group\n\nprint(df_g)## # A tibble: 3 × 4\n##   lake   mu_g   dev_g     n\n##   <chr> <dbl>   <dbl> <int>\n## 1 a      13.4 1.12       50\n## 2 b      15.4 0.997      50\n## 3 c      14.5 0.00344    50\ndf_g <- df_g %>% \n  mutate(ss = dev_g * n)\n\nprint(df_g)## # A tibble: 3 × 5\n##   lake   mu_g   dev_g     n     ss\n##   <chr> <dbl>   <dbl> <int>  <dbl>\n## 1 a      13.4 1.12       50 55.9  \n## 2 b      15.4 0.997      50 49.9  \n## 3 c      14.5 0.00344    50  0.172\ns_b <- sum(df_g$ss)\nprint(s_b)## [1] 105.9365"},{"path":"multiple-group-comparison.html","id":"within-group-variability","chapter":"5 Multiple-Group Comparison","heading":"5.1.2 Within-group variability","text":"can follow steps estimate within-group variability \\(S_w = \\sum_g \\sum_i (x_{} - \\hat{\\mu}_{g()})^2\\). Let’s estimate \\((x_{} - \\hat{\\mu}_{g()})^2\\) first:can take look group-level data following code; column mu_g contains group-specific means fish length, dev_i contains \\((x_i - \\hat{\\mu}_{g()})^2\\):Sum \\(\\) lake \\(g\\) (\\(\\sum_i (x_i - \\hat{\\mu}_{g()})^2\\)):sum \\(g\\) get \\(S_w\\):","code":"\ndf_i <- df_anova %>% \n  group_by(lake) %>% \n  mutate(mu_g = mean(length)) %>% # use mutate() to retain individual rows\n  ungroup() %>% \n  mutate(dev_i = (length - mu_g)^2) # deviation from group mean for each fish\n# filter() & slice(): show first 3 rows each group\nprint(df_i %>% filter(lake == \"a\") %>% slice(1:3))\n\nprint(df_i %>% filter(lake == \"b\") %>% slice(1:3))\n\nprint(df_i %>% filter(lake == \"c\") %>% slice(1:3))\ndf_i_g <- df_i %>% \n  group_by(lake) %>% \n  summarize(ss = sum(dev_i))\n\nprint(df_i_g)## # A tibble: 3 × 2\n##   lake     ss\n##   <chr> <dbl>\n## 1 a      417.\n## 2 b      565.\n## 3 c      486.\ns_w <- sum(df_i_g$ss)\nprint(s_w)## [1] 1467.645"},{"path":"multiple-group-comparison.html","id":"variability-to-variance","chapter":"5 Multiple-Group Comparison","heading":"5.1.3 Variability to Variance","text":"referred \\(S_b\\) \\(S_w\\) “variability,” essentially represents summation squared deviations. convert variances, can divide appropriate numbers. Chapter 2, mentioned denominator variance sample size minus one. principle applies , caution.-group variability, denoted \\(S_b\\), realized sample size number groups, \\(N_g\\), case three (representing number lakes). Therefore, divide three minus one obtain unbiased estimate -group variance, denoted \\(\\hat{\\sigma}_b^2\\):\\[\n\\hat{\\sigma}^2_b = \\frac{S_b}{N_g-1}\n\\]Meanwhile, need careful estimating within-group variance. Since within-group variance measured individual level, number data used equal number fish individuals. Yet, subtract number groups – rationale behind beyond scope, essentially accounting fact degrees freedom “used ” estimating group means. , estimate within-group variance \\(\\hat{\\sigma}^2_w\\) follows:\\[\n\\hat{\\sigma}^2_w = \\frac{S_w}{N-N_g}\n\\]","code":"\n# n_distinct() count the number of unique elements\nn_g <- n_distinct(df_anova$lake)\ns2_b <- s_b / (n_g - 1)\nprint(s2_b)## [1] 52.96827\ns2_w <- s_w / (nrow(df_anova) - n_g)\nprint(s2_w)## [1] 9.983982"},{"path":"multiple-group-comparison.html","id":"test-statistic-1","chapter":"5 Multiple-Group Comparison","heading":"5.2 Test Statistic","text":"","code":""},{"path":"multiple-group-comparison.html","id":"f-statistic","chapter":"5 Multiple-Group Comparison","heading":"5.2.1 F-statistic","text":"ANOVA, use F-statistic – ratio -group variability within-group variability. exercise essentially performed yield test statistic:\\[\nF = \\frac{\\text{-group variance}}{\\text{within-group variance}} = \\frac{\\hat{\\sigma}^2_b}{\\hat{\\sigma}^2_w}\n\\]F-statistic data calculated 5.31. indicates -group variance approximately five times higher within-group variance. difference appears significant, important determine whether statistically substantial. make claim, can use Null Hypothesis reference.","code":"\nf_value <- s2_b / s2_w\nprint(f_value)## [1] 5.305325"},{"path":"multiple-group-comparison.html","id":"null-hypothesis-1","chapter":"5 Multiple-Group Comparison","heading":"5.2.2 Null Hypothesis","text":"F-statistic follows F-distribution difference means among groups. Therefore, null hypothesis considering example means groups equal, represented \\(\\mu_a = \\mu_b = \\mu_c\\). ’s worth noting alternative hypotheses can take different forms, \\(\\mu_a \\ne \\mu_b = \\mu_c\\), \\(\\mu_a = \\mu_b \\ne \\mu_c\\), \\(\\mu_a \\ne \\mu_b \\ne \\mu_c\\). ANOVA, however, unable distinguish alternative hypotheses.degrees freedom F-distribution determined two parameters: \\(N_g - 1\\) \\(N - N_g\\). visualize distribution, can utilize thedf() function. Similar t-test, can plot F-distribution draw vertical line represent observed F-statistic. approach allows us assess position observed F-statistic within distribution determine associated p-value.\nFigure 5.2: F-distribution. vertical red line denotes observed F-statistic.\nUnlike t-statistics, F-statistics can take positive values (F-statistics ratio positive values). p-value \\(\\Pr(F_0 > F)\\), \\(F_0\\) possible F-statistics null hypothesis. Let’s estimate probability using pf():","code":"\nx <- seq(0, 10, by = 0.1)\ny <- df(x = x, df1 = n_g - 1, df2 = nrow(df_anova) - n_g)\n\ntibble(x = x, y = y) %>% \n  ggplot(aes(x = x,\n             y = y)) + \n  geom_line() + # F distribution\n  geom_vline(xintercept = f_value,\n             color = \"salmon\") # observed F-statistic\n# pf() estimate the probability of less than q\n# Pr(F0 > F) is 1 - Pr(F0 < F)\np_value <- 1 - pf(q = f_value, df1 = n_g - 1, df2 = nrow(df_anova) - n_g)\nprint(p_value)## [1] 0.00596054"},{"path":"multiple-group-comparison.html","id":"anova-in-r","chapter":"5 Multiple-Group Comparison","heading":"5.3 ANOVA in R","text":"Like t-test, R provides functions perform ANOVA easily. case, utilize aov() function. first argument function “formula,” used describe structure model. scenario, aim explain fish body length grouping according lakes. formula expression, represent relationship length ~ lake, using tilde symbol (~) indicate stochastic nature relationship.function returns Sum Squares Deg. Freedom – value matches calculated. get deeper insights, wrap object summary():F-statistic p-value identical manual estimate.","code":"\n# first argument is formula\n# second argument is data frame for reference\n# do not forget specify data = XXX! aov() refer to columns in the data frame\nm <- aov(formula = length ~ lake,\n         data = df_anova)\n\nprint(m)## Call:\n##    aov(formula = length ~ lake, data = df_anova)\n## \n## Terms:\n##                      lake Residuals\n## Sum of Squares   105.9365 1467.6454\n## Deg. of Freedom         2       147\n## \n## Residual standard error: 3.159744\n## Estimated effects may be unbalanced\nsummary(m)##              Df Sum Sq Mean Sq F value  Pr(>F)   \n## lake          2  105.9   52.97   5.305 0.00596 **\n## Residuals   147 1467.6    9.98                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"multiple-group-comparison.html","id":"post-hoc-tests","chapter":"5 Multiple-Group Comparison","heading":"5.4 Post-hoc Tests","text":"conducting ANOVA finding significant difference among group means, post-hoc test often performed determine specific groups differ significantly . Post-hoc tests help identify pairwise comparisons contribute observed overall difference.several post-hoc tests available ANOVA, including:Tukey’s Honestly Significant Difference (HSD): test compares possible pairs group means provides adjusted p-values control family-wise error rate. commonly used reliable post-hoc test.Tukey’s Honestly Significant Difference (HSD): test compares possible pairs group means provides adjusted p-values control family-wise error rate. commonly used reliable post-hoc test.Bonferroni correction: method adjusts significance level individual comparison control overall Type error rate. adjusted p-value obtained dividing desired significance level (e.g., 0.05) number comparisons.Bonferroni correction: method adjusts significance level individual comparison control overall Type error rate. adjusted p-value obtained dividing desired significance level (e.g., 0.05) number comparisons.Scheffe’s test: test controls family-wise error rate considering possible pairwise comparisons. conservative Tukey’s HSD, suitable cases specific comparisons particular interest.Scheffe’s test: test controls family-wise error rate considering possible pairwise comparisons. conservative Tukey’s HSD, suitable cases specific comparisons particular interest.Dunnett’s test: test useful comparing multiple groups control group. controls overall error rate conducting multiple t-tests control group groups.Dunnett’s test: test useful comparing multiple groups control group. controls overall error rate conducting multiple t-tests control group groups.choice post-hoc test depends specific research question, assumptions, desired control Type error rate. important select appropriate test interpret results accordingly draw valid conclusions group differences ANOVA analysis.","code":""},{"path":"multiple-group-comparison.html","id":"laboratory-4","chapter":"5 Multiple-Group Comparison","heading":"5.5 Laboratory","text":"","code":""},{"path":"multiple-group-comparison.html","id":"application-to-plantgrowth","chapter":"5 Multiple-Group Comparison","heading":"5.5.1 Application to PlantGrowth","text":"R, built-data set called PrantGrowth. analyze data set, carry following tasks:data set consists two columns: weight, group. Create figures similar Figure 4.1 Chapter 4.data set consists two columns: weight, group. Create figures similar Figure 4.1 Chapter 4.Conduct ANOVA examine whether differences weight among different group.Conduct ANOVA examine whether differences weight among different group.Discuss values reported scientific article?Discuss values reported scientific article?","code":""},{"path":"regression.html","id":"regression","chapter":"6 Regression","heading":"6 Regression","text":"previous examples focused cases involved distinct group structure. However, always case structure exists. Instead, might examine relationship continuous variables. chapter, introduce technique called linear regression.Key words: intercept, slope (coefficient), least squares, residual, coefficient determination (\\(\\mbox{R}^2\\))","code":""},{"path":"regression.html","id":"explore-data-structure-1","chapter":"6 Regression","heading":"6.1 Explore Data Structure","text":"exercise, use algae biomass data streams. Download data locate data_raw/.dataset comprises data collected 50 sampling sites. variable biomass represents standing biomass, indicating dry mass algae time collection. hand, conductivity serves proxy water quality, higher values usually indicating higher nutrient content water. Let now proceed draw scatter plot help visualize relationship two variables.\nFigure 6.1: relationship algae biomass conductivity.\nseems noticeable positive correlation increased conductivity results higher algal biomass. Nevertheless, can accurately represent relationship line?","code":"\nlibrary(tidyverse)\ndf_algae <- read_csv(\"data_raw/data_algae.csv\")\nprint(df_algae)## # A tibble: 50 × 4\n##    biomass unit_biomass conductivity unit_cond\n##      <dbl> <chr>               <dbl> <chr>    \n##  1   19.8  mg_per_m2            30.2 ms       \n##  2   24.4  mg_per_m2            40.4 ms       \n##  3   27.4  mg_per_m2            59.4 ms       \n##  4   48.2  mg_per_m2            91.3 ms       \n##  5   19.2  mg_per_m2            24.2 ms       \n##  6   57.0  mg_per_m2            90.4 ms       \n##  7   51.9  mg_per_m2            94.7 ms       \n##  8   40.8  mg_per_m2            67.8 ms       \n##  9   37.1  mg_per_m2            64.8 ms       \n## 10    3.55 mg_per_m2            10.9 ms       \n## # ℹ 40 more rows\ndf_algae %>% \n  ggplot(aes(x = conductivity,\n             y = biomass)) +\n  geom_point()"},{"path":"regression.html","id":"drawing-the-fittest-line","chapter":"6 Regression","heading":"6.2 Drawing the “fittest” line","text":"","code":""},{"path":"regression.html","id":"linear-formula","chapter":"6 Regression","heading":"6.2.1 Linear formula","text":"first step represent relationship formula; let ASSUME relationship can described following linear formula:\\[\ny_i = \\alpha + \\beta x_i\n\\]formula, algal biomass, denoted \\(y_i\\), site \\(\\), expressed function conductivity, represented \\(x_i\\). variable explained (\\(y_i\\)) referred response (dependent) variable, variable used predict response variable (\\(x_i\\)) referred explanatory (independent) variable. However, two additional constants, namely \\(\\alpha\\) \\(\\beta\\). \\(\\alpha\\) commonly known intercept, \\(\\beta\\) referred slope coefficient. intercept represents value \\(y\\) \\(x\\) equal zero, slope indicates change \\(y\\) associated unit increase \\(x\\) (refer Figure 6.2).\nFigure 6.2: Intercept slope.\nHowever, formula, “model,” incomplete. must add error term \\(\\varepsilon_i\\) (residual) consider uncertainty associated observation process.\\[\ny_i = \\alpha + \\beta x_i + \\varepsilon_i\\\\\n\\varepsilon_i \\sim \\text{Normal}(0, \\sigma^2)\n\\]Notice \\(y_i\\), \\(x_i\\), \\(\\varepsilon_i\\) subscripts, \\(\\alpha\\) \\(\\beta\\) . means \\(y_i\\), \\(x_i\\), \\(\\varepsilon_i\\) vary data point \\(\\), \\(\\alpha\\) \\(\\beta\\) constants. Although \\(\\alpha + \\beta x_i\\) reproduce \\(y_i\\) perfectly, “fill” gaps error term \\(\\varepsilon_i\\), assumed follow Normal distribution.R, finding best \\(\\alpha\\) \\(\\beta\\) – .e., parameters model – easy. Function lm() everything . Let apply function example data set:Coefficients:, says (Intercept) 5.3 conductivity 0.5. values corresponds \\(\\alpha\\) \\(\\beta\\). Meanwhile, Residual standard error: indicates SD error term \\(\\sigma\\). Thus, substituting values formula yields:\\[\ny_i = 5.30 + 0.50 x_i + \\varepsilon_i\\\\\n\\varepsilon_i \\sim \\text{Normal}(0, 4.6^2)\n\\]can draw “fittest” line figure:\nFigure 6.3: Drawing fitted model prediction.\nHowever, lm() find \\(\\alpha\\) \\(\\beta\\)?","code":"\n# lm() takes a formula as the first argument\n# don't forget to supply your data\nm <- lm(biomass ~ conductivity,\n        data = df_algae)\n\nsummary(m)## \n## Call:\n## lm(formula = biomass ~ conductivity, data = df_algae)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.5578 -2.8729 -0.7307  2.5479 11.4700 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   5.29647    1.57178    3.37  0.00149 ** \n## conductivity  0.50355    0.02568   19.61  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.649 on 48 degrees of freedom\n## Multiple R-squared:  0.889,  Adjusted R-squared:  0.8867 \n## F-statistic: 384.5 on 1 and 48 DF,  p-value: < 2.2e-16\n# coef() extracts estimated coefficients\n# e.g., coef(m)[1] is (Intercept)\n\nalpha <- coef(m)[1]\nbeta <- coef(m)[2]\n\ndf_algae %>% \n  ggplot(aes(x = conductivity,\n             y = biomass)) +\n  geom_point() +\n  geom_abline(intercept = alpha,\n              slope = beta) # draw the line"},{"path":"regression.html","id":"minimizing-the-errors","chapter":"6 Regression","heading":"6.2.2 Minimizing the errors","text":"Notice error term difference \\(y_i\\) \\(\\alpha + \\beta x_i\\):\\[\n\\varepsilon_i = y_i - (\\alpha + \\beta x_i)\n\\]words, term represents portion observation \\(y_i\\) explained conductivity \\(x_i\\). Therefore, logical approach find values \\(\\alpha\\) \\(\\beta\\) minimize unexplained portion across data points \\(\\).Least squares methods widely employed statistical approach achieve objective. methods aim minimize sum squared errors, denoted \\(\\sum_i \\varepsilon_i^2\\), across data points. deviation expected value \\(\\alpha + \\beta x_i\\) increases, quantity also increases. Consequently, determining parameter values (\\(\\alpha\\) \\(\\beta\\)) minimize sum squared errors yields best-fitting formula (see Section 6.2.3 details).","code":""},{"path":"regression.html","id":"least-squares","chapter":"6 Regression","heading":"6.2.3 Least Squares","text":"Introducing matrix representation indeed helpful way explain least square method. , represent vector observed values \\(y_i\\) \\(Y\\) (\\(Y = \\{y_1, y_2,...,y_n\\}^T\\)), vector parameters \\(\\Theta\\) (\\(\\Theta = \\{\\alpha, \\beta\\}^T\\)), matrix composed \\(1\\)s \\(x_i\\) values \\(X\\). matrix \\(X\\) can written :\\[\nX =\n\\begin{pmatrix}\n  1 & x_1\\\\\n  1 & x_2\\\\\n  1 & x_3\\\\\n  \\vdots & \\vdots\\\\\n  1 & x_n\n\\end{pmatrix}\n\\]Using matrix notation, can express error vector \\(\\pmb{\\varepsilon}\\) :\\[\n\\pmb{\\varepsilon} = Y - X\\Theta\n\\]column \\(1\\)s required represent intercept; \\(X\\Theta\\) reads:\\[\nX\\Theta =\n\\begin{pmatrix}\n  1 & x_1\\\\\n  1 & x_2\\\\\n  1 & x_3\\\\\n  \\vdots & \\vdots\\\\\n  1 & x_n\n\\end{pmatrix}\n\\begin{pmatrix}\n  \\alpha\\\\\n  \\beta\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  \\alpha + \\beta x_1\\\\\n  \\alpha + \\beta x_2\\\\\n  \\alpha + \\beta x_3\\\\\n  \\vdots\\\\\n  \\alpha + \\beta x_n\n\\end{pmatrix}\n\\]Minimizing squared magnitude \\(\\pmb{\\varepsilon}\\) (solve partial derivative \\(||\\pmb{\\varepsilon}||^2\\) \\(\\Theta\\)) leads solution (detailed derivation, refer Wikipedia):\\[\n\\hat{\\Theta} = (X^TX)^{-1}X^{T}Y\n\\]Let see reproduces result lm():lm() output reference:","code":"\n# create matrix X\nv_x <- df_algae %>% pull(conductivity)\nX <- cbind(1, v_x)\n\n# create a vector of y\nY <- df_algae %>% pull(biomass)\n\n# %*%: matrix multiplication\n# t(): transpose a matrix\n# solve(): computes the inverse matrix\ntheta <- solve(t(X) %*% X) %*% t(X) %*% Y\nprint(theta)##          [,1]\n##     5.2964689\n## v_x 0.5035548\nm <- lm(biomass ~ conductivity,\n        data = df_algae)\n\ncoef(m)##  (Intercept) conductivity \n##    5.2964689    0.5035548"},{"path":"regression.html","id":"standard-errors-and-t-values","chapter":"6 Regression","heading":"6.2.4 Standard Errors and t-values","text":"Ensuring reliability estimated point estimates \\(\\alpha\\) \\(\\beta\\) crucial. summary(m) output, two statistical quantities, namely Std. Error t-value, play significant role assessing uncertainty associated estimates.standard error (Std. Error; “SE”) represents estimated standard deviation parameter estimates (\\(\\hat{\\theta}\\) either \\(\\hat{\\alpha}\\) \\(\\hat{\\beta}\\)). smaller value standard error indicates higher degree statistical reliability estimate. hand, t-value concept covered Chapter 4. value defined :\\[\nt = \\frac{\\hat{\\theta} - \\theta_0}{\\text{SE}(\\hat{\\theta})}\n\\]t-statistic akin t-test; however, context regression analysis, another group compare (.e., \\(\\theta_0\\)). Typically, regression analysis, use zero reference (\\(\\theta_0 = 0\\)). Therefore, higher t-values lm() indicate greater deviation zero. Consequently, Null Hypothesis regression analysis \\(\\beta = 0\\). hypothesis sensible specific example since interested quantifying effect conductivity. \\(\\beta = 0\\), implies conductivity effect algal biomass. Since \\(\\theta_0 = 0\\), following code reproduces reported t-values:defining Null Hypothesis t-values, can compute probability observing estimated parameters Null Hypothesis. Similar t-test, lm() utilizes Student’s t-distribution purpose. However, difference lies estimate degrees freedom. case, 50 data points need subtract number parameters, \\(\\alpha\\) \\(\\beta\\). Therefore, degrees freedom \\(50 - 2 = 48\\). value employed calculating p-value:","code":"\n# extract coefficients\ntheta <- coef(m)\n\n# extract standard errors\nse <- sqrt(diag(vcov(m)))\n\n# t-value\nt_value <- theta / se\nprint(t_value)##  (Intercept) conductivity \n##     3.369732    19.609446\n# for intercept\n# (1 - pt(t_value[1], df = 48)) calculates pr(t > t_value[1])\n# pt(-t_value[1], df = 48) calculates pr(t < -t_value[1])\np_alpha <- (1 - pt(t_value[1], df = 48)) + pt(-t_value[1], df = 48)\n\n# for slope\np_beta <- (1 - pt(t_value[2], df = 48)) + pt(-t_value[2], df = 48)\n\nprint(p_alpha)## (Intercept) \n## 0.001492023\nprint(p_beta)## conductivity \n## 7.377061e-25"},{"path":"regression.html","id":"unexplained-variation","chapter":"6 Regression","heading":"6.3 Unexplained Variation","text":"","code":""},{"path":"regression.html","id":"retrieve-errors","chapter":"6 Regression","heading":"6.3.1 Retrieve Errors","text":"function lm() provides estimates \\(\\alpha\\) \\(\\beta\\), directly provide values \\(\\varepsilon_i\\). gain deeper understanding statistical model, necessary examine residuals \\(\\varepsilon_i\\). using resid() function, can obtain values \\(\\varepsilon_i\\). allows explore extract insights developed statistical model.element retains order data point original data frame, eps[1] (\\(\\varepsilon_1\\)) identical \\(y_1 - (\\alpha + \\beta x_1)\\). Let confirm:ensure two columns indeed identical, visual check may prone overlooking small differences. perform precise comparison, can use () function check data aligns (, evaluated five decimal points using round()). comparing two columns manner, can confirm whether identical .","code":"\n# eps - stands for epsilon\neps <- resid(m)\nhead(eps)##          1          2          3          4          5          6 \n## -0.6838937 -1.2149034 -7.8576927 -3.0109473  1.7076481  6.1773586\n# pull vector data\nv_x <- df_algae %>% pull(conductivity)\nv_y <- df_algae %>% pull(biomass)\n\n# theta[1] = alpha\n# theta[2] = beta\nerror <- v_y - (theta[1] + theta[2] * v_x)\n\n# cbind() combines vectors column-wise\n# head() retrieves the first 6 rows\nhead(cbind(eps, error))##          eps      error\n## 1 -0.6838937 -0.6838937\n## 2 -1.2149034 -1.2149034\n## 3 -7.8576927 -7.8576927\n## 4 -3.0109473 -3.0109473\n## 5  1.7076481  1.7076481\n## 6  6.1773586  6.1773586\n# round values at 5 decimal\neps <- round(eps, 5)\nerror <- round(error, 5)\n\n# eps == error return \"TRUE\" or \"FALSE\"\n# all() returns TRUE if all elements meet eps == error\nall(eps == error)## [1] TRUE"},{"path":"regression.html","id":"visualize-errors","chapter":"6 Regression","heading":"6.3.2 Visualize Errors","text":"visualize errors residuals, represent distance best-fitted line data point, can make use geom_segment() function:\nFigure 6.4: Vertical segments indicate errors.\n","code":"\n# add error column\ndf_algae <- df_algae %>% \n  mutate(eps = eps)\n\ndf_algae %>% \n  ggplot(aes(x = conductivity,\n             y = biomass)) +\n  geom_point() +\n  geom_abline(intercept = alpha,\n              slope = beta) + \n  geom_segment(aes(x = conductivity, # start-coord x\n                   xend = conductivity, # end-coord x\n                   y = biomass, # start-coord y\n                   yend = biomass - eps), # end-coord y\n               linetype = \"dashed\")"},{"path":"regression.html","id":"coefficient-of-determination","chapter":"6 Regression","heading":"6.3.3 Coefficient of Determination","text":"One crucial motivation developing regression model, statistical model, assess extent explanatory variable (\\(x_i\\)) explains variation response variable (\\(y_i\\)). fitting regression model, can quantify amount variability response variable can attributed explanatory variable. assessment provides valuable insights strength significance relationship variables consideration.coefficient determination, denoted R², statistical measure assesses proportion variance response variable can explained explanatory variable(s) regression model. provides indication well regression model fits observed data. formula R² :\\[\n\\text{R}^2 = 1 - \\frac{SS}{SS_0}\n\\]\\(SS\\) summed squares residuals (\\(\\sum_i \\varepsilon_i^2\\)), \\(SS_0\\) summed squares response variable (\\(SS_0 = \\sum_i(y_i - \\hat{\\mu}_y)^2\\)). term \\(\\frac{SS}{SS_0}\\) represents proportion variability “unexplained” – therefore, \\(1 - \\frac{SS}{SS_0}\\) gives proportion variability “explained.” R² value 0 1. R² value 0 indicates explanatory variable(s) explain variability response variable, R² value 1 indicates explanatory variable(s) can fully explain variability response variable.R2 provided default output lm(), let’s confirm equation reproduces reported value:Compare lm() output:Multiple R-squared: corresponds calculated coefficient determination.","code":"\n# residual variance\nss <- sum(resid(m)^2)\n\n# null variance\nss_0 <- sum((v_y - mean(v_y))^2)\n\n# coefficient of determination\nr2 <- 1 - ss / ss_0\n\nprint(r2)## [1] 0.8890251\nsummary(m)## \n## Call:\n## lm(formula = biomass ~ conductivity, data = df_algae)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -9.5578 -2.8729 -0.7307  2.5479 11.4700 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   5.29647    1.57178    3.37  0.00149 ** \n## conductivity  0.50355    0.02568   19.61  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.649 on 48 degrees of freedom\n## Multiple R-squared:  0.889,  Adjusted R-squared:  0.8867 \n## F-statistic: 384.5 on 1 and 48 DF,  p-value: < 2.2e-16"},{"path":"regression.html","id":"laboratory-5","chapter":"6 Regression","heading":"6.4 Laboratory","text":"","code":""},{"path":"regression.html","id":"develop-regression-models","chapter":"6 Regression","heading":"6.4.1 Develop regression models","text":"R provides built-data set called iris. iris data contain data points three different species (Species column). Split data set species (create three separate data frames) perform regression species separately analyze relationship Sepal.Width (response variable) Petal.Width (explanatory variable).","code":"\nhead(iris)##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa"},{"path":"regression.html","id":"multiple-explanatory-variables","chapter":"6 Regression","heading":"6.4.2 Multiple explanatory variables","text":"Regression analysis can involve multiple explanatory variables. explore , consider utilizing Petal.Length additional explanatory variable species. , investigate (1) variations estimates regression coefficients (2) differences coefficients determination compared model single explanatory variable.","code":""},{"path":"linear-model.html","id":"linear-model","chapter":"7 Linear Model","heading":"7 Linear Model","text":"extensively covered three important statistical analyses: t-test, ANOVA, regression analysis. methods may seem distinct, fall umbrella Linear Model framework4.Linear Model encompasses models depict connection response variable one explanatory variables. assumes error term follows normal distribution. chapter, elucidate framework illustrate relationship t-test, ANOVA, regression analysis.Key words: level measurement, dummy variable","code":""},{"path":"linear-model.html","id":"the-frame","chapter":"7 Linear Model","heading":"7.1 The Frame","text":"apparent distinctiveness t-test, ANOVA, regression analysis arises applied different types data:t-test: comparing differences two groups.ANOVA: examining differences among two groups.Regression: exploring relationship response variable one explanatory variables.Despite differences, analyses can unified single formula. discussed regression formula Chapter 6, :\\[\n\\begin{aligned}\ny_i &= \\alpha + \\beta_1 x_{1,} + \\varepsilon_i\\\\\n\\varepsilon_i &\\sim \\text{Normal}(0, \\sigma^2)\n\\end{aligned}\n\\]\\(y_i\\) represents response variable (fish body length plant height), \\(x_{1,}\\) denotes continuous explanatory variable, \\(\\alpha\\) represents intercept, \\(\\beta_1\\) corresponds slope. equivalent model can expressed differently follows:\\[\n\\begin{aligned}\ny_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2)\\\\\n\\mu_i &= \\alpha + \\beta_1 x_{1,}\n\\end{aligned}\n\\]structure provides insight relationship t-test, ANOVA, regression analysis. fundamental purpose regression analysis model mean \\(\\mu_i\\) changes increasing decreasing values \\(x_{1,}\\). Thus, t-test, ANOVA, regression analyze mean – expected value Normal distribution. primary distinction t-test, ANOVA, regression lies nature explanatory variable, can either continuous categorical (group).","code":""},{"path":"linear-model.html","id":"two-group-case","chapter":"7 Linear Model","heading":"7.1.1 Two-Group Case","text":"establish connection among approaches, one can employ “dummy indicator” variables represent group variables. case t-test, categorical variable consists two groups, typically denoted b (although can represented numbers well), one can convert categories numerical values. instance, assigning 0 b 1 enables following conversion:\\[\n\\pmb{x'}_2 =\n\\begin{pmatrix}\n   \\\\\n   \\\\\n   b\\\\\n   \\vdots\\\\\n   b\\end{pmatrix}\n\\rightarrow\n\\pmb{x}_2 =\n\\begin{pmatrix}\n0\\\\\n0\\\\\n1\\\\\n\\vdots\\\\\n1\n\\end{pmatrix}\n\\]model can written :\\[\n\\begin{aligned}\ny_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2)\\\\\n\\mu_i &= \\alpha + \\beta_2 x_{2,}\n\\end{aligned}\n\\]incorporating variable model, interesting outcomes arise. Since \\(x_{2,} = 0\\) observation belongs group , mean group (\\(\\mu_i = \\mu_a\\)) determined \\(\\mu_a = \\alpha + \\beta \\times 0 = \\alpha\\). Consequently, intercept represents mean value first group. hand, observation group b, mean group b (\\(\\mu_i = \\mu_b\\)) given \\(\\mu_b = \\alpha + \\beta \\times 1 = \\alpha + \\beta\\). ’s important recall \\(\\mu_a = \\alpha\\). substituting \\(\\alpha\\) \\(\\mu_a\\), equation \\(\\beta = \\mu_b - \\mu_a\\) obtained, indicating slope represents difference means two groups – key statistic t-test.Let confirm using dataset Chapter 4 (fish body length two lakes):lm() function, provide “categorical” variable character factor form, automatically converted binary (0/1) variable internally. means can include variable formula conducting standard regression analysis. lm() function takes care conversion process, allowing seamlessly incorporate categorical variables lm() function.estimated coefficient Lake b (lakeb) identical difference group means. can compare statistics (t value Pr(>|t|) output t.test() well:t-statistic p-value match lm() output.","code":"\nlibrary(tidyverse)\ndf_fl <- read_csv(\"data_raw/data_fish_length.csv\")\nprint(df_fl)## # A tibble: 100 × 3\n##    lake  length unit \n##    <chr>  <dbl> <chr>\n##  1 a       10.8 cm   \n##  2 a       13.6 cm   \n##  3 a       10.1 cm   \n##  4 a       18.6 cm   \n##  5 a       14.2 cm   \n##  6 a       10.1 cm   \n##  7 a       14.7 cm   \n##  8 a       15.6 cm   \n##  9 a       15   cm   \n## 10 a       11.9 cm   \n## # ℹ 90 more rows\n# group means\nv_mu <- df_fl %>% \n  group_by(lake) %>% \n  summarize(mu = mean(length)) %>% \n  pull(mu)\n\n# mu_a: should be identical to intercept\nv_mu[1]## [1] 13.35\n# mu_b - mu_a: should be identical to slope\nv_mu[2] - v_mu[1]## [1] 2.056\n# in lm(), letters are automatically converted to 0/1 binary variable.\n# alphabetically ordered (in this case, a = 0, b = 1)\nm <- lm(length ~ lake,\n        data = df_fl)\n\nsummary(m)## \n## Call:\n## lm(formula = length ~ lake, data = df_fl)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -8.150 -2.156  0.022  2.008  7.994 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  13.3500     0.4477  29.819   <2e-16 ***\n## lakeb         2.0560     0.6331   3.247   0.0016 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.166 on 98 degrees of freedom\n## Multiple R-squared:  0.09715,    Adjusted R-squared:  0.08794 \n## F-statistic: 10.55 on 1 and 98 DF,  p-value: 0.001596\nlake_a <- df_fl %>% \n  filter(lake == \"a\") %>% \n  pull(length)\n\nlake_b <- df_fl %>% \n  filter(lake == \"b\") %>% \n  pull(length)\n\nt.test(x = lake_b, y = lake_a)## \n##  Welch Two Sample t-test\n## \n## data:  lake_b and lake_a\n## t = 3.2473, df = 95.846, p-value = 0.001606\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  0.7992071 3.3127929\n## sample estimates:\n## mean of x mean of y \n##    15.406    13.350"},{"path":"linear-model.html","id":"multiple-group-case","chapter":"7 Linear Model","heading":"7.1.2 Multiple-Group Case","text":"argument applies ANOVA. ANOVA, deal two groups explanatory variable. handle , can convert group variable multiple dummy variables. instance, variable \\(\\pmb{x'}_2 = \\{, b, c\\}\\), can convert \\(x_2 = \\{0, 1, 0\\}\\) (\\(b \\rightarrow 1\\) others \\(0\\)) \\(\\pmb{x}_3 = \\{0, 0, 1\\}\\) (\\(c \\rightarrow 1\\) others \\(0\\)). Thus, model formula :\\[\n\\begin{aligned}\ny_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2)\\\\\n\\mu_i &= \\alpha + \\beta_{2} x_{2,}  + \\beta_{3} x_{3,}\n\\end{aligned}\n\\]substitute dummy variables, :\\[\n\\begin{aligned}\n\\mu_a &= \\alpha + \\beta_2 \\times 0 + \\beta_3 \\times 0 = \\alpha &&\\text{}~x_{2,}~\\text{}~x_{3,}~\\text{zero}\\\\\n\\mu_b &= \\alpha + \\beta_2 \\times 1 + \\beta_3 \\times 0 = \\alpha + \\beta_2 &&x_{2,} = 1~\\text{}~x_{3,}=0\\\\\n\\mu_c &= \\alpha + \\beta_2 \\times 0 + \\beta_3 \\times 1 = \\alpha + \\beta_3 &&x_{2,} = 0~\\text{}~x_{3,}=1\\\\\n\\end{aligned}\n\\]Therefore, group serves reference, \\(\\beta\\)s represent deviations reference group. Now, let attempt ANOVA dataset provided Chapter 5:, lm() function converts categorical variable dummy variables internally. Compare mean differences estimated parameters:Also, report F-statistic (5.305) p-value (0.005961). Compare aov() output:results identical.","code":"\ndf_anova <- read_csv(\"data_raw/data_fish_length_anova.csv\")\nprint(df_anova)## # A tibble: 150 × 3\n##    lake  length unit \n##    <chr>  <dbl> <chr>\n##  1 a       10.8 cm   \n##  2 a       13.6 cm   \n##  3 a       10.1 cm   \n##  4 a       18.6 cm   \n##  5 a       14.2 cm   \n##  6 a       10.1 cm   \n##  7 a       14.7 cm   \n##  8 a       15.6 cm   \n##  9 a       15   cm   \n## 10 a       11.9 cm   \n## # ℹ 140 more rows\n# group means\nv_mu <- df_fl %>% \n  group_by(lake) %>% \n  summarize(mu = mean(length)) %>% \n  pull(mu)\n\nprint(c(v_mu[1], # mu_a: should be identical to intercept\n        v_mu[2] - v_mu[1], # mu_b - mu_a: should be identical to the slope for lakeb\n        v_mu[3] - v_mu[1])) # mu_c - mu_a: should be identical to the slope for lakec## [1] 13.350  2.056     NA\n# lm() output\nm <- lm(length ~ lake,\n        data = df_anova)\n\nsummary(m)## \n## Call:\n## lm(formula = length ~ lake, data = df_anova)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -8.150 -1.862 -0.136  1.846  7.994 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  13.3500     0.4469  29.875  < 2e-16 ***\n## lakeb         2.0560     0.6319   3.253  0.00141 ** \n## lakec         1.1160     0.6319   1.766  0.07948 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.16 on 147 degrees of freedom\n## Multiple R-squared:  0.06732,    Adjusted R-squared:  0.05463 \n## F-statistic: 5.305 on 2 and 147 DF,  p-value: 0.005961\nm_aov <- aov(length ~ lake,\n             data = df_anova)\n\nsummary(m_aov)##              Df Sum Sq Mean Sq F value  Pr(>F)   \n## lake          2  105.9   52.97   5.305 0.00596 **\n## Residuals   147 1467.6    9.98                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"linear-model.html","id":"the-common-structure","chapter":"7 Linear Model","heading":"7.1.3 The Common Structure","text":"examples show t-test, ANOVA, regression model structure:\\[\ny_i = \\text{(deterministic component)} + \\text{(stochastic component)}\n\\] framework Linear Model, deterministic component expressed \\(\\alpha + \\sum_k \\beta_k x_{k,}\\) stochastic component (random error) expressed Normal distribution. structure makes several assumptions. main assumptions:Linearity: relationship explanatory variables response variable linear. means effect explanatory variable response variable additive constant.Linearity: relationship explanatory variables response variable linear. means effect explanatory variable response variable additive constant.Independence: observations independent . assumption implies values response variable one observation influence values observations.Independence: observations independent . assumption implies values response variable one observation influence values observations.Homoscedasticity: variance response variable constant across levels explanatory variables. words, spread dispersion response variable values explanatory variables.Homoscedasticity: variance response variable constant across levels explanatory variables. words, spread dispersion response variable values explanatory variables.Normality: error term follows normal distribution level explanatory variables. assumption important many statistical tests estimators used linear model based assumption normality.Normality: error term follows normal distribution level explanatory variables. assumption important many statistical tests estimators used linear model based assumption normality.multicollinearity: explanatory variables highly correlated . Multicollinearity can lead problems estimating coefficients accurately can make interpretation difficult.multicollinearity: explanatory variables highly correlated . Multicollinearity can lead problems estimating coefficients accurately can make interpretation difficult.Violations assumptions can affect validity reliability results obtained Linear Model.","code":""},{"path":"linear-model.html","id":"combine-multiple-types-of-variables","chapter":"7 Linear Model","heading":"7.2 Combine Multiple Types of Variables","text":"","code":""},{"path":"linear-model.html","id":"iris-example","chapter":"7 Linear Model","heading":"7.2.1 iris Example","text":"previous section clarified t-test, ANOVA, regression common model structure; therefore, categorical continuous variables can included model. , let use iris data (available R default) show example. data set comprises continuous (Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) categorical variables (Species):, let model Petal.Length function Petal.Width Species:Please note model output include setosa used reference group ((Intercept)).want obtain predicted values, can use predict() function. make predictions, need provide new data frame containing values explanatory variables prediction.plotting predicted values observed data points, can visually evaluate accuracy model assess goodness fit.\nFigure 7.1: Example model fitting iris data. Points observations, lines model predictions.\n","code":"\n# convert the data format to tibble\niris <- as_tibble(iris)\nprint(iris)## # A tibble: 150 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n##  1          5.1         3.5          1.4         0.2 setosa \n##  2          4.9         3            1.4         0.2 setosa \n##  3          4.7         3.2          1.3         0.2 setosa \n##  4          4.6         3.1          1.5         0.2 setosa \n##  5          5           3.6          1.4         0.2 setosa \n##  6          5.4         3.9          1.7         0.4 setosa \n##  7          4.6         3.4          1.4         0.3 setosa \n##  8          5           3.4          1.5         0.2 setosa \n##  9          4.4         2.9          1.4         0.2 setosa \n## 10          4.9         3.1          1.5         0.1 setosa \n## # ℹ 140 more rows\ndistinct(iris, Species)## # A tibble: 3 × 1\n##   Species   \n##   <fct>     \n## 1 setosa    \n## 2 versicolor\n## 3 virginica\n# develop iris model\nm_iris <- lm(Petal.Length ~ Petal.Width + Species,\n             data = iris)\n\nsummary(m_iris)## \n## Call:\n## lm(formula = Petal.Length ~ Petal.Width + Species, data = iris)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.02977 -0.22241 -0.01514  0.18180  1.17449 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)        1.21140    0.06524  18.568  < 2e-16 ***\n## Petal.Width        1.01871    0.15224   6.691 4.41e-10 ***\n## Speciesversicolor  1.69779    0.18095   9.383  < 2e-16 ***\n## Speciesvirginica   2.27669    0.28132   8.093 2.08e-13 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.3777 on 146 degrees of freedom\n## Multiple R-squared:  0.9551, Adjusted R-squared:  0.9542 \n## F-statistic:  1036 on 3 and 146 DF,  p-value: < 2.2e-16\n# create a data frame for prediction\n# variable names must be identical to the original dataframe for analysis\nn_rep <- 100\ndf_pred <- tibble(Petal.Width = rep(seq(min(iris$Petal.Width),\n                                        max(iris$Petal.Width),\n                                        length = n_rep),\n                                    n_distinct(iris$Species)),\n                  Species = rep(unique(iris$Species),\n                                each = n_rep))\n\n# make prediction based on supplied values of explanatory variables\ny_pred <- predict(m_iris,\n                  newdata = df_pred)\n\ndf_pred <- df_pred %>% \n  mutate(y_pred = y_pred)\n\nprint(df_pred)## # A tibble: 300 × 3\n##    Petal.Width Species y_pred\n##          <dbl> <fct>    <dbl>\n##  1       0.1   setosa    1.31\n##  2       0.124 setosa    1.34\n##  3       0.148 setosa    1.36\n##  4       0.173 setosa    1.39\n##  5       0.197 setosa    1.41\n##  6       0.221 setosa    1.44\n##  7       0.245 setosa    1.46\n##  8       0.270 setosa    1.49\n##  9       0.294 setosa    1.51\n## 10       0.318 setosa    1.54\n## # ℹ 290 more rows\niris %>% \n  ggplot(aes(x = Petal.Width,\n             y = Petal.Length,\n             color = Species)) +\n  geom_point(alpha = 0.5) +\n  geom_line(data = df_pred,\n            aes(y = y_pred)) # redefine y values for lines; x and color are inherited from ggplot()"},{"path":"linear-model.html","id":"level-of-measurament","chapter":"7 Linear Model","heading":"7.2.2 Level of Measurament","text":"Categorical Variables: Qualitative discrete characteristics fall specific categories groups. numerical value order associated . Examples: gender (male/female), marital status (single/married/divorced), color (red/green/blue).Categorical Variables: Qualitative discrete characteristics fall specific categories groups. numerical value order associated . Examples: gender (male/female), marital status (single/married/divorced), color (red/green/blue).Ordinal Variables: Categories levels natural order ranking. Examples: survey ratings (e.g., Likert scale), education levels (e.g., high school, bachelor’s, master’s), socioeconomic status (e.g., low, medium, high).Ordinal Variables: Categories levels natural order ranking. Examples: survey ratings (e.g., Likert scale), education levels (e.g., high school, bachelor’s, master’s), socioeconomic status (e.g., low, medium, high).Interval Variables: Numerical values representing continuous scale intervals values meaningful consistent. Unlike ordinal variables, interval variables equally spaced intervals adjacent values, allowing meaningful arithmetic operations like addition subtraction. However, interval variables contain natural zero point; zero arbitrary merely reference point. Examples: temperatureInterval Variables: Numerical values representing continuous scale intervals values meaningful consistent. Unlike ordinal variables, interval variables equally spaced intervals adjacent values, allowing meaningful arithmetic operations like addition subtraction. However, interval variables contain natural zero point; zero arbitrary merely reference point. Examples: temperatureRatio Variables: Numerical variable shares characteristics interval variables also meaningful absolute zero point. Thus, type variables complete absence measured attribute/quantity. Examples: height (centimeters), weight (kilograms), time (seconds), distance (meters), income (dollars).Ratio Variables: Numerical variable shares characteristics interval variables also meaningful absolute zero point. Thus, type variables complete absence measured attribute/quantity. Examples: height (centimeters), weight (kilograms), time (seconds), distance (meters), income (dollars).carry meaningful arithmetic operations categorical ordinal variables; therefore, treat variables “character” “factors” R. R recognizes intervals different groups levels variables meaningful numerical sense.contrast, interval ratio variables considered “numerical” variables R. variables possess meaningful numerical scale equal intervals, case ratio variables, true zero point.","code":""},{"path":"linear-model.html","id":"laboratory-6","chapter":"7 Linear Model","heading":"7.3 Laboratory","text":"","code":""},{"path":"linear-model.html","id":"normality-assumption","chapter":"7 Linear Model","heading":"7.3.1 Normality Assumption","text":"Linear Model framework assumes normality. verify validity normality assumption, typically employ Shapiro-Wilk test (shapiro.test() R). Discuss whether test applied (1) response variable (2) model residuals. apply Shapiro-Wilk test model m_iris. Type ?shapiro.test R console learn usage.","code":""},{"path":"linear-model.html","id":"model-interpretation","chapter":"7 Linear Model","heading":"7.3.2 Model Interpretation","text":"model depicted Figure 7.1 can interpreted follows: “species distinct intercept value.” Extract intercept values species m_iris object.","code":""},{"path":"linear-model.html","id":"alternative-model","chapter":"7 Linear Model","heading":"7.3.3 Alternative Model","text":"Let’s explore alternative model consider species’ identity. modification affect results? Develop model excluding Species variable create new figure resembling Figure 7.1, single regression line.","code":""},{"path":"generalized-linear-model.html","id":"generalized-linear-model","chapter":"8 Generalized Linear Model","heading":"8 Generalized Linear Model","text":"One key assumptions Linear Model framework Normality – error term follows normal distribution (Chapter 7). However, assumption frequently leads situations models predict unrealistic values response variable. chapter, introduce Generalized Linear Model (GLM) framework, enables flexible modeling.Key words: link function","code":""},{"path":"generalized-linear-model.html","id":"count-data","chapter":"8 Generalized Linear Model","heading":"8.1 Count Data","text":"","code":""},{"path":"generalized-linear-model.html","id":"plant-density","chapter":"8 Generalized Linear Model","heading":"8.1.1 Plant Density","text":"Recall data set garden plant counts (Chapter 3):\nFigure 8.1: Garden view sampling plots. White squares represent plots. Red dots indicate plant individuals counted.\nvariable plant count following characteristics:possible values discrete (count data) since counting number plant individuals plot.possible values discrete (count data) since counting number plant individuals plot.possible values always positive, counts negative.possible values always positive, counts negative.distribution appears non-symmetric around sample mean.distribution appears non-symmetric around sample mean.happens apply simple linear model assumes Normal distribution? Let model plant count function nitrate follows:\\[\n\\begin{aligned}\ny_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2)\\\\\n\\mu_i &= \\alpha + \\beta~\\text{nitrate}_i\n\\end{aligned}\n\\]seems satisfactory, anomalous behavior occurs plotting process figure. Plot data predicted values together.intersection predicted line x-axis suggests presence negative predicted values plant counts. However, logically implausible plant counts negative.occurrence stems nature model employed.Linear Model Normal error distribution, allow mean encompass wide range values, including negative values.Even mean predicted values intersect x-axis, consider possibility observations taking negative values, dictated nature Normal distribution (\\(\\mu_i + \\varepsilon_i\\) can negative even \\(\\mu_i > 0\\)), reality can never occur.","code":"\ndf_count <- read_csv(\"data_raw/data_garden_count.csv\")\nprint(df_count)## # A tibble: 30 × 3\n##     plot count nitrate\n##    <dbl> <dbl>   <dbl>\n##  1     1     1    14.6\n##  2     2     4    32.6\n##  3     3     6    29.3\n##  4     4     4    30.4\n##  5     5     7    34.1\n##  6     6     2    26.6\n##  7     7     2    30.2\n##  8     8     2    28.7\n##  9     9     3    32.5\n## 10    10     2    26  \n## # ℹ 20 more rows\nm_normal <- lm(count ~ nitrate,\n               df_count)\n\nsummary(m_normal)## \n## Call:\n## lm(formula = count ~ nitrate, data = df_count)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.9084 -0.9000 -0.2572  0.6134  3.0316 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -5.90545    1.42516  -4.144 0.000285 ***\n## nitrate      0.30286    0.05105   5.933 2.19e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.291 on 28 degrees of freedom\n## Multiple R-squared:  0.557,  Adjusted R-squared:  0.5411 \n## F-statistic:  35.2 on 1 and 28 DF,  p-value: 2.193e-06\n# extract estimates\nalpha <- coef(m_normal)[1] # intercept\nbeta <- coef(m_normal)[2] # slope\n\ndf_count %>% \n  ggplot(aes(x = nitrate,\n             y = count)) +\n  geom_point() +\n  geom_abline(intercept = alpha,\n              slope = beta)"},{"path":"generalized-linear-model.html","id":"poisson-model","chapter":"8 Generalized Linear Model","heading":"8.1.2 Poisson Model","text":"One possible approach consider assumption error term follows Poisson distribution. purpose discussion, let’s assume model based Poisson distribution appropriate. Unlike Normal distribution, Poisson distribution generates non-negative discrete values, makes suitable fit plant count variable. incorporate , can make following modifications model:\\[\n\\begin{aligned}\ny_i &\\sim \\text{Poisson}(\\lambda_i)\\\\\n\\log\\lambda_i &= \\alpha + \\beta~\\text{nitrate}_i\n\\end{aligned}\n\\]changes crucial:plant count, denoted \\(y_i\\), now assumed follow Poisson distribution parameter \\(\\lambda_i\\). ensures negative values \\(y_i\\) possible (.e., \\(\\Pr(y_i < 0) = 0\\)).mean \\(\\lambda_i\\) log-transformed expressing function nitrate. Given mean Poisson distribution negative, must restrict range \\(\\lambda_i\\) positive values. log-transformation guarantees positivity \\(\\lambda_i\\) (\\(\\lambda_i = \\exp(\\alpha + \\beta~\\text{nitrate}_i)\\)) irrespective values \\(\\alpha\\), \\(\\beta\\), \\(\\text{nitrate}_i\\).implementation Poisson model quite simple. following example, use function glm():major difference lm() argument family = \"poisson\". argument specifies probability distribution used; example, used Poisson distribution model count data. However, looking estimates…estimated intercept still negative…’s wrong? sweat, estimated parameters log scale; result translates :\\[\n\\lambda_i = \\exp(-4.05 + 0.17~\\text{nitrate}_i)\n\\]Therefore, even intercept estimated negative, mean \\(\\lambda_i\\) guaranteed non-negative.Another important difference lm() test-statistic long t-statistic – instead, glm() reports z-statistic. However, z-statistic similar t-statistic, defined \\(z=\\frac{\\hat{\\theta} - \\theta_0}{\\text{SE}(\\hat{\\theta})}\\) (\\(\\hat{\\theta} \\\\{\\hat{\\alpha}, \\hat{\\beta}\\}\\), \\(\\theta_0 = 0\\)). Therefore, can reproduce z-statistic dividing parameter estimates (\\(\\hat{\\alpha}\\) \\(\\hat{\\beta}\\)) standard errors:call statistic “z-statistic” known follow z-distribution (also known standardized Normal distribution \\(\\text{Normal}(0, 1)\\)), opposed Student’s t-distribution. reported p-value (Pr(>|z|)) estimated assumption, can seen follows.Notice p-values identical output Normal model – therefore, choice probability distribution critically affects results statistical analysis, often times, qualitatively (particular example though).Visualization provides convincing difference Normal Poisson models (Figure 8.2).\nFigure 8.2: Comparison predicted values Normal (broken, black) Poisson models (solid, red).\nClearly, Poisson model better job.","code":"\nm_pois <- glm(count ~ nitrate,\n              data = df_count,\n              family = \"poisson\")\nsummary(m_pois)## \n## Call:\n## glm(formula = count ~ nitrate, family = \"poisson\", data = df_count)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -4.04899    1.05326  -3.844 0.000121 ***\n## nitrate      0.17028    0.03467   4.911 9.06e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 48.598  on 29  degrees of freedom\n## Residual deviance: 19.073  on 28  degrees of freedom\n## AIC: 93.858\n## \n## Number of Fisher Scoring iterations: 5\n# parameter estimates and their SEs\ntheta <- coef(m_pois)\nse <- sqrt(diag(vcov(m_pois)))\nz_value <- theta / se\n\nprint(z_value)## (Intercept)     nitrate \n##   -3.844229    4.910935\n# estimate Pr(>|z|) using a standardized normal distribution\np_value <- (1 - pnorm(abs(z_value), mean = 0, sd = 1)) * 2\nprint(p_value)##  (Intercept)      nitrate \n## 1.209323e-04 9.064303e-07\n# make predictions\ndf_pred <- tibble(nitrate = seq(min(df_count$nitrate),\n                                max(df_count$nitrate),\n                                length = 100))\n\n# y_pois is exponentiated because predict() returns values in log-scale\ny_normal <- predict(m_normal, newdata = df_pred)\ny_pois <- predict(m_pois, newdata = df_pred) %>% exp()\n\ndf_pred <- df_pred %>% \n  mutate(y_normal,\n         y_pois)\n\n# figure\ndf_count %>% \n  ggplot(aes(x = nitrate,\n             y = count)) +\n  geom_point() +\n  geom_line(data = df_pred,\n            aes(y = y_normal),\n            linetype = \"dashed\") +\n  geom_line(data = df_pred,\n            aes(y = y_pois),\n            color = \"salmon\")"},{"path":"generalized-linear-model.html","id":"proportional-data","chapter":"8 Generalized Linear Model","heading":"8.2 Proportional Data","text":"","code":""},{"path":"generalized-linear-model.html","id":"mussel-egg-fertilization","chapter":"8 Generalized Linear Model","heading":"8.2.1 Mussel Egg Fertilization","text":"another type data Normal distribution suitable - proportional data. Let’s consider example dataset investigation conducted number fertilized eggs 30 eggs examined 100 female freshwater mussels. can download dataset .Male freshwater mussels release sperm water, drawn female mussels downstream. Considering nature fertilization process, female mussels expected higher probability fertilization surrounded greater number males. explore relationship, let’s load data R visualize .data frame contains variables ind_id (mussel individual ID), n_fertilized (number fertilized eggs), n_examined (number eggs examined), density (number mussels 1 m\\(^2\\) quadrat). visualize relationship proportion eggs fertilized density gradient, plot proportion fertilized eggs density gradient (Figure 8.3).\nFigure 8.3: Relationship proportion fertilized eggs mussel density surround.\nexample, response variable, represents number eggs fertilized, possesses following characteristics:Discreteness: variable takes discrete values rather continuous ones.Discreteness: variable takes discrete values rather continuous ones.Upper limit: number eggs fertilized bounded upper limit (case, 30).Upper limit: number eggs fertilized bounded upper limit (case, 30).discrete nature variable might initially lead one consider Poisson distribution potential candidate. However, due presence upper limit number fertilized eggs exceed number eggs examined, Poisson distribution suitable option.","code":"\ndf_mussel <- read_csv(\"data_raw/data_mussel.csv\")\nprint(df_mussel)## # A tibble: 100 × 4\n##    ind_id n_fertilized n_examined density\n##     <dbl>        <dbl>      <dbl>   <dbl>\n##  1      1            0         30       3\n##  2      2            1         30      14\n##  3      3            2         30      14\n##  4      4            6         30      19\n##  5      5            2         30      17\n##  6      6            0         30       8\n##  7      7            4         30      19\n##  8      8            0         30      11\n##  9      9            0         30       7\n## 10     10            0         30       7\n## # ℹ 90 more rows\n# calculate the proportion of fertilized eggs\ndf_mussel <- df_mussel %>% \n  mutate(prop_fert = n_fertilized / n_examined)\n\n# plot\ndf_mussel %>% \n  ggplot(aes(x = density,\n             y = prop_fert)) +\n  geom_point() +\n  labs(y = \"Proportion of eggs fertilized\",\n       x = \"Mussel density\")"},{"path":"generalized-linear-model.html","id":"binomial-model","chapter":"8 Generalized Linear Model","heading":"8.2.2 Binomial Model","text":"Binomial distribution natural appropriate choice modeling variable. Binomial distribution characterized two parameters: number trials (\\(N\\)) probability success (\\(p\\)). trial certain probability success, outcome trial either assigned value 1 (indicating success) 0 (indicating failure). given example, can consider “fertilization” successful outcome among 30 trials. Consequently, number eggs fertilized (\\(y_i\\)) individual mussel (\\(\\)) can described :\\[\ny_i \\sim \\text{Binomial}(N_i, p_i)\n\\]formulation adequately captures nature response variable, outcome Binomial distribution constrained within range \\(0\\) \\(N_i\\), aligning upper limit imposed number examined eggs.prediction fertilization probability \\(p_i\\) increase increasing mussel’s density – relate \\(p_i\\) mussel density? value \\(p_i\\) must constrained within range \\(0.0-1.0\\) “probability.” Given , log-transformation, used Poisson model, suitable choice. Instead, can use logit-transformation convert linear formula fall within range \\(0.0 - 1.0\\).\\[\n\\begin{aligned}\ny_i &\\sim \\text{Binomial}(N_i, p_i)\\\\\n\\log(\\frac{p_i}{1-p_i}) &= \\alpha + \\beta~\\text{density}_i\n\\end{aligned}\n\\]logit transformation, represented \\(\\log\\left(\\frac{p_i}{1-p_i}\\right)\\), guarantees values \\(p_i\\) confined within range 0.0 1.0. relationship expressed following equation:\\[\np_i = \\frac{\\exp(\\alpha + \\beta~\\text{density}_i)}{1 + \\exp(\\alpha + \\beta~\\text{density}_i)}\n\\]straightforward coding can verify .\nFigure 8.4: Logit-transformation ensures transformed variable fall range zero one.\nglm() function can used implement modeling approach. However, certain modifications need made response variable order ensure compatibility binomial distribution.contrast Normal Poisson models, response variable encoded using cbind(n_fertilized, n_examined - n_fertilized) function. cbind() function combines number successes (fertilized) failures (fertilized) single matrix. approach allows modeling data using binomial distribution, taking account successes total number trials within observation.Similar previous examples, summary() function provides estimates model parameters. applying summary() function fitted model, can obtain information coefficient estimates, standard errors, p-values, relevant statistics.result translates :\\[\n\\begin{aligned}\ny_i &\\sim \\text{Binomial}(N_i, p_i)\\\\\n\\log(\\frac{p_i}{1 - p_i}) &= -8.06 + 0.34~\\text{density}_i\n\\end{aligned}\n\\]make predictions, necessary back-transform predicted fertilization probability since estimated logit scale. back-transformation convert predicted values back original probability scale (\\(0.0-1.0\\)).Draw figure (Figure 8.5).\nFigure 8.5: Prediction Binomial model.\nPerfect.","code":"\n# x: produce 100 numbers from -100 to 100 (assume logit scale)\n# y: convert with inverse-logit transformation (ordinary scale)\ndf_test <- tibble(logit_x = seq(-10, 10, length = 100),\n                  x = exp(logit_x) / (1 + exp(logit_x)))\n\ndf_test %>% \n  ggplot(aes(x = logit_x,\n             y = x)) +\n  geom_point() +\n  geom_line() +\n  labs(y = \"x\",\n       x = \"logit(x)\")\nm_binom <- glm(cbind(n_fertilized, n_examined - n_fertilized) ~ density,\n               data = df_mussel,\n               family = \"binomial\")\ncbind(df_mussel$n_fertilized, df_mussel$n_examined - df_mussel$n_fertilized) %>% \n  head()##      [,1] [,2]\n## [1,]    0   30\n## [2,]    1   29\n## [3,]    2   28\n## [4,]    6   24\n## [5,]    2   28\n## [6,]    0   30\nsummary(m_binom)## \n## Call:\n## glm(formula = cbind(n_fertilized, n_examined - n_fertilized) ~ \n##     density, family = \"binomial\", data = df_mussel)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -8.06411    0.49944  -16.15   <2e-16 ***\n## density      0.34209    0.03077   11.12   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 233.752  on 99  degrees of freedom\n## Residual deviance:  70.744  on 98  degrees of freedom\n## AIC: 158.3\n## \n## Number of Fisher Scoring iterations: 6\n# make prediction\ndf_pred <- tibble(density = seq(min(df_mussel$density),\n                                max(df_mussel$density),\n                                length = 100))\n\n# y_binom is inv.logit-transformed because predict() returns values in logit-scale\ny_binom <- predict(m_binom, newdata = df_pred) %>% boot::inv.logit()\n\ndf_pred <- df_pred %>% \n  mutate(y_binom)\ndf_mussel %>% \n  ggplot(aes(x = density,\n             y = prop_fert)) +\n  geom_point() +\n  geom_line(data = df_pred,\n            aes(y = y_binom)) +\n  labs(y = \"Proportion of eggs fertilized\",\n       x = \"Mussel density\")"},{"path":"generalized-linear-model.html","id":"the-glm-framework","chapter":"8 Generalized Linear Model","heading":"8.3 The GLM Framework","text":"models, including assuming Normal distribution, fall category Generalized Linear Model (GLM) framework. framework follows common structure:\\[\n\\begin{aligned}\ny_i &\\sim \\text{D}(\\Theta)\\\\\n\\text{Link}(\\mbox{E}(y_i)) &= \\alpha + \\sum_k \\beta_k x_{k, }\n\\end{aligned}\n\\]structure, \\(\\Theta\\) represents parameter vector probability distribution, \\(\\text{D}(\\cdot)\\) \\(\\text{Link}(\\cdot)\\) denote chosen probability distribution associated link function, respectively. example, select Poisson distribution \\(\\text{D}(\\cdot)\\), corresponding \\(\\text{Link}(\\cdot)\\) function natural logarithm (log) (see Section 8.1.2). expected value \\(\\mbox{E}(y_i)\\) expressed function parameter(s) probability distribution \\(f(\\Theta)\\), related explanatory variables \\(x\\). instance, Normal Poisson models, model mean, Binomial model, model success probability. certain distributions, \\(f(\\Theta)\\) may function multiple parameters.GLM framework differs Linear Model framework offering greater flexibility choice probability distribution, achieved introduction Link function. function allows us constrain modeled parameter within specific range. non-normal distributions common natural phenomena, framework plays critical role modern statistical analysis.However, determine appropriate probability distribution? single correct answer question, clear factors consider—specifically, characteristics response variable. following criteria can particularly helpful:variable discrete?upper bound?sample variance far greater mean?assist selecting appropriate probability distribution modeling, provided concise decision tree graph. Please note choices presented frequently encountered ecological modeling, rather exhaustive list probability distributions (see Table 8.1). full list probability distributions can found (Wikipedia page)\nFigure 8.6: Tree chart choose probability distribution GLM analysis.\n\nTable 8.1: Common distributions used GLM analysis\n","code":""},{"path":"generalized-linear-model.html","id":"laboratory-7","chapter":"8 Generalized Linear Model","heading":"8.4 Laboratory","text":"","code":""},{"path":"generalized-linear-model.html","id":"glm-exercise","chapter":"8 Generalized Linear Model","heading":"8.4.1 GLM exercise","text":"publicly available stream fish distribution dataset accessible online repository. can download using link: Download Dataset. dataset comprises information number species found site (n_sp) several associated environmental variables, including distance sea (distance), catchment area (cat_area), environmental heterogeneity (hull_area). analyze dataset, develop Generalized Linear Model (GLM) appropriate probability distribution.","code":""},{"path":"generalized-linear-model.html","id":"effect-size","chapter":"8 Generalized Linear Model","heading":"8.4.2 Effect size","text":"Often, interested comparing effect sizes different explanatory variables. However, use raw environmental values, regression coefficients directly comparable due differing units. Recall regression coefficients represent increment \\(y\\) per unit increase \\(x\\). facilitate comparison, standardize explanatory variables subtracting means dividing standard deviations:\\[\n\\mbox{E}(y) = \\alpha + \\beta x \\rightarrow\n\\mbox{E}(y) = \\alpha' + \\beta' \\frac{(x - \\mu_x)}{\\sigma_x}\n\\], \\(\\alpha'\\) \\(\\beta'\\) represent intercept slope standardizing \\(x\\), respectively.Express \\(\\alpha'\\) \\(\\beta'\\) function \\(\\alpha\\) \\(\\beta\\), respectively.Express \\(\\alpha'\\) \\(\\beta'\\) function \\(\\alpha\\) \\(\\beta\\), respectively.function scale() perform standardization. Perform GLM analysis fish species richness standardized variables distance, cat_area, hull_area, identify influential variable among .function scale() perform standardization. Perform GLM analysis fish species richness standardized variables distance, cat_area, hull_area, identify influential variable among .","code":""},{"path":"likelihood.html","id":"likelihood","chapter":"9 Likelihood","heading":"9 Likelihood","text":"Chapter 8, introduced non-normal distributions represent count proportional data. glm() function used obtain parameter estimates, function achieve ? reality, least squares approach applicable normal distributions. Therefore, require alternative approach estimate parameters within GLM framework. section, introduce concept likelihood fundamental principle parameter estimation GLM framework.","code":""},{"path":"likelihood.html","id":"finding-the-more-likely","chapter":"9 Likelihood","heading":"9.1 Finding the “More Likely”","text":"least squares method, goal minimize sum squared errors, ensuring fitted model smallest deviation observed data points. However, approach applicable non-normal variables. cases, need rely “probability” observing data point numerical value.Let’s consider example count data, denoted \\(\\pmb{y} = \\{y_1, y_2, ..., y_N\\}\\). simplify matter, let’s focus specific data point, say \\(y_1 = 3\\). Assuming Poisson distribution, can calculate probability observing particular value using arbitrary parameter value mean, denoted \\(\\lambda\\). refresh memory, can refer back Poisson distribution discussed Chapter 3.\\[\n\\Pr(y = k) = \\frac{\\lambda^{k}\\exp(-\\lambda)}{k!}\n\\]equation represents probability variable \\(y\\) taking specific value \\(k\\). current example, interested case \\(k = 3\\). specifying value mean parameter \\(\\lambda\\), can compute probability associated value. Let’s try using \\(\\lambda = 3.5\\) calculation:\\[\n\\Pr(y_1 = 3) = \\frac{3.5^3 \\exp(-3.5)}{3!} \\approx 0.215\n\\]can calculate dpois() R:assume mean \\(3.5\\) Poisson distribution, probability observing value \\(3\\) approximately \\(0.215\\). can experiment different values \\(\\lambda\\) see potentially “better” value. exploring different values \\(\\lambda\\), can observe probability observing specific value changes. allows us assess value \\(\\lambda\\) provides better fit captures data accurately. following example explores \\(\\lambda = 0 - 10\\) \\(0.1\\) interval.Make plot:\nFigure 9.1: Relationship parameter \\(\\lambda\\) probability observing value three.\nexamining figure data, appears probability highest around \\(\\lambda = 3\\). confirm observation, let’s arrange data frame df_pois.reasoning sound probability \\(y_1\\) equal \\(3\\) maximized \\(\\lambda = 3\\). represents simplest form likelihood function denoted \\(L(\\lambda~|~y_1)\\), \\((\\cdot~|~y_1)\\) indicates \\(y_1\\) fixed given. evaluate probability observing fixed value \\(y_1\\), vary value \\(\\lambda\\).However, multiple values \\(y_i\\)? Let’s consider scenario \\(\\pmb{y} = \\{3, 2, 5\\}\\). cases, must account probability simultaneously observing values. events independent, probability observing multiple events can expressed product individual probabilities. Consequently, likelihood function takes following form:\\[\n\\begin{aligned}\nL(\\lambda~|~\\pmb{y}) &= \\Pr(y_1 = 3) \\times \\Pr(y_2 = 2) \\times \\Pr(y_3 = 5)\\\\\n&= \\frac{\\lambda^{3}\\exp(-\\lambda)}{3!} \\times \\frac{\\lambda^{2}\\exp(-\\lambda)}{2!} \\times \\frac{\\lambda^{5}\\exp(-\\lambda)}{5!}\\\\\n&= \\prod_i^3 \\frac{\\lambda^{y_i} \\exp(-\\lambda)}{y_i!}\n\\end{aligned}\n\\]Implement R:Similar previous example, let’s search suitable value \\(\\lambda\\) better captures observed data.\nFigure 9.2: Likelihood observing three count data points \\(\\pmb{y}\\) simultaneously.\nexercise interval \\(0.01\\), \\(\\lambda \\approx 3.33\\) identified suitable value. Interestingly, sample mean matches value:? See .","code":"\n# dpois()\n# the first argument is \"k\"\n# the second argument is \"lambda\"\ndpois(3, lambda = 3.5)## [1] 0.2157855\n# write the formula to confirm\n(p <- (3.5^3 * exp(-3.5)) / factorial(3))## [1] 0.2157855\n# change lambda from 0 to 10 by 0.1\nlambda <- seq(0, 10, by = 0.1)\n\n# probability\npr <- dpois(3, lambda = lambda)\n\n# create a data frame\ndf_pois <- tibble(y = 3,\n                  lambda = lambda,\n                  pr = pr)\n\nprint(df_pois)## # A tibble: 101 × 3\n##        y lambda       pr\n##    <dbl>  <dbl>    <dbl>\n##  1     3    0   0       \n##  2     3    0.1 0.000151\n##  3     3    0.2 0.00109 \n##  4     3    0.3 0.00333 \n##  5     3    0.4 0.00715 \n##  6     3    0.5 0.0126  \n##  7     3    0.6 0.0198  \n##  8     3    0.7 0.0284  \n##  9     3    0.8 0.0383  \n## 10     3    0.9 0.0494  \n## # ℹ 91 more rows\ndf_pois %>% \n  ggplot(aes(x = lambda,\n             y = pr)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"lambda\",\n       y = \"Pr(k = 3)\")\n# arrange() re-orders the dataframe based on the speficied column in an ascending order\n# desc() flips the order (descending)\n\ndf_pois %>% \n  arrange(desc(pr)) %>% \n  print()## # A tibble: 101 × 3\n##        y lambda    pr\n##    <dbl>  <dbl> <dbl>\n##  1     3    3   0.224\n##  2     3    3.1 0.224\n##  3     3    2.9 0.224\n##  4     3    3.2 0.223\n##  5     3    2.8 0.222\n##  6     3    3.3 0.221\n##  7     3    2.7 0.220\n##  8     3    3.4 0.219\n##  9     3    2.6 0.218\n## 10     3    3.5 0.216\n## # ℹ 91 more rows\n# try lambda = 3 for y = 3, 2, 5\npr <- dpois(c(3, 2, 5), lambda = 3)\nprint(pr)## [1] 0.2240418 0.2240418 0.1008188\n# probability of observing 3, 2, 5 simultaneously\n# with lambda = 3\nprod(pr)## [1] 0.005060573\n# lambda = 0 - 10 by 0.01\ny <- c(3, 2, 5)\nlambda <- seq(0, 10, by = 0.01)\n\n# sapply repeats the task in FUN\n# each element in \"X\" will be sequencially substituted in \"z\"\npr <- sapply(X = lambda,\n             FUN = function(z) prod(dpois(y, lambda = z)))\n\n# make a data frame and arrange by pr (likelihood)\ndf_pois <- tibble(lambda = lambda,\n                  pr = pr)\n\ndf_pois %>% \n  arrange(desc(pr)) %>% \n  print()## # A tibble: 1,001 × 2\n##    lambda      pr\n##     <dbl>   <dbl>\n##  1   3.33 0.00534\n##  2   3.34 0.00534\n##  3   3.32 0.00534\n##  4   3.35 0.00534\n##  5   3.31 0.00534\n##  6   3.36 0.00534\n##  7   3.3  0.00534\n##  8   3.37 0.00534\n##  9   3.29 0.00533\n## 10   3.38 0.00533\n## # ℹ 991 more rows\n# visualize\ndf_pois %>% \n  ggplot(aes(x = lambda,\n             y = pr)) +\n  geom_line() +\n  labs(y = \"Likelihood\")\nmean(c(3, 2, 5))## [1] 3.333333"},{"path":"likelihood.html","id":"maximum-likelihood-method","chapter":"9 Likelihood","heading":"9.2 Maximum Likelihood Method","text":"","code":""},{"path":"likelihood.html","id":"simple-case","chapter":"9 Likelihood","heading":"9.2.1 Simple case","text":"Finding parameter value maximizes probability observing set observed values known Maximum Likelihood Estimate (MLE). estimation method commonly employed GLM framework various statistical inference techniques. MLE approach applicable long likelihood can defined, making versatile method many statistical analyses.However, procedure employed previous section (Section 9.1) mathematically rigorous optimal value \\(\\lambda\\) dependent resolution interval. interval becomes smaller, possible discover even better value \\(\\lambda\\) ad infinitum due continuous nature.reliable approach identifying “peak” utilizing first-order derivative. first-order derivative represents slope likelihood function given \\(\\lambda\\) value. derivative equals zero, indicates reached peak likelihood function. can leverage mathematical principle determine maximum likelihood estimate \\(\\lambda\\). Since computing derivative product mathematically challenging, can apply logarithm transformation likelihood function. Recall \\(\\log ab = \\log + \\log b\\). taking logarithm, can convert product operation \\(\\prod\\) summation operation \\(\\sum\\). Specifically:\\[\n\\begin{aligned}\n\\log L(\\lambda | \\pmb{y}) &= \\log \\prod_i^N \\frac{\\lambda^{y_i} \\exp(-\\lambda)}{y_i!}\\\\\n&= \\log \\frac{\\lambda^{y_1} \\exp(-\\lambda)}{y_1!} + \\log \\frac{\\lambda^{y_2} \\exp(-\\lambda)}{y_2!} + ... + \\log \\frac{\\lambda^{y_N} \\exp(-\\lambda)}{y_N!}\\\\\n&= \\sum_i^N \\log \\frac{\\lambda^{y_i} \\exp(-\\lambda)}{y_i!}\\\\\n&= \\sum_i^N (y_i \\log \\lambda -\\lambda -\\log y_i!)\n\\end{aligned}\n\\]Taking first derivative \\(\\log L(\\lambda | \\pmb{y})\\):\\[\n\\begin{aligned}\n\\frac{\\partial\\log L(\\lambda|\\pmb{y})}{\\partial \\lambda} &= \\sum_i^N (\\frac{y_i}{\\lambda} -1)\\\\\n&= \\frac{\\sum_i^N y_i}{\\lambda} - \\sum_i^N 1\\\\\n&= \\frac{\\sum_i^N y_i}{\\lambda} - N\\\\\n\\end{aligned}\n\\] obtain value \\(\\lambda\\) maximizes likelihood, set derivative equal zero:\\[\n\\begin{aligned}\n\\frac{\\partial\\log L(\\lambda|\\pmb{y})}{\\partial \\lambda} &= \\frac{\\sum_i^N y_i}{\\lambda} - N = 0\\\\\n\\lambda &= \\frac{\\sum_i^N y_i}{N}\n\\end{aligned}\n\\]Interesting – maximum likelihood estimate \\(\\lambda\\) equal sample mean. reason use sample mean estimate \\(\\lambda\\) Chapter 3.","code":""},{"path":"likelihood.html","id":"general-case","chapter":"9 Likelihood","heading":"9.2.2 General case","text":"practice, obtaining MLE analytically can highly complex often infeasible. Consequently, GLM framework relies numerical search algorithms approximate MLE estimates (see Ben Bolker’s book chapter full details; particular, quasi-Newton methods). Within GLMs, common establish relationships mean \\(\\lambda\\) explanatory variables order investigate influences. instance:\\[\n\\begin{aligned}\ny_i &\\sim \\mbox{Poisson}(\\lambda_i)\\\\\n\\log(\\lambda_i) &= \\alpha + \\sum_k \\beta_k x_k\n\\end{aligned}\n\\]\\(x_k\\) represents \\(k\\)th explanatory variable. translates :\\[\n\\log L(\\alpha, \\beta_k|\\pmb{y}) = \\sum_i \\left[y_i (\\alpha + \\sum_k\\beta_k x_k) - \\exp(\\alpha + \\sum_k \\beta_k x_k) - \\log y_i! \\right]\n\\]solving \\(k+1\\) partial derivatives:\\[\n\\begin{aligned}\n\\frac{\\partial \\log L(\\alpha, \\beta_k|\\pmb{y})}{\\partial \\alpha} &= 0\\\\\n\\frac{\\partial \\log L(\\alpha, \\beta_k|\\pmb{y})}{\\partial \\beta_1} &= 0\\\\\n&...\\\\\n\\frac{\\partial \\log L(\\alpha, \\beta_k|\\pmb{y})}{\\partial \\beta_k} &= 0\\\\\n\\end{aligned}\n\\]Thus, function glm() lot us! log likelihood fitted model can extracted logLik():","code":"\n# load garden plant data\n# df_count <- read_csv(\"data_raw/data_garden_count.csv\")\n\nm_pois <- glm(count ~ nitrate,\n              data = df_count,\n              family = \"poisson\")\n\nlogLik(m_pois)## 'log Lik.' -44.92912 (df=2)"},{"path":"likelihood.html","id":"laboratory-8","chapter":"9 Likelihood","heading":"9.3 Laboratory","text":"","code":""},{"path":"likelihood.html","id":"binomial-distribution","chapter":"9 Likelihood","heading":"9.3.1 Binomial Distribution","text":"binomial distribution probability distribution describes number successes fixed number independent Bernoulli trials, trial probability success, denoted \\(p\\). probability mass function (PMF) binomial distribution given :\\[\n\\Pr(y = k) = \\begin{pmatrix} N\\\\ k \\end{pmatrix} p^k(1 - p)^{N-k}\n\\]\\(\\begin{pmatrix} N\\\\ k \\end{pmatrix}\\) binomial coefficient, representing number ways choose \\(k\\) successes \\(N\\) trials.function dbinom() calculates \\(\\Pr(y = k)\\) binomial distribution. Using function, calculate likelihood observing vector \\(\\pmb{y} = \\{2, 2, 0, 0, 3, 1, 3, 3, 4, 3\\}\\) following values \\(p\\): \\(p \\\\{0, 0.01, 0.02, ..., 1.0\\}\\), \\(N = 10\\).function dbinom() calculates \\(\\Pr(y = k)\\) binomial distribution. Using function, calculate likelihood observing vector \\(\\pmb{y} = \\{2, 2, 0, 0, 3, 1, 3, 3, 4, 3\\}\\) following values \\(p\\): \\(p \\\\{0, 0.01, 0.02, ..., 1.0\\}\\), \\(N = 10\\).set \\(p\\) examined, find parameter value maximizes likelihood.set \\(p\\) examined, find parameter value maximizes likelihood.Using \\(p\\) maximizes likelihood, calculate \\(N \\times p\\). Compare value sample mean vector \\(\\pmb{y}\\).Using \\(p\\) maximizes likelihood, calculate \\(N \\times p\\). Compare value sample mean vector \\(\\pmb{y}\\).","code":""},{"path":"likelihood.html","id":"normal-distribution-1","chapter":"9 Likelihood","heading":"9.3.2 Normal Distribution","text":"normal distribution continuous probability distribution describes distribution continuous random variable. characterized mean \\(\\mu\\) standard deviation \\(\\sigma\\). probability density function (PDF) normal distribution given :\\[\nf(y) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\left(-\\frac{(y - \\mu)^2}{2 \\sigma^2}\\right)\n\\]normal distribution deals continuous variables, need technique derive Maximum Likelihood Estimation (MLE). purpose, consider \\(f(y)\\) height rectangle simplify problem, use \\(\\Delta y\\) base length. probability observing value \\(y = k\\), denoted \\(\\Pr(y = k)\\), can approximated area rectangle:\\[\n\\Pr(y = k) = \\text{rectangle area} = f(y) \\times \\Delta y\n\\]Assuming \\(N\\) observations, likelihood observing entire vector \\(\\pmb{y}\\) given :\\[\nL(\\mu, \\sigma | \\pmb{y}) = \\prod_{}^N f(y_i) \\Delta y\n\\]Yield logarithm \\(L(\\mu, \\sigma | \\pmb{y})\\).Yield logarithm \\(L(\\mu, \\sigma | \\pmb{y})\\).Derive first-order derivative log-likelihood function respect \\(\\mu\\) solve \\(\\mu\\) setting \\(\\frac{\\partial \\log L(\\mu, \\sigma)}{\\partial \\mu} = 0\\). Assume \\(\\sigma\\) constant.Derive first-order derivative log-likelihood function respect \\(\\mu\\) solve \\(\\mu\\) setting \\(\\frac{\\partial \\log L(\\mu, \\sigma)}{\\partial \\mu} = 0\\). Assume \\(\\sigma\\) constant.","code":""},{"path":"model-comparison.html","id":"model-comparison","chapter":"10 Model Comparison","heading":"10 Model Comparison","text":"acquired knowledge constructing models evaluating performance. However, challenge lies selecting “optimal” model pool potential candidates. may think can employ metrics like Coefficient Determination (\\(\\mbox{R}^2\\)) compare choose superior models; however, predicament arises metrics tend favor models larger number explanatory variables, even variables actually irrelevant contribute meaningfully model’s effectiveness. Chapter, introduce popular methods balance trade-goodness fit model’s complexity.Key words: adjusted \\(\\mbox{R}^2\\), likelihood ratio test, Akaike’s Information Criterion","code":""},{"path":"model-comparison.html","id":"model-fit-and-complexity","chapter":"10 Model Comparison","heading":"10.1 Model Fit and Complexity","text":"field biological sciences, common encounter multiple hypotheses need exploratory analysis. instances, becomes necessary develop various models, corresponding specific hypothesis, subsequently compare respective performances. crucial aspect lies quantify “performance” models, acknowledging term can somewhat ambiguous. determination model performance ultimately dictates model superior, profoundly impacts conclusions drawn research. Thus, significance process overstated.One potential approach utilize Coefficient Determination (Chapter 6) measure model performance. metrics provide indication proportion variation response variable explained model, making reasonable options. However, situations measures meaningful model comparison.illustrate , let’s consider following simulated data. advantage using simulated data already know correct outcomes, enabling us validate verify analysis results.\nFigure 10.1: Simulated data\naforementioned code generated response variable, denoted \\(y\\), function variable \\(x_1\\) (\\(y_i = \\beta_0 + \\beta_1 x_1 + \\varepsilon_i\\)). values intercept slope set 0.1 0.5 respectively. Given knowledge data generated, can establish “correct” model. Now, let’s assess performance model:estimated values intercept slope model approximately 0.08 0.5 respectively, reasonably close true values. coefficient determination, indicating proportion variation response variable explained model, 0.467.Now, let’s consider scenario include another variable, x2, completely irrelevant.Surprisingly, \\(\\mbox{R}^2\\) value increased 0.469 even though model includes unnecessary variable.outcome occurs \\(\\mbox{R}^2\\) value consider complexity model number parameters employed. fact, can increase number parameters indefinitely, ’s easy construct model \\(\\mbox{R}^2\\) value \\(1.00\\). achieved assigning \\(N\\) parameters \\(N\\) data points, effectively making parameters equivalent response variable . However, approach undesirable since model essentially explains response variable response variable (\\(y = y\\)), self-evident. Therefore, aim develop meaningful model, essential employ alternative measures model comparisons.","code":"\nset.seed(1) # for reproducibility\n\n# hypothetical sample size\nn <- 100\n\n# true intercept and slope\nb <- c(0.1, 0.5)\n\n# hypothetical explanatory variable\nx1 <- rnorm(n = n, mean = 0, sd = 1)\n\n# create a design matrix\nX <- model.matrix(~x1)\n\n# expected values of y is a function of x\n# %*% means matrix multiplication\n# y = X %*% b equals y = b[1] + b[2] * x\n# recall linear algebra\ny_hat <- X %*% b\n\n# add normal errors\ny <- rnorm(n = n, mean = y_hat, sd = 0.5)\n\n# plot\ndf0 <- tibble(y = y, x1 = x1)\n\ndf0 %>% \n  ggplot(aes(y = y,\n             x = x1)) + \n  geom_point()\n# correct model used to generate the data\nm1 <- lm(y ~ x1, data = df0)\nsummary(m1)## \n## Call:\n## lm(formula = y ~ x1, data = df0)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.93842 -0.30688 -0.06975  0.26970  1.17309 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.08115    0.04849   1.673   0.0974 .  \n## x1           0.49947    0.05386   9.273 4.58e-15 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.4814 on 98 degrees of freedom\n## Multiple R-squared:  0.4674, Adjusted R-squared:  0.4619 \n## F-statistic: 85.99 on 1 and 98 DF,  p-value: 4.583e-15\n# add a column x2 which is irrelevant for y\ndf0 <- df0 %>% \n  mutate(x2 = rnorm(n))\n\n# add x2 to the model\nm2 <- lm(y ~ x1 + x2, data = df0)\nsummary(m2)## \n## Call:\n## lm(formula = y ~ x1 + x2, data = df0)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.92274 -0.32325 -0.07538  0.26640  1.16629 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.08178    0.04870   1.679   0.0963 .  \n## x1           0.49996    0.05408   9.244 5.75e-15 ***\n## x2          -0.02294    0.04697  -0.488   0.6264    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.4833 on 97 degrees of freedom\n## Multiple R-squared:  0.4687, Adjusted R-squared:  0.4577 \n## F-statistic: 42.78 on 2 and 97 DF,  p-value: 4.793e-14"},{"path":"model-comparison.html","id":"comparison-metrics","chapter":"10 Model Comparison","heading":"10.2 Comparison Metrics","text":"various measures exist evaluate model performance, discuss three fundamental ones: Adjusted \\(\\mbox{R}^2\\), likelihood ratio test, Akaike’s Information Criterion (AIC). first two focus well model fits available dataset appropriately considering model complexity. hand, AIC assesses model’s capability make predictions unseen data (“--sample” prediction). universally optimal measure — depends specific research objective. goal assess model’s fit current data, first two measures similar ones can used. However, evaluating model’s predictive ability objective, AIC employed. crucial factor consider comparing models, essential ensure :models use dataset. Models fitted different data sets compared means.assume identical probability distribution among candidate models. example, normal model compared Poisson model.now describe rationale behind method.","code":""},{"path":"model-comparison.html","id":"adjusted-mboxr2","chapter":"10 Model Comparison","heading":"10.2.1 Adjusted \\(\\mbox{R}^2\\)","text":"Adjusted \\(\\mbox{R}^2\\) natural extension ordinary \\(\\mbox{R}^2\\). original formula \\(R^2\\) :\\[\nR^2 = 1 - \\frac{SS}{SS_0}\n\\], \\(SS\\) represents sum squares residuals (\\(\\sum \\varepsilon_i\\)), \\(SS_0\\) represents sum squares response variable (\\(\\sum (y_i - \\mu_y)\\), \\(\\mu_y\\) sample mean \\(y\\)). Adjusted \\(R^2\\) modifies formula account number parameters used:\\[\n\\begin{align*}\n\\text{Adj. }R^2 &= 1 - \\frac{SS/(N-k)}{SS_0/(N-1)} \\\\\n&= 1 - \\frac{\\sigma^2_{\\varepsilon}}{\\sigma^2_0}\n\\end{align*}\n\\]equation , \\(k\\) denotes number parameters utilized model. Hence, numerator indicates residual variance model, \\(\\sigma^2_{\\varepsilon}\\), denominator represents variance response variable, \\(y\\). can obtain values model objects m1 m2:Now can observe model m2 exhibits lower value adjusted \\(\\mbox{R}^2\\). discrepancy arises adjusted \\(\\mbox{R}^2\\) incorporates penalty term inclusion additional explanatory variables (\\(k\\)). order adjusted \\(\\mbox{R}^2\\) decrease, variable x2 must significantly decrease \\(\\sigma^2_{\\varepsilon}\\) beyond expected chance alone. words, x2 needs provide substantial improvement explaining response variable counterbalance penalty imposed inclusion extra variable.Note \\(\\mbox{R}^2\\) \\(\\mbox{Adj. R}^2\\) applicable normal models. case GLM framework (Chapter 8), utilize different measure called \\(\\mbox{R}^2_D\\), \\(D\\) represents “deviance.” Deviance (\\(D\\)) defined negative twice log-likelihood. estimate \\(\\mbox{R}^2_D\\) using deviance (\\(D\\)), comparable traditional \\(\\mbox{R}^2\\) tailored GLMs.\\[\n\\begin{align}\n\\mbox{R}^2_D &= 1 - \\frac{-2 \\ln L}{-2 \\ln L_0}\\\\\n&= 1 - \\frac{D}{D_0}\n\\end{align}\n\\]formula, \\(L\\) (\\(L_0\\)) represents likelihood fitted (null) model, \\(D\\) (\\(D_0\\)) represents deviance. several substitutes available \\(\\mbox{Adjusted R}^2\\). One method, known McFadden’s method 5, incorporates model complexity calculation using following formula:\\[\n\\mbox{Adj. R}^2_{D} = 1 - \\frac{\\ln L - k}{\\ln L_0}\n\\]","code":"\n# Adjusted R-square for m1 without x2\nsm1 <- summary(m1)\nprint(sm1$adj.r.squared)## [1] 0.4619164\n# Adjusted R-square for m2 with x2\nsm2 <- summary(m2)\nprint(sm2$adj.r.squared)## [1] 0.4577026"},{"path":"model-comparison.html","id":"likelihood-ratio-test","chapter":"10 Model Comparison","heading":"10.2.2 Likelihood Ratio Test","text":"Another possible approach likelihood ratio test, statistical test used compare fit two nested models. calculates log ratio likelihoods two models test statistic:\\[\n\\begin{align}\n\\mbox{LR} &= -2 \\ln \\frac{L_0}{L_1}\\\\\n&= -2 (\\ln L_0 - \\ln L_1)\n\\end{align}\n\\]likelihoods null alternative models denoted \\(L_0\\) \\(L_1\\) respectively. null hypothesis, test statistic \\(\\mbox{LR}\\) follows chi-square distribution, sample size approaches infinity. enables us determine whether inclusion new variable (variables) significantly enhanced model’s likelihood beyond expected chance.employ approach likelihood encounters problem similar \\(\\mbox{R}^2\\). log likelihood models can obtained using logLik() function:model m2 higher likelihood likelihood absolute measure model performance. However, likelihood ratio test overcomes limitation comparing observed increase likelihood expected chance. test statistic \\(\\mbox{LR}\\) follows chi-square distribution6, serves reference distribution. chi-square distribution single parameter - degrees freedom - , case, corresponds difference number model parameters used.model m1 utilized two parameters (intercept one slope), whereas model m2 employed three parameters (intercept two slopes). Consequently, test statistic assumed follow chi-square distribution one degree freedom. possible calculate p-value manually, anova() function can perform analysis:Pr(>Chi) value, represents p-value, significantly greater 0.05. indicates inclusion variable x2 improve model’s likelihood degree expected chance.likelihood ratio test can applied wide range models long capable estimating likelihood. Therefore, method highly versatile. However, one limitation competing models must nested, meaning null model subset alternative model. example provided, m1 includes explanatory variable x1, m2 incorporates x1 x2. However, m1 can viewed special case m2 slope x2 fixed zero. Thus, m1 can considered “nested” subset m2.","code":"\n# log likelihood: correct model\nlogLik(m1)## 'log Lik.' -67.77319 (df=3)\n# log likelihood: incorrect model\nlogLik(m2)## 'log Lik.' -67.6504 (df=4)\n# test = \"Chisq\" specifies a chi-square distribution\n# as a distribution of LR\nanova(m1, m2, test = \"Chisq\")## Analysis of Variance Table\n## \n## Model 1: y ~ x1\n## Model 2: y ~ x1 + x2\n##   Res.Df    RSS Df Sum of Sq Pr(>Chi)\n## 1     98 22.709                      \n## 2     97 22.653  1  0.055702   0.6253"},{"path":"model-comparison.html","id":"aic","chapter":"10 Model Comparison","heading":"10.2.3 AIC","text":"AIC takes different approach evaluating model performance compared measures. measures like \\(\\mbox{R}^2\\) primarily assess goodness fit available dataset used model fitting, AIC rooted information theoretic perspective. evaluates model’s ability predict unseen data population.mathematical details AIC beyond scope discussion, interested, reference Burnham Anderson 2002. However, crucial understand AIC differs fundamentally measures assesses model’s robustness new data added.Despite underlying mathematical complexity, AIC formula remarkably simple:\\[\n\\mbox{AIC} = 2k - 2\\ln L\n\\], \\(k\\) represents number model parameters, \\(L\\) likelihood. Lower AIC values indicate better predictability model. formula consists two terms: first term twice number parameters, second term model’s deviance. Thus, lower AIC value preferred. one might perceive variant \\(\\mbox{R}^2\\) similar measures, important note penalty term AIC derived Kullback–Leibler divergence.evident formula, AIC can estimated model valid likelihood, making method widely applicable, including GLMs. R, model’s AIC can computed using AIC() function.AIC value m2 higher, indicating second model lower capability predict unseen data. AIC several valuable features make useful criterion. instance, unlike likelihood tests, can used compare two models. Moreover, field biological sciences, prediction often holds significant interest, AIC particularly relevant. example, helps determine model performs best predicting unobserved sites within study region. AIC suggests selected “parsimonious” model lowest AIC provide best performance among competing models. Consequently, AIC finds widespread use biology, especially ecology.However, important exercise caution interpreting results. Specifically:model lowest AIC necessarily imply “true” model. limited explanatory variables collected, AIC can indicate model better among choices available. possible candidate models incorrect.model lowest AIC necessarily imply “true” model. limited explanatory variables collected, AIC can indicate model better among choices available. possible candidate models incorrect.AIC designed infer causal mechanisms. Variables causally related may enhance model’s ability predict unseen data. goal causal inference, different criterion, backdoor criterion, employed. example, AIC suitable analyzing controlled experiments aimed uncovering causal mechanisms biology.AIC designed infer causal mechanisms. Variables causally related may enhance model’s ability predict unseen data. goal causal inference, different criterion, backdoor criterion, employed. example, AIC suitable analyzing controlled experiments aimed uncovering causal mechanisms biology.often misuse metric, particularly overlooking second component (see arguments Arif MacNeil 2022). Therefore, crucial clearly define objective analysis ensure appropriate choice statistical methods.","code":"\n# AIC: correct model\nAIC(m1)## [1] 141.5464\n# AIC: incorrect model\nAIC(m2)## [1] 143.3008"},{"path":"model-comparison.html","id":"laboratory-9","chapter":"10 Model Comparison","heading":"10.3 Laboratory","text":"","code":""},{"path":"appendix-project.html","id":"appendix-project","chapter":"Appendix: Project","heading":"Appendix: Project","text":"","code":""},{"path":"appendix-project.html","id":"r-project","chapter":"Appendix: Project","heading":"R Project","text":"Throughout book, use ‘R Project’ core workspace unit. serves centralized location relevant materials, R scripts (.R) data files, consolidated. various ways organize project, prefer creating single ‘R Project’ set scripts data contribute specific publication (see example ). set ‘R Project,’ ’ll need RStudio base R software installed. R can used independently, highly recommend using conjunction RStudio due latter’s numerous features facilitate data analysis. can download R RStudio following websites:R (can choose CRAN mirror downloading)RStudioUpon launching RStudio, encounter interface depicted Figure 10.2. Initially, interface comprises three primary panels: Console, Environment, Files. Console write code execute calculations, data manipulation, analysis. Environment panel lists saved objects, Files panel displays files designated location computer.\nFigure 10.2: RStudio interface.\npasting script console, notice variable x appears environment panel.x object stores information. particular case, stored sequence 1 2 object x. stored information x, can access simply typing x.Excellent! possible work data using approach, important highlight generally regarded poor practice. project evolves, accumulate substantial amount material, writing 2000 lines code single project. Consequently, becomes essential implement effective code management strategies. presently handle code management?","code":"\nx <- c(1, 2)\nx## [1] 1 2"},{"path":"appendix-project.html","id":"script-editor","chapter":"Appendix: Project","heading":"Script Editor","text":"highly recommended manage scripts Editor instead. Editor draft fine-tune code executing Console. create space Editor, press Ctrl + Shift + N. new panel appear top left corner. Let’s type following script Editor. Please note key combination Ctrl + Shift + N assumes Windows Linux operating system. ’re using Mac, can use Command + Shift + N instead., hit Ctr + S save Editor file. RStudio prompt enter file name Editor7.","code":"\ny <- c(3, 4)"},{"path":"appendix-project.html","id":"file-name","chapter":"Appendix: Project","heading":"File Name","text":"also crucial establish consistent naming rules files. project progresses, number files within sub-directory may increase significantly. Without clear consistent naming rules files, navigating project can become challenging, also others involved. alleviate issue, consider implementing following recommendations file naming:SPACE. Use underscore.\n: script_week1.R\nDon’t: script week1.R\n: script_week1.RDon’t: script week1.RNO UPPERCASE. Use lowercase file names.\n: script_week1.R\nDon’t: Script_week1.R\n: script_week1.RDon’t: Script_week1.RBE CONSISTENT. Apply consistent naming rules within project.\n: R scripts figures always start common prefix, e.g., figure_XXX.R figure_YYY.R(XXX YYY specifies details).\nDon’t: R scripts figures start random text, e.g., XXX_fig.R , Figure_Y2.R , plotB.R.\n: R scripts figures always start common prefix, e.g., figure_XXX.R figure_YYY.R(XXX YYY specifies details).Don’t: R scripts figures start random text, e.g., XXX_fig.R , Figure_Y2.R , plotB.R.","code":""},{"path":"appendix-project.html","id":"structure-your-project","chapter":"Appendix: Project","heading":"Structure Your Project","text":"fail save haphazardly store code files computer, risk losing essential items becomes inevitable sooner later. mitigate risk, highly recommend gathering relevant materials within single R Project. create new R Project, follow procedure outlined :Go File > New Project top menuSelect New DirectorySelect New ProjectA new window appear, prompting name directory select location computer. choose location directory, click ‘Browse’ button. organizing project directories computer, highly recommend creating dedicated space. instance, computer, folder named /github store R Project directories.internal structure R Project crucial effective navigation ensures clarity others published. R Project typically consists various file types, .R, .csv, .rds, .Rmd, others. Without organized arrangement files, high probability encountering significant coding errors. Therefore, place great importance maintaining well-structured project. Table 10.1, present recommended subdirectory structure.Table 10.1:  Suggested internal structure R Project","code":""},{"path":"appendix-project.html","id":"robust-coding","chapter":"Appendix: Project","heading":"Robust coding","text":"mandatory, highly recommend using RStudio conjunction Git GitHub. Coding inherently prone errors, even skilled programmers make mistakes—without exception. However, crucial difference beginner advanced programmers lies ability develop robust coding practices accompanied self-error-detection system. Git plays vital role process. Throughout book, occasionally delve importance Git usage.","code":""},{"path":"appendix-git-github.html","id":"appendix-git-github","chapter":"Appendix: Git & GitHub","heading":"Appendix: Git & GitHub","text":"","code":""},{"path":"appendix-git-github.html","id":"git-github","chapter":"Appendix: Git & GitHub","heading":"Git & GitHub","text":"section, explain integrate Git GitHub R Studio. R Studio already excellent tool, becomes even powerful combined Git GitHub. Git free open-source distributed version control system tracks changes code work project, ensuring aware modifications made script () files. Tracking changes crucial avoid unintended errors code helps prevent creation redundant files. Although Git primarily local system, online counterpart called GitHub.set system, ’ll need follow steps. first step install Git computer:Windows: Install Git . installation process, ’ll prompted “Adjusting PATH environment.” Choose “Git command line also 3rd-party software” ’s already selected.Windows: Install Git . installation process, ’ll prompted “Adjusting PATH environment.” Choose “Git command line also 3rd-party software” ’s already selected.Mac: Follow instructions provided .Mac: Follow instructions provided .Git installed, open R Studio navigate Create Project > New Directory > New Project. see checkbox labeled “Create git repository,” make sure select creating new project (refer Figure 10.3). action display Git pane upper right panel R Studio, indicating Git integration enabled.\nFigure 10.3: installing Git, see Create git repository.\nunable find options mentioned , follow steps:Click Tools menu bar R Studio.Select Terminal choose New Terminal.terminal window, type git press Enter. command display location Git executable computer.Next, go back Tools menu bar select Global Options.options window, navigate Git/SVN.Look field labeled Git executable specify location Git executable obtained terminal.setting Git, can proceed create account GitHub. ’s free platform, choosing username, consider using lowercase letters including name make easier others find .R Studio seamlessly integrates Git GitHub, using Git client can provide additional visual aids. various options Git clients (see choices ), exercise, use GitHub Desktop. Install GitHub Desktop computer .","code":""},{"path":"appendix-git-github.html","id":"commit-push","chapter":"Appendix: Git & GitHub","heading":"Commit & Push","text":"","code":""},{"path":"appendix-git-github.html","id":"register-your-git-repo","chapter":"Appendix: Git & GitHub","heading":"Register Your Git repo","text":"open R Project ’ve just created git repository, follow steps:Open R Studio.Click File menu bar.Select Open Project.Browse directory created R Project select corresponding .Rproj file.R Studio open project, see project name top-right corner window.create sample .R file named sample.R within project, can use shortcut Ctrl + Shift + N follow steps:Click File menu bar.Select New File.Choose R Script.new script editor open, can write R code.Write code editor save sample.R clicking File selecting Save using shortcut Ctrl + S.Remember save file writing code.open GitHub Desktop app, locate application computer launch . opened, see GUI similar one depicted Figure 10.4.\nFigure 10.4: GUI GitHub Desktop\nClick “Current Repository” button located top left corner GitHub Desktop app interface. , select “Add” followed “Add existing repository” dropdown menu, shown Figure 10.5.\nFigure 10.5: Add dropdown top left\n","code":"\n## produce 100 random numbers that follows a normal distribution\nx <- rnorm(100, mean = 0, sd = 1)\n\n## estimate mean\nmean(x)\n\n## estimate SD\nsd(x)"},{"path":"appendix-git-github.html","id":"commit","chapter":"Appendix: Git & GitHub","heading":"Commit","text":"GitHub Desktop prompt enter local path Git repository. Browse select directory R Project located. selected directory, local Git repository appear list repositories left side bar GitHub Desktop, shown Figure 10.5.Now repository added GitHub Desktop, ready start committing files Git. Committing process recording changes made files Git. proceed committing, click Git repository GitHub Desktop. see following options information displayed:list changed files: section shows files modified since last commit.summary changes: section provides overview changes made selected files.Commit message: can enter descriptive message explaining changes made commit.providing meaningful commit message, can keep track changes made files easily understand purpose commit.\nFigure 10.6: see Create sample.R summary (required) bottom left\nstage, file (.R file) saved local directory recorded Git change history. commit changes, follow steps:First, select file(s) want commit. can checking checkboxes next files listed “Changes” section GitHub Desktop app.First, select file(s) want commit. can checking checkboxes next files listed “Changes” section GitHub Desktop app.selected files, go bottom left GitHub Desktop app window. see small box says summary (required) Create sample.R.selected files, go bottom left GitHub Desktop app window. see small box says summary (required) Create sample.R.box, enter descriptive title explains commit. important provide meaningful summary, proceed commit unless enter information. example, exercise, can enter Initial commit commit title. subsequent commits, recommended provide informative commit messages accurately describe changes made. can refer online resources recommendations write effective commit titles descriptions.box, enter descriptive title explains commit. important provide meaningful summary, proceed commit unless enter information. example, exercise, can enter Initial commit commit title. subsequent commits, recommended provide informative commit messages accurately describe changes made. can refer online resources recommendations write effective commit titles descriptions.entering commit title, click button says Commit master. action record changes made selected files Git.entering commit title, click button says Commit master. action record changes made selected files Git.Now, changes selected files successfully recorded Git commit. ’s important commit regularly provide meaningful commit messages can track changes understand evolution project needed.","code":""},{"path":"appendix-git-github.html","id":"push","chapter":"Appendix: Git & GitHub","heading":"Push","text":"’s important note changes currently recorded local computer’s Git repository published online GitHub repository. send local changes online GitHub repository, need use “Push” operation GitHub Desktop.make commit GitHub Desktop, prompted dialog box asking want push commit online repository, shown Figure 10.7. first push, won’t corresponding repository GitHub linked local repository. case, GitHub Desktop ask want publish GitHub. Please note “publishing” means making repository available GitHub, remain private unless explicitly choose make public.comfortable changes made want send online repository, click “Push” button GitHub Desktop. action push commits remote repository GitHub, effectively synchronizing local repository online repository.\nFigure 10.7: Push code, hit highlighted menu button\n","code":""},{"path":"appendix-git-github.html","id":"edit","chapter":"Appendix: Git & GitHub","heading":"Edit","text":"created file named sample.R per previous instructions. Now, let’s make minor change content sample.R file. Open sample.R file preferred text editor R Studio, make modification code. example, can update code ::Save changes sample.R file. Now, let’s go back GitHub Desktop see can handle change.making change sample.R file, open GitHub Desktop . notice GitHub Desktop automatically detects difference new old versions file. highlights specific parts script edited, can extremely helpful reviewing comparing changes code.feature provided GitHub Desktop makes easier track modifications understand specific updates made files. helps streamline coding process providing clear visual representation differences different versions code.\nFigure 10.8: Git detects edits codes\n","code":"\n## produce 100 random numbers that follows a normal distribution\nx <- rnorm(100, mean = 0, sd = 1)\n\n## estimate mean\nmean(x)\n\n## estimate SD\nsd(x)\n## produce 100 random numbers that follows a normal distribution\nx <- rnorm(100, mean = 0, sd = 1)\n\n## estimate mean\nmedian(x)\n\n## estimate SD\nvar(x)"},{"path":"appendix-data-structure.html","id":"appendix-data-structure","chapter":"Appendix: Data Structure","heading":"Appendix: Data Structure","text":"","code":""},{"path":"appendix-data-structure.html","id":"overview","chapter":"Appendix: Data Structure","heading":"Overview","text":"R 6 basic data types.character: \"aquatic\", \"ecology\" (order)factor: similar character, levels (alphabetically ordered default)numeric: 20.0 , 15.5integer: 3, 7logical: TRUE , FALSEcomplex: 1+2i (complex numbers real imaginary parts)elements form one following data structures.vector: series elements. single data type allowed single vectormatrix: elements organized rows columns. single data type allowed single matrixdata frame: looks similar matrix, allows different data types different columns","code":""},{"path":"appendix-data-structure.html","id":"vector","chapter":"Appendix: Data Structure","heading":"Vector","text":"","code":""},{"path":"appendix-data-structure.html","id":"create-vector","chapter":"Appendix: Data Structure","heading":"Create Vector","text":"examples atomic character vectors, numeric vectors, integer vectors, etc. many ways create vector data. following examples use c(), :, seq(), rep():","code":"\n#ex.1a manually create a vector using c()\nx <- c(1,3,4,8)\nx## [1] 1 3 4 8\n#ex.1b character\nx <- c(\"a\", \"b\", \"c\")\nx## [1] \"a\" \"b\" \"c\"\n#ex.1c logical\nx <- c(TRUE, FALSE, FALSE)\nx## [1]  TRUE FALSE FALSE\n#ex.2 sequence of numbers\nx <- 1:5\nx## [1] 1 2 3 4 5\n#ex.3a replicate same numbers or characters\nx <- rep(2, 5) # replicate 2 five times\nx## [1] 2 2 2 2 2\n#ex.3b replicate same numbers or characters\nx <- rep(\"a\", 5) # replicate \"a\" five times\nx## [1] \"a\" \"a\" \"a\" \"a\" \"a\"\n#ex.4a use seq() function\nx <- seq(1, 5, by = 1)\nx## [1] 1 2 3 4 5\n#ex.4b use seq() function\nx <- seq(1, 5, by = 0.1)\nx##  [1] 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8\n## [20] 2.9 3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9 4.0 4.1 4.2 4.3 4.4 4.5 4.6 4.7\n## [39] 4.8 4.9 5.0"},{"path":"appendix-data-structure.html","id":"check-features","chapter":"Appendix: Data Structure","heading":"Check Features","text":"R provides many functions examine features vectors objects, example:class() - return high-level data structure objecttypeof() - return low-level data structure objectattributes() - metadata objectlength() - number elements objectsum() - sum object’s elementsmean() - mean object’s elementsNumeric VectorCharacter Vector","code":"\nx <- c(1.2, 3.1, 4.0, 8.2)\nx## [1] 1.2 3.1 4.0 8.2\nclass(x)## [1] \"numeric\"\ntypeof(x)## [1] \"double\"\nlength(x)## [1] 4\nsum(x)## [1] 16.5\nmean(x)## [1] 4.125\ny <- c(\"a\", \"b\", \"c\")\nclass(y)## [1] \"character\"\nlength(y)## [1] 3"},{"path":"appendix-data-structure.html","id":"access","chapter":"Appendix: Data Structure","heading":"Access","text":"Element ID\nUse brackets [] accessing specific elements object. example, want access element #2 vector x, may specify x[2]:Equation\nR provides many ways access elements suffice specific conditions. can use mathematical symbols specify need, example:== equal> larger >= equal & larger < smaller <= equal & smaller thanwhich() function returns element # suffices specified conditionThe following examples return logical vector indicating whether element x suffices specified condition:can access elements suffice specified condition using brackets, example:Using (), can see elements (.e., #) matches need:","code":"\nx <- c(2,2,3,2,5)\nx[2] # access element #2## [1] 2\nx[c(2,4)] # access elements #2 and 4## [1] 2 2\nx[2:4] # access elements #2-4## [1] 2 3 2\n# creating a vector\nx <- c(2,2,3,2,5)\n\n# ex.1a equal\nx == 2## [1]  TRUE  TRUE FALSE  TRUE FALSE\n# ex.1b larger than\nx > 2 ## [1] FALSE FALSE  TRUE FALSE  TRUE\n# ex.2a equal\nx[x == 2]## [1] 2 2 2\n# ex.2b larger than\nx[x > 2]## [1] 3 5\n# ex.3a equal\nwhich(x == 2) # returns which elements are equal to 2## [1] 1 2 4\n# ex.3b larger than\nwhich(x > 2)## [1] 3 5"},{"path":"appendix-data-structure.html","id":"matrix","chapter":"Appendix: Data Structure","heading":"Matrix","text":"","code":""},{"path":"appendix-data-structure.html","id":"create-matrix","chapter":"Appendix: Data Structure","heading":"Create Matrix","text":"Matrix set elements (single data type) organized rows columns:","code":"\n#ex.1 cbind: combine objects by column\nx <- cbind(c(1,2,3), c(4,5,6))\nx##      [,1] [,2]\n## [1,]    1    4\n## [2,]    2    5\n## [3,]    3    6\n#ex.2 rbind: combine objects by row\nx <- rbind(c(1,2,3), c(4,5,6))\nx##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    4    5    6\n#ex.3 matrix: specify elements and the number of rows (nrow) and columns (ncol)\nx <- matrix(1:9, nrow = 3, ncol = 3)\nx##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9"},{"path":"appendix-data-structure.html","id":"check-features-1","chapter":"Appendix: Data Structure","heading":"Check Features","text":"R provides many functions examine features matrix data, example:dim() number rows columnsrowSums() row sumscolSums() column sumsInteger MatrixCharacter Matrix","code":"\nx <- matrix(1:9, nrow = 3, ncol = 3)\nx##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\nclass(x)## [1] \"matrix\" \"array\"\ntypeof(x)## [1] \"integer\"\ndim(x)## [1] 3 3\ny <- matrix(c(\"a\",\"b\", \"c\", \"d\", \"e\", \"f\"), nrow = 3, ncol = 2)\ny##      [,1] [,2]\n## [1,] \"a\"  \"d\" \n## [2,] \"b\"  \"e\" \n## [3,] \"c\"  \"f\"\nclass(y)## [1] \"matrix\" \"array\"\ntypeof(y)## [1] \"character\"\ndim(y)## [1] 3 2"},{"path":"appendix-data-structure.html","id":"access-1","chapter":"Appendix: Data Structure","heading":"Access","text":"accessing matrix elements, need pick row(s) /column(s), example:can assess element mathematical expressions just like vectors:However, care must taken accessing elements, automatically converted vector data:() needs additional argument return row column #:","code":"\nx <- matrix(1:9, nrow = 3, ncol = 3)\nx##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\nx[2,3] # access an element in row #2 and colum #3## [1] 8\nx[2,] # access elements in row #2## [1] 2 5 8\nx[c(2,3),] # access elements in rows #2 and 3##      [,1] [,2] [,3]\n## [1,]    2    5    8\n## [2,]    3    6    9\nx[,c(2,3)] # access elements in columns #2 and 3##      [,1] [,2]\n## [1,]    4    7\n## [2,]    5    8\n## [3,]    6    9\nx == 2 # equal##       [,1]  [,2]  [,3]\n## [1,] FALSE FALSE FALSE\n## [2,]  TRUE FALSE FALSE\n## [3,] FALSE FALSE FALSE\nx > 2 # larger than##       [,1] [,2] [,3]\n## [1,] FALSE TRUE TRUE\n## [2,] FALSE TRUE TRUE\n## [3,]  TRUE TRUE TRUE\nx[x == 2] # equal## [1] 2\nx[x > 2] # larger than## [1] 3 4 5 6 7 8 9\nwhich(x == 2, arr.ind = TRUE)##      row col\n## [1,]   2   1\nwhich(x > 2, arr.ind = TRUE)##      row col\n## [1,]   3   1\n## [2,]   1   2\n## [3,]   2   2\n## [4,]   3   2\n## [5,]   1   3\n## [6,]   2   3\n## [7,]   3   3"},{"path":"appendix-data-structure.html","id":"data-frame","chapter":"Appendix: Data Structure","heading":"Data Frame","text":"data frame collection elements organized rows columns, differs matrix several ways.allows inclusion multiple data types different columns.column data frame name associated .can access columns data frame respective names using $ operator.data frame commonly used data structure manipulating ecological data. loading dataset spreadsheet (discuss later), automatically recognized data frame. Let’s consider example:Creating data frame\nfollowing example, variables x y organized single data frame named df0. variables renamed part process creating data frame.Call column namesAccess columnsYou can access elements like matrix well:","code":"\n# Create data frame\nx <- c(\"Pristine\", \"Pristine\", \"Disturbed\", \"Disturbed\", \"Pristine\") # Lake type\ny <- c(1.2, 2.2, 10.9, 50.0, 3.0) # TSS: total suspended solids (mg/L)\ndf0 <- data.frame(LakeType = x, TSS = y) # x is named as \"LakeType\" while y is named as \"TSS\"\ndf0##    LakeType  TSS\n## 1  Pristine  1.2\n## 2  Pristine  2.2\n## 3 Disturbed 10.9\n## 4 Disturbed 50.0\n## 5  Pristine  3.0\ncolnames(df0) # call column names## [1] \"LakeType\" \"TSS\"\ndf0$LakeType # access LakeType## [1] \"Pristine\"  \"Pristine\"  \"Disturbed\" \"Disturbed\" \"Pristine\"\ndf0$TSS # access TSS## [1]  1.2  2.2 10.9 50.0  3.0\ndf0[,1] # access column #1## [1] \"Pristine\"  \"Pristine\"  \"Disturbed\" \"Disturbed\" \"Pristine\"\ndf0[1,] # access row #1##   LakeType TSS\n## 1 Pristine 1.2\ndf0[c(2,4),] # access row #2 and 4##    LakeType  TSS\n## 2  Pristine  2.2\n## 4 Disturbed 50.0"},{"path":"appendix-data-structure.html","id":"exercise","chapter":"Appendix: Data Structure","heading":"Exercise","text":"","code":""},{"path":"appendix-data-structure.html","id":"vector-1","chapter":"Appendix: Data Structure","heading":"Vector","text":"Create three numeric vectors length 3, 6 20, respectively. vector must created using different functions R.Create three character vectors length 3, 6 20, respectively. vector must created using different functions R.Copy following script R script perform following analysis:Identify element IDs x greater 2.0Identify element values x greater 2.0","code":"\nset.seed(1)\nx <- rnorm(100)"},{"path":"appendix-data-structure.html","id":"matrix-1","chapter":"Appendix: Data Structure","heading":"Matrix","text":"Create numeric matrix 4 rows 4 columns. column must contain identical elements.Create numeric matrix 4 rows 4 columns. row must contain identical elements.Create character matrix 4 rows 4 columns. column must contain identical elements.Create character matrix 4 rows 4 columns. row must contain identical elements.Copy following script R script perform following analysis:Identify element IDs x greater 2.0 (specify row column IDs)Identify element values x greater 2.0 calculate mean.","code":"\nset.seed(1)\nx <- matrix(rnorm(100), nrow = 10, ncol = 10)"},{"path":"appendix-data-structure.html","id":"data-frame-1","chapter":"Appendix: Data Structure","heading":"Data Frame","text":"Create data frame 3 variables 10 elements (name variables x, y z. x must character y z must numeric.Check data structure (higher-level) x, y zCopy following script R script perform following analysis:Calculate means temperature abundance states VA NC separately.","code":"\nset.seed(1)\nx <- rnorm(100, mean = 10, sd = 3)\ny <- rpois(100, lambda = 10)\nz <- rep(c(\"VA\", \"NC\"), 50)\ndf0 <- data.frame(temperature = x, abundance = y, state = z)"},{"path":"appendix-tidyverse.html","id":"appendix-tidyverse","chapter":"Appendix: Tidyverse","heading":"Appendix: Tidyverse","text":"","code":""},{"path":"appendix-tidyverse.html","id":"overview-1","chapter":"Appendix: Tidyverse","heading":"Overview","text":"R packages collections functions extend functionality R programming language. provide convenient way users access utilize specialized tools data analysis, visualization, statistical modeling, . Among , tidyverse provides useful functions data manipulation visualization.tidyverse:: bundles R packages designed make data manipulation, exploration, visualization efficient intuitive. Developed Hadley Wickham contributors, Tidyverse packages share common philosophy syntax, emphasizing consistent tidy data format. core packages, dplyr::, tidyr::, ggplot2::, provide powerful tools data wrangling, reshaping, creating visualizations.Chapter, use iris data, available default R. default data structure iris prepared data.frame(), convert data format tibble() exercise.cover basics familiar tidyverse::, plenty documentation available tidyverse package.Tidyverse websitedplyrtidyrggplot2from Data Viz","code":"\nlibrary(tidyverse)\niris <- as_tibble(iris)"},{"path":"appendix-tidyverse.html","id":"data-manipulation","chapter":"Appendix: Tidyverse","heading":"Data Manipulation","text":"","code":""},{"path":"appendix-tidyverse.html","id":"data-format","chapter":"Appendix: Tidyverse","heading":"Data Format","text":"dplyr:: tidyr:: (others) offer useful functions data manipulation. Key manipulations include: row/column manipulations, group operation, reshape, .","code":""},{"path":"appendix-tidyverse.html","id":"row-manipulation","chapter":"Appendix: Tidyverse","heading":"Row Manipulation","text":"filter(): select/remove rowsarrange() : arrange order rows","code":"\n# single match \"==\"\nfilter(iris, Species == \"virginica\")\n\n# multiple match \"%in%\"\nfilter(iris, Species %in% c(\"virginica\", \"versicolor\"))\n\n# except \"!=\"\nfilter(iris, Species != \"virginica\")\n\n# except multiple \"!(x %in% c(\"a\", \"b\"))\nfilter(iris, !(Species %in% c(\"virginica\", \"versicolor\")))\n\n# greater than \">\"\nfilter(iris, Sepal.Length > 5)\n\n# equal & greater than \">=\"\nfilter(iris, Sepal.Length >= 5)\n\n# less than \"<\"\nfilter(iris, Sepal.Length < 5)\n\n# equal & less than \"<=\"\nfilter(iris, Sepal.Length <= 5)\n# arrange in an ascending order\narrange(iris, Sepal.Length)\n\n# arrange in an descending order\narrange(iris, desc(Sepal.Length))"},{"path":"appendix-tidyverse.html","id":"column-manipulation","chapter":"Appendix: Tidyverse","heading":"Column Manipulation","text":"select(): select/remove column(s)mutate(): add column(s)","code":"\n# select one column\nselect(iris, Sepal.Length)\n\n# select multiple columns\nselect(iris, c(Sepal.Length, Sepal.Width))\n\n# remove one column\nselect(iris, -Sepal.Length)\n\n# remove multiple columns\nselect(iris, -c(Sepal.Length, Sepal.Width))\n\n# select/remove multiple columns with a start rule\n# starts_with(\"x\")\nselect(iris, starts_with(\"Sepal\"))\nselect(iris, -starts_with(\"Sepal\"))\n\n# select/remove multiple columns with an end rule\n# ends_with(\"x\")\nselect(iris, ends_with(\"Width\"))\nselect(iris, -ends_with(\"Width\"))\n# add a new column\nx <- 1:150\nmutate(iris, x = x)## # A tibble: 150 × 6\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species     x\n##           <dbl>       <dbl>        <dbl>       <dbl> <fct>   <int>\n##  1          5.1         3.5          1.4         0.2 setosa      1\n##  2          4.9         3            1.4         0.2 setosa      2\n##  3          4.7         3.2          1.3         0.2 setosa      3\n##  4          4.6         3.1          1.5         0.2 setosa      4\n##  5          5           3.6          1.4         0.2 setosa      5\n##  6          5.4         3.9          1.7         0.4 setosa      6\n##  7          4.6         3.4          1.4         0.3 setosa      7\n##  8          5           3.4          1.5         0.2 setosa      8\n##  9          4.4         2.9          1.4         0.2 setosa      9\n## 10          4.9         3.1          1.5         0.1 setosa     10\n## # ℹ 140 more rows"},{"path":"appendix-tidyverse.html","id":"piping","chapter":"Appendix: Tidyverse","heading":"Piping","text":"%>% (pipe) allows sequential operations multiple functions (shortcut: Ctr + Shift + M). pipe passes object following function first argument.","code":"\n# the following codes produce the same data frame\n# apply functions separately\ndf_vir <- filter(iris, Species == \"virginica\")\ndf_vir_sl <- select(df_vir, Sepal.Length)\nprint(df_vir_sl)## # A tibble: 50 × 1\n##    Sepal.Length\n##           <dbl>\n##  1          6.3\n##  2          5.8\n##  3          7.1\n##  4          6.3\n##  5          6.5\n##  6          7.6\n##  7          4.9\n##  8          7.3\n##  9          6.7\n## 10          7.2\n## # ℹ 40 more rows\n# piping\niris %>% \n  filter(Species == \"virginica\") %>% \n  select(Sepal.Length)## # A tibble: 50 × 1\n##    Sepal.Length\n##           <dbl>\n##  1          6.3\n##  2          5.8\n##  3          7.1\n##  4          6.3\n##  5          6.5\n##  6          7.6\n##  7          4.9\n##  8          7.3\n##  9          6.7\n## 10          7.2\n## # ℹ 40 more rows"},{"path":"appendix-tidyverse.html","id":"reshape","chapter":"Appendix: Tidyverse","heading":"Reshape","text":"pivot_wider() : reshape data frame wide formatpivot_longer(): reshape data frame long format","code":"\niris_w <- iris %>% \n  mutate(id = rep(1:50, 3)) %>% # add an ID column\n  select(id, Sepal.Length, Species) %>% \n  pivot_wider(id_cols = \"id\", # unique row ID based on\n              values_from = \"Sepal.Length\", # values in each cell from\n              names_from = \"Species\") # new column names from\n\nprint(iris_w)## # A tibble: 50 × 4\n##       id setosa versicolor virginica\n##    <int>  <dbl>      <dbl>     <dbl>\n##  1     1    5.1        7         6.3\n##  2     2    4.9        6.4       5.8\n##  3     3    4.7        6.9       7.1\n##  4     4    4.6        5.5       6.3\n##  5     5    5          6.5       6.5\n##  6     6    5.4        5.7       7.6\n##  7     7    4.6        6.3       4.9\n##  8     8    5          4.9       7.3\n##  9     9    4.4        6.6       6.7\n## 10    10    4.9        5.2       7.2\n## # ℹ 40 more rows\niris_l <- iris_w %>% \n  pivot_longer(cols = c(\"setosa\",\n                        \"versicolor\",\n                        \"virginica\"), # columns with values to be reshaped\n               names_to = \"Species\", # column IDs move to \"Species\"\n               values_to = \"Sepal.Length\") # column values move to \"Sepal.Length\"\n\nprint(iris_l)## # A tibble: 150 × 3\n##       id Species    Sepal.Length\n##    <int> <chr>             <dbl>\n##  1     1 setosa              5.1\n##  2     1 versicolor          7  \n##  3     1 virginica           6.3\n##  4     2 setosa              4.9\n##  5     2 versicolor          6.4\n##  6     2 virginica           5.8\n##  7     3 setosa              4.7\n##  8     3 versicolor          6.9\n##  9     3 virginica           7.1\n## 10     4 setosa              4.6\n## # ℹ 140 more rows"},{"path":"appendix-tidyverse.html","id":"group-operation","chapter":"Appendix: Tidyverse","heading":"Group Operation","text":"group_by() & summarize(): group--group operation. summarize() retain individual rows.group_by() & mutate(): group--group operation. mutate() retains individual rows along summary columns. forget ungroup() avoid errors following operations.","code":"\n# grouping by \"Species\", then take means \"Speal.Length\" for each species\niris %>% \n  group_by(Species) %>% \n  summarize(mu_sl = mean(Sepal.Length))## # A tibble: 3 × 2\n##   Species    mu_sl\n##   <fct>      <dbl>\n## 1 setosa      5.01\n## 2 versicolor  5.94\n## 3 virginica   6.59\n# grouping by \"Species\", then take means & SD \"Speal.Length\" for each species\niris %>% \n  group_by(Species) %>% \n  summarize(mu_sl = mean(Sepal.Length),\n            sd_sl = sd(Sepal.Length))## # A tibble: 3 × 3\n##   Species    mu_sl sd_sl\n##   <fct>      <dbl> <dbl>\n## 1 setosa      5.01 0.352\n## 2 versicolor  5.94 0.516\n## 3 virginica   6.59 0.636\n# grouping by \"Species\", then take means \"Speal.Length\" for each species\niris %>% \n  group_by(Species) %>% \n  mutate(mu_sl = mean(Sepal.Length)) %>% \n  ungroup()## # A tibble: 150 × 6\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species mu_sl\n##           <dbl>       <dbl>        <dbl>       <dbl> <fct>   <dbl>\n##  1          5.1         3.5          1.4         0.2 setosa   5.01\n##  2          4.9         3            1.4         0.2 setosa   5.01\n##  3          4.7         3.2          1.3         0.2 setosa   5.01\n##  4          4.6         3.1          1.5         0.2 setosa   5.01\n##  5          5           3.6          1.4         0.2 setosa   5.01\n##  6          5.4         3.9          1.7         0.4 setosa   5.01\n##  7          4.6         3.4          1.4         0.3 setosa   5.01\n##  8          5           3.4          1.5         0.2 setosa   5.01\n##  9          4.4         2.9          1.4         0.2 setosa   5.01\n## 10          4.9         3.1          1.5         0.1 setosa   5.01\n## # ℹ 140 more rows"},{"path":"appendix-tidyverse.html","id":"join","chapter":"Appendix: Tidyverse","heading":"Join","text":"left_join(): merge data frames based column(s)","code":"\n# matching by a single column\n## left join by \"Species\": one to one\ndf1 <- tibble(Species = c(\"A\", \"B\", \"C\"),\n              x = c(1, 2, 3))\n\ndf2 <- tibble(Species = c(\"A\", \"B\", \"C\"),\n              y = c(4, 5, 6))\n\nleft_join(x = df1,\n          y = df2,\n          by = \"Species\")## # A tibble: 3 × 3\n##   Species     x     y\n##   <chr>   <dbl> <dbl>\n## 1 A           1     4\n## 2 B           2     5\n## 3 C           3     6\n# matching by a single column\n## left join by \"Species\": one to many\ndf3 <- tibble(Species = c(\"A\", \"A\", \"B\", \"C\"),\n              y = c(4, 5, 6, 7))\n\nleft_join(x = df1,\n          y = df3,\n          by = \"Species\")## # A tibble: 4 × 3\n##   Species     x     y\n##   <chr>   <dbl> <dbl>\n## 1 A           1     4\n## 2 A           1     5\n## 3 B           2     6\n## 4 C           3     7\n# matching by a single column\n## left join by \"Species\": one to missing\ndf4 <- tibble(Species = c(\"A\", \"A\", \"C\"),\n              y = c(4, 5, 7))\n\nleft_join(x = df1,\n          y = df4,\n          by = \"Species\")## # A tibble: 4 × 3\n##   Species     x     y\n##   <chr>   <dbl> <dbl>\n## 1 A           1     4\n## 2 A           1     5\n## 3 B           2    NA\n## 4 C           3     7\n# matching by multiple columns\n## one to one\ndf5 <- tibble(Species = c(\"A\", \"B\", \"C\"),\n              x = c(1, 2, 3),\n              z = c(\"cool\", \"awesome\", \"magical\"))\n\nleft_join(x = df1,\n          y = df5,\n          by = c(\"Species\", \"x\"))## # A tibble: 3 × 3\n##   Species     x z      \n##   <chr>   <dbl> <chr>  \n## 1 A           1 cool   \n## 2 B           2 awesome\n## 3 C           3 magical\n# matching by multiple columns\n## one to many\ndf6 <- tibble(Species = c(\"A\", \"A\", \"B\", \"C\"),\n              x = c(1, 1, 2, 3),\n              z = c(\"cool\", \"cool\", \"awesome\", \"magical\"))\n\nleft_join(x = df1,\n          y = df6,\n          by = c(\"Species\", \"x\"))## # A tibble: 4 × 3\n##   Species     x z      \n##   <chr>   <dbl> <chr>  \n## 1 A           1 cool   \n## 2 A           1 cool   \n## 3 B           2 awesome\n## 4 C           3 magical\n# matching by multiple columns\n## one to missing\ndf6 <- tibble(Species = c(\"A\", \"B\", \"C\"),\n              x = c(1, 2, 4),\n              z = c(\"cool\", \"awesome\", \"magical\"))\n\nleft_join(x = df1,\n          y = df6,\n          by = c(\"Species\", \"x\"))## # A tibble: 3 × 3\n##   Species     x z      \n##   <chr>   <dbl> <chr>  \n## 1 A           1 cool   \n## 2 B           2 awesome\n## 3 C           3 <NA>"},{"path":"appendix-tidyverse.html","id":"exercise-1","chapter":"Appendix: Tidyverse","heading":"Exercise","text":"Complete exercise .","code":""},{"path":"appendix-tidyverse.html","id":"visualization","chapter":"Appendix: Tidyverse","heading":"Visualization","text":"ggplot2:: offers range convenient functions data visualization. foundational function, ggplot(), provides initial framework adding supplementary layers using + operator. ggplot(), define variables plotted x- y-axis aes(). example:Please note aes() refers columns data frame. Variables names exist data frame used.","code":"# without pipe\nggplot(data = iris,\n       mapping = aes(x = Sepal.Length,\n                     y = Sepal.Width)) +\n  # additional layers...\n\n# with pipe\niris %>% \n  ggplot(mapping = aes(x = Sepal.Length,\n                       y = Sepal.Width)) +\n  # additional layers...  "},{"path":"appendix-tidyverse.html","id":"point","chapter":"Appendix: Tidyverse","heading":"Point","text":"geom_point() : Add point layer","code":"\n# basic plot\niris %>% \n  ggplot(aes(x = Sepal.Length,\n             y = Sepal.Width)) +\n  geom_point()\n# change color by \"Species\" column\niris %>% \n  ggplot(aes(x = Sepal.Length,\n             y = Sepal.Width,\n             color = Species)) +\n  geom_point()"},{"path":"appendix-tidyverse.html","id":"line","chapter":"Appendix: Tidyverse","heading":"Line","text":"geom_line() : Add line layer","code":"\n# sample data\ndf0 <- tibble(x = rep(1:50, 3),\n              y = x * 2)\n\n# basic plot\ndf0 %>% \n  ggplot(aes(x = x,\n             y = y)) +\n  geom_line()"},{"path":"appendix-tidyverse.html","id":"histogram","chapter":"Appendix: Tidyverse","heading":"Histogram","text":"geom_histogram() : add histogram layer","code":"\n# basic plot; bins = 30 by default\niris %>% \n  ggplot(aes(x = Sepal.Length)) +\n  geom_histogram()\n# change bin width\niris %>% \n  ggplot(aes(x = Sepal.Length)) +\n  geom_histogram(binwidth = 0.5)\n# change bin number\niris %>% \n  ggplot(aes(x = Sepal.Length)) +\n  geom_histogram(bins = 50)"},{"path":"appendix-tidyverse.html","id":"boxplot","chapter":"Appendix: Tidyverse","heading":"Boxplot","text":"geom_boxplot() : add boxplot layer","code":"\n# basic plot\niris %>% \n  ggplot(aes(x = Species,\n             y = Sepal.Length)) +\n  geom_boxplot()\n# change fill by \"Species\"\niris %>% \n  ggplot(aes(x = Species,\n             y = Sepal.Length,\n             fill = Species)) +\n  geom_boxplot()\n# change fill by \"Species\", but consistent color\niris %>% \n  ggplot(aes(x = Species,\n             y = Sepal.Length,\n             fill = Species)) +\n  geom_boxplot(color = \"darkgrey\")"},{"path":"appendix-tidyverse.html","id":"exercise-2","chapter":"Appendix: Tidyverse","heading":"Exercise","text":"maintenance","code":""},{"path":"appendix-base-plot.html","id":"appendix-base-plot","chapter":"Appendix: Base Plot","heading":"Appendix: Base Plot","text":"","code":""},{"path":"appendix-base-plot.html","id":"overview-2","chapter":"Appendix: Base Plot","heading":"Overview","text":"R offers variety functions aid visualizing data. graphics package R provides set functions basic graphics (list functions). demonstrate functionality graphics functions, utilize built-iris dataset R.","code":"\nhead(iris)## # A tibble: 6 × 5\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n## 1          5.1         3.5          1.4         0.2 setosa \n## 2          4.9         3            1.4         0.2 setosa \n## 3          4.7         3.2          1.3         0.2 setosa \n## 4          4.6         3.1          1.5         0.2 setosa \n## 5          5           3.6          1.4         0.2 setosa \n## 6          5.4         3.9          1.7         0.4 setosa"},{"path":"appendix-base-plot.html","id":"plot","chapter":"Appendix: Base Plot","heading":"Plot","text":"creating plot, typically need specify formula define relationship variables. instance, wish visualize association x y (y vertical axis x horizontal axis), formula y ~ x (left side formula represents vertical axis). iris dataset, access following columns: Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, Species. subsequent example, plot relationship Sepal.Length Sepal.Width:data = argument informs function dataset variables (Sepal.Length Sepal.Width) extracted.","code":"\nplot(Sepal.Length ~ Sepal.Width, data = iris)"},{"path":"appendix-base-plot.html","id":"symbol","chapter":"Appendix: Base Plot","heading":"Symbol","text":"pch argument. Choose 1 25 (google r plot pch details)","code":"\nplot(Sepal.Length ~ Sepal.Width, data = iris,\n     pch = 19)"},{"path":"appendix-base-plot.html","id":"symbol-size","chapter":"Appendix: Base Plot","heading":"Symbol size","text":"cex argument. cex = 1 default value. cex = 2 twice large default value.","code":"\nplot(Sepal.Length ~ Sepal.Width, data = iris,\n     pch = 19, cex = 2)"},{"path":"appendix-base-plot.html","id":"symbol-color-border","chapter":"Appendix: Base Plot","heading":"Symbol color (border)","text":"col argument (quote \"color name\" specifying). Google r color name color options.","code":"\nplot(Sepal.Length ~ Sepal.Width, data = iris,\n     pch = 21, cex = 2, col = \"gray\")"},{"path":"appendix-base-plot.html","id":"symbol-color-fill","chapter":"Appendix: Base Plot","heading":"Symbol color (fill)","text":"bg argument (quote \"color name\" specifying). Available subset symbol options (symbols pre-defined filled color).","code":"\nplot(Sepal.Length ~ Sepal.Width, data = iris,\n     pch = 21, cex = 2, bg = \"lightgray\")"},{"path":"appendix-base-plot.html","id":"label","chapter":"Appendix: Base Plot","heading":"Label","text":"ylab xlab arguments. Provide \"quoted text\".","code":"\nplot(Sepal.Length ~ Sepal.Width, data = iris,\n     pch = 21, cex = 2, bg = \"lightgray\",\n     xlab = \"Sepal width (cm)\", ylab = \"Sepal length (cm)\")"},{"path":"appendix-base-plot.html","id":"axis","chapter":"Appendix: Base Plot","heading":"Axis","text":"Delete axes axes = F re-draw box() axis() functions.","code":"\nplot(Sepal.Length ~ Sepal.Width, data = iris,\n     pch = 21, cex = 2, bg = \"lightgray\",\n     xlab = \"Sepal width (cm)\", ylab = \"Sepal length (cm)\",\n     axes = F)\nbox(bty = \"l\") # L-shaped border lines\naxis(1) # 1: draw x-axis\naxis(2, las = 2) # 2: draw y-axis, las = 2: make axis lables horizontal"},{"path":"appendix-base-plot.html","id":"boxplot-1","chapter":"Appendix: Base Plot","heading":"Boxplot","text":"boxplot() used x-axis factor-type data (default, plot() produce boxplot x-axis factor variable). iris dataset, column Species factor variable. Compare Sepal.Length among species using boxplot().can customize plot(), slighlty different.","code":"\nboxplot(Sepal.Length ~ Species, data = iris)"},{"path":"appendix-base-plot.html","id":"box-color","chapter":"Appendix: Base Plot","heading":"Box color","text":"col argument.","code":"\nboxplot(Sepal.Length ~ Species, data = iris,\n        col = \"lightgray\")"},{"path":"appendix-base-plot.html","id":"border-color","chapter":"Appendix: Base Plot","heading":"Border color","text":"border argument.","code":"\nboxplot(Sepal.Length ~ Species, data = iris,\n        col = \"lightgray\", border = \"grey48\")"},{"path":"appendix-base-plot.html","id":"box-width","chapter":"Appendix: Base Plot","heading":"Box width","text":"boxwex argument.","code":"\nboxplot(Sepal.Length ~ Species, data = iris,\n        col = \"lightgray\", border = \"grey48\",\n        boxwex = 0.4 )"},{"path":"appendix-base-plot.html","id":"axis-1","chapter":"Appendix: Base Plot","heading":"Axis","text":"Delete axes axes = F re-draw box() axis() functions.","code":"\nboxplot(Sepal.Length ~ Species, data = iris,\n        col = \"lightgray\", border = \"grey48\",\n        boxwex = 0.4, ylab = \"Sepal length (cm)\",\n        axes = F)\nbox(bty = \"l\")\naxis(1, at = c(1, 2, 3), labels = c(\"Setosa\", \"Versicolor\", \"Virginica\") )\naxis(2, las = 2)"}]
